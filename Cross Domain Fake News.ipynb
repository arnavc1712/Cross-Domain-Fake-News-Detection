{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "# import demoji\n",
    "import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain1_path = \"./dEFEND/gossipcop_content_no_ignore.tsv\"\n",
    "domain2_path = \"./dEFEND/politifact_content_no_ignore.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain1_frame = pd.read_csv(domain1_path,delimiter=\"\\t\").set_index('id')\n",
    "domain2_frame = pd.read_csv(domain2_path,delimiter=\"\\t\").set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts for each label for the 2 domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gossipcop-9096198130</th>\n",
       "      <td>1</td>\n",
       "      <td>Sarah Jessica Parker is getting candid about h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-6982710185</th>\n",
       "      <td>1</td>\n",
       "      <td>Many celebrities have been sharing their thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-7887456921</th>\n",
       "      <td>1</td>\n",
       "      <td>He reportedly hasn't seen her in over four yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-1594778479</th>\n",
       "      <td>1</td>\n",
       "      <td>The fashion crowd is speaking out about Kim Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-8172018375</th>\n",
       "      <td>1</td>\n",
       "      <td>What term do you want to search? Search with g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-854842</th>\n",
       "      <td>0</td>\n",
       "      <td>Aisha Tyler‘s divorce from Jeffrey Tietjens ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-843491</th>\n",
       "      <td>0</td>\n",
       "      <td>All four of Queen Elizabeth and Prince Philip'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-897778</th>\n",
       "      <td>0</td>\n",
       "      <td>Theresa Caputo is adjusting to her new life af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-899849</th>\n",
       "      <td>0</td>\n",
       "      <td>Follow Us on Twitter  Nominations for the 25th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-927093</th>\n",
       "      <td>0</td>\n",
       "      <td>Though the happy couple can’t be together in p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5816 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label                                            content\n",
       "id                                                                            \n",
       "gossipcop-9096198130      1  Sarah Jessica Parker is getting candid about h...\n",
       "gossipcop-6982710185      1  Many celebrities have been sharing their thoug...\n",
       "gossipcop-7887456921      1  He reportedly hasn't seen her in over four yea...\n",
       "gossipcop-1594778479      1  The fashion crowd is speaking out about Kim Ka...\n",
       "gossipcop-8172018375      1  What term do you want to search? Search with g...\n",
       "...                     ...                                                ...\n",
       "gossipcop-854842          0  Aisha Tyler‘s divorce from Jeffrey Tietjens ha...\n",
       "gossipcop-843491          0  All four of Queen Elizabeth and Prince Philip'...\n",
       "gossipcop-897778          0  Theresa Caputo is adjusting to her new life af...\n",
       "gossipcop-899849          0  Follow Us on Twitter  Nominations for the 25th...\n",
       "gossipcop-927093          0  Though the happy couple can’t be together in p...\n",
       "\n",
       "[5816 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain1_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "label       \n",
       "0       3586\n",
       "1       2230"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain1_frame.groupby(['label'])[['label']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "label       \n",
       "0        145\n",
       "1        270"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain2_frame.groupby(['label'])[['label']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[()\\\"_#/@;*%:<>{}`+=~|.!?,'$-\\[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"[0-9]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(decoder_input_sentence):\n",
    "    start = \"<BOS> \"\n",
    "    end = \" <EOS>\"\n",
    "    final_target = [start + text + end for text in decoder_input_sentence] \n",
    "    return final_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenizer(text_lists):\n",
    "#     return [line.split(\" \") for line in text_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(text_lists):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text_lists)\n",
    "    \n",
    "    dictionary = tokenizer.word_index\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k,v in dictionary.items():\n",
    "        word2idx[k]=v\n",
    "        idx2word[v]=k\n",
    "    \n",
    "    return word2idx,idx2word,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = domain1_frame['content'].apply(lambda x: clean_text(x)).values.tolist()+ domain2_frame['content'].apply(lambda x: clean_text(x)).values.tolist()\n",
    "decoder_inputs = tagger(encoder_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_word2idx,t_idx2word,tokenizer = create_vocab(decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t_encoder_inputs = tokenizer.texts_to_sequences(encoder_inputs)\n",
    "t_decoder_inputs = tokenizer.texts_to_sequences(decoder_inputs)\n",
    "\n",
    "max_encoder_len = max([len(val) for val in t_encoder_inputs])\n",
    "max_decoder_len = max([len(val) for val in t_decoder_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(X,Y,batch_size=3,max_len=5):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        \n",
    "        for j in range(0,len(X),batch_size):\n",
    "            \n",
    "            encoder_input = np.zeros((batch_size,max_len))\n",
    "            for j,input_seq in enumerate(X[j:j+batch_size]):\n",
    "                for i,word_idx in enumerate(input_seq[:max_len]):\n",
    "                    encoder_input[j,i]= word_idx\n",
    "\n",
    "            decoder_input = np.zeros((batch_size,max_len+1))\n",
    "            decoder_target = np.zeros((batch_size,max_len+1))\n",
    "\n",
    "            for j,target_seq in enumerate(Y[j:j+batch_size]):\n",
    "                for i,word_idx in enumerate(target_seq[:max_len+1]):\n",
    "                    decoder_input[j,i] = word_idx\n",
    "                    \n",
    "                    if i>0:\n",
    "                        decoder_target[j,i-1] = word_idx\n",
    "\n",
    "            yield [[encoder_input,decoder_input],decoder_target]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_encoder_inputs = [[1,2,3,4]]\n",
    "t_decoder_inputs = [[65,1,2,3,4,64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = test_generator(t_encoder_inputs,t_decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[1., 2., 3., 4., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]), array([[65.,  1.,  2.,  3.,  4., 64.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.]])],\n",
       " array([[ 1.,  2.,  3.,  4., 64.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for sent in decoder_inputs:\n",
    "    for word in sent.split(\" \"):\n",
    "        if word not in vocab:\n",
    "            vocab.add(word)\n",
    "\n",
    "vocab = sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "idx2word = {}\n",
    "\n",
    "for idx,word in enumerate(vocab):\n",
    "    word2idx[word]=idx\n",
    "    idx2word[idx]=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_input = max([len(sent.split()) for sent in decoder_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_input =5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X,Y,batch_size=3):\n",
    "    while True:\n",
    "        for j in range(0,len(X),batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size,max_len_input),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size,max_len_input+1),dtype=\"float32\")\n",
    "            decoder_target_data = np.zeros((batch_size,max_len_input+1),dtype=\"float32\")\n",
    "            \n",
    "            for i,(input_text,target_text) in enumerate(zip(X[j:j+batch_size],Y[j:j+batch_size])):\n",
    "                for t,word in enumerate(input_text.split(\" \")[:max_len_input]):\n",
    "                    encoder_input_data[i,t] = word2idx[word]\n",
    "                    \n",
    "                ### Decoder target data is one timestep ahead of decoder input data\n",
    "                \n",
    "                for t,word in enumerate(target_text.split(\" \")[:max_len_input+1]):\n",
    "                    if t<len(target_text.split(\" \")[:max_len_input+1]):\n",
    "                        decoder_input_data[i,t] = word2idx[word]\n",
    "                    if t>0:\n",
    "                        decoder_target_data[i,t-1] = word2idx[word]\n",
    "                    decoder_target_data[i,-1] = word2idx[\"<EOS>\"]\n",
    "        \n",
    "        yield [[encoder_input_data,decoder_input_data],decoder_target_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generate_batch(encoder_inputs,decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMM",
   "language": "python",
   "name": "smm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

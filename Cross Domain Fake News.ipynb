{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, Dense,Embedding,RepeatVector,Bidirectional\n",
    "from keras.models import Model\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "# import demoji\n",
    "import tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_list = list(set(stopwords.words('english')))\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain1_path = \"./dEFEND/gossipcop_content_no_ignore.tsv\"\n",
    "domain2_path = \"./dEFEND/politifact_content_no_ignore.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain1_frame = pd.read_csv(domain1_path,delimiter=\"\\t\").set_index('id')\n",
    "domain2_frame = pd.read_csv(domain2_path,delimiter=\"\\t\").set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts for each label for the 2 domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gossipcop-9096198130</th>\n",
       "      <td>1</td>\n",
       "      <td>Sarah Jessica Parker is getting candid about h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-6982710185</th>\n",
       "      <td>1</td>\n",
       "      <td>Many celebrities have been sharing their thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-7887456921</th>\n",
       "      <td>1</td>\n",
       "      <td>He reportedly hasn't seen her in over four yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-1594778479</th>\n",
       "      <td>1</td>\n",
       "      <td>The fashion crowd is speaking out about Kim Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-8172018375</th>\n",
       "      <td>1</td>\n",
       "      <td>What term do you want to search? Search with g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-854842</th>\n",
       "      <td>0</td>\n",
       "      <td>Aisha Tyler‘s divorce from Jeffrey Tietjens ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-843491</th>\n",
       "      <td>0</td>\n",
       "      <td>All four of Queen Elizabeth and Prince Philip'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-897778</th>\n",
       "      <td>0</td>\n",
       "      <td>Theresa Caputo is adjusting to her new life af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-899849</th>\n",
       "      <td>0</td>\n",
       "      <td>Follow Us on Twitter  Nominations for the 25th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-927093</th>\n",
       "      <td>0</td>\n",
       "      <td>Though the happy couple can’t be together in p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5816 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label                                            content\n",
       "id                                                                            \n",
       "gossipcop-9096198130      1  Sarah Jessica Parker is getting candid about h...\n",
       "gossipcop-6982710185      1  Many celebrities have been sharing their thoug...\n",
       "gossipcop-7887456921      1  He reportedly hasn't seen her in over four yea...\n",
       "gossipcop-1594778479      1  The fashion crowd is speaking out about Kim Ka...\n",
       "gossipcop-8172018375      1  What term do you want to search? Search with g...\n",
       "...                     ...                                                ...\n",
       "gossipcop-854842          0  Aisha Tyler‘s divorce from Jeffrey Tietjens ha...\n",
       "gossipcop-843491          0  All four of Queen Elizabeth and Prince Philip'...\n",
       "gossipcop-897778          0  Theresa Caputo is adjusting to her new life af...\n",
       "gossipcop-899849          0  Follow Us on Twitter  Nominations for the 25th...\n",
       "gossipcop-927093          0  Though the happy couple can’t be together in p...\n",
       "\n",
       "[5816 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain1_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "label       \n",
       "0       3586\n",
       "1       2230"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain1_frame.groupby(['label'])[['label']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "label       \n",
       "0        145\n",
       "1        270"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain2_frame.groupby(['label'])[['label']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[()\\\"_#/@;*%:{}<>`+=~|.!?,'$-\\[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"[0-9]\", \"\", text)\n",
    "    \n",
    "#     for words in stopwords_list:\n",
    "#         text = re.sub(r\"\\b{}\\b\".format(words),\"\",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(decoder_input_sentence):\n",
    "    start = \"<BOS> \"\n",
    "    end = \" <EOS>\"\n",
    "    final_target = [start + text + end for text in decoder_input_sentence] \n",
    "    return final_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenizer(text_lists):\n",
    "#     return [line.split(\" \") for line in text_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(text_lists):\n",
    "    tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "    tokenizer.fit_on_texts(text_lists)\n",
    "    \n",
    "    dictionary = tokenizer.word_index\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k,v in dictionary.items():\n",
    "        word2idx[k]=v\n",
    "        idx2word[v]=k\n",
    "    \n",
    "    return word2idx,idx2word,tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = domain1_frame['content'].apply(lambda x: clean_text(x)).values.tolist()+ domain2_frame['content'].apply(lambda x: clean_text(x)).values.tolist()\n",
    "# encoder_inputs = \n",
    "decoder_inputs = tagger(encoder_inputs[:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake news detection training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CC = domain1_frame['content'].apply(lambda x: clean_text(x)).values.tolist()+ domain2_frame['content'].apply(lambda x: clean_text(x)).values.tolist()\n",
    "Y_CC = np.array(domain1_frame['label'].apply(lambda x:int(x)).values.tolist()+ domain2_frame['label'].apply(lambda x:int(x)).values.tolist())\n",
    "Y_CC_oh = to_categorical(Y_CC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DC = domain1_frame['content'].apply(lambda x: clean_text(x)).values.tolist()+ domain2_frame['content'].apply(lambda x: clean_text(x)).values.tolist()\n",
    "Y_DC = np.array([0]*len(domain1_frame) + [1]*len(domain2_frame))\n",
    "Y_DC_oh = to_categorical(Y_DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx,idx2word,tokenizer = create_vocab(encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to ambiguity with regards to Keras Tokenizer num_words, below is a good enough fix, though it changes the tokenizer word_index outside of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "\n",
    "sorted_by_word_count = sorted(tokenizer.word_counts.items(), key=lambda kv: kv[1], reverse=True)\n",
    "tokenizer.word_index = {}\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "i = 0\n",
    "for word,count in sorted_by_word_count:\n",
    "    if i == num_words:\n",
    "        break\n",
    "\n",
    "    tokenizer.word_index[word] = i + 1    # <= because tokenizer is 1 indexed\n",
    "    word2idx[word] = i+1\n",
    "    idx2word[i+1]=word\n",
    "    i += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index[tokenizer.oov_token] = num_words+1\n",
    "word2idx[tokenizer.oov_token] = num_words+1\n",
    "idx2word[num_words+1]=tokenizer.oov_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = len(tokenizer.word_index)+1\n",
    "# tokenizer.word_index[\"<BOS>\"] = idx\n",
    "# word2idx[\"<BOS>\"] = idx\n",
    "# idx2word[idx] = \"<BOS>\"\n",
    "\n",
    "# idx = len(tokenizer.word_index)+1\n",
    "# tokenizer.word_index[\"<EOS>\"] = idx\n",
    "# word2idx[\"<EOS>\"] = idx\n",
    "# idx2word[idx] = \"<EOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t_encoder_inputs = tokenizer.texts_to_sequences(encoder_inputs)\n",
    "t_decoder_inputs = tokenizer.texts_to_sequences(decoder_inputs)\n",
    "\n",
    "\n",
    "# \n",
    "t_encoder_inputs = pad_sequences(t_encoder_inputs,maxlen=10,padding='post', truncating='post')\n",
    "\n",
    "# t_decoder_inputs = pad_sequences(t_encoder_inputs,maxlen=100,padding='post', truncating='post')\n",
    "# t_decoder_inputs = np.insert(t_decoder_inputs,0,word2idx[\"<BOS>\"],axis=1)\n",
    "# t_decoder_inputs = np.insert(t_decoder_inputs,t_decoder_inputs.shape[1],word2idx[\"<EOS>\"],axis=1)\n",
    "\n",
    "\n",
    "max_encoder_len = max([len(val) for val in t_encoder_inputs])\n",
    "max_decoder_len = max([len(val) for val in t_decoder_inputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(X,Y,Y_CC,Y_DC,batch_size=128,max_len=5):\n",
    "    \n",
    "        \n",
    "    y_cc = to_categorical(Y_CC)\n",
    "    y_dc = to_categorical(Y_DC)\n",
    "    for idx in range(0,len(X),batch_size):\n",
    "\n",
    "        encoder_input = np.zeros((batch_size,max_encoder_len))\n",
    "#         decoder_input = np.zeros((batch_size,max_decoder_len))\n",
    "        decoder_target = np.zeros((batch_size,max_encoder_len,len(word2idx)+1))\n",
    "        for j,input_seq in enumerate(X[idx:idx+batch_size]):\n",
    "            for i,word_idx in enumerate(input_seq):\n",
    "                encoder_input[j,i]= word_idx\n",
    "                decoder_target[j,i,word_idx] = 1\n",
    "           \n",
    "        yield [encoder_input,[decoder_target,y_cc[idx:idx+batch_size],y_dc[idx:idx+batch_size]]]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data_generator(X,Y,Y_CC,Y_DC):\n",
    "    encoder_input = np.zeros((len(X),max_encoder_len))\n",
    "    decoder_input = np.zeros((len(X),max_decoder_len))\n",
    "    decoder_target = np.zeros((len(X),max_encoder_len,len(word2idx)+1)) ## Extra index for padding, word2idx is 1 indexed\n",
    "    for j,(input_seq,target_seq) in enumerate(zip(X,Y)):\n",
    "        for i,word_idx in enumerate(input_seq):\n",
    "            encoder_input[j,i]= word_idx\n",
    "            decoder_target[j,i,word_idx] = 1\n",
    "    \n",
    "    y_cc = to_categorical(Y_CC)\n",
    "    y_dc = to_categorical(Y_DC)\n",
    "\n",
    "    return [encoder_input,[decoder_target,y_cc,y_dc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len= len(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Glove Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_len+1, 100))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i+1] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 100)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(max_encoder_len,),name=\"encoder_inputs\")\n",
    "\n",
    "encoder_embedding = Embedding(vocab_len+1,100,trainable=True,weights=[embedding_matrix],input_length=max_encoder_len,mask_zero=True,name=\"encoder_embedding\")\n",
    "\n",
    "encoder_inputs = encoder_embedding(inputs)\n",
    "\n",
    "\n",
    "encoder = LSTM(64,return_state=True)\n",
    "\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "\n",
    "encoder_states = [state_h,state_c]\n",
    "\n",
    "\n",
    "########## Content Classification Part ###############\n",
    "\n",
    "fully_connected_CC = Dense(128,\n",
    "                       activation=\"tanh\",\n",
    "                       name=\"non_linear_CC\")\n",
    "\n",
    "logits_CC = fully_connected_CC(encoder_outputs)\n",
    "\n",
    "softmax_layer_CC = Dense(2,\n",
    "                     activation=\"softmax\",\n",
    "                     name=\"softmax_layer_CC\")\n",
    "\n",
    "output_CC = softmax_layer_CC(logits_CC)\n",
    "\n",
    "\n",
    "######### Domain Classification Part ##############\n",
    "\n",
    "fully_connected_DC = Dense(128,\n",
    "                       activation=\"tanh\",\n",
    "                       name=\"non_linear_DC\")\n",
    "\n",
    "logits_DC = fully_connected_DC(encoder_outputs)\n",
    "\n",
    "softmax_layer_DC = Dense(2,\n",
    "                     activation=\"softmax\",\n",
    "                     name=\"softmax_layer_DC\")\n",
    "\n",
    "output_DC = softmax_layer_DC(logits_DC)\n",
    "\n",
    "\n",
    "########### Autoencoder PART #############\n",
    "decoder_inputs = RepeatVector(max_encoder_len)(encoder_outputs)\n",
    "\n",
    "\n",
    "decoder_lstm = LSTM(64, \n",
    "                         return_state=True,\n",
    "                        return_sequences=True,\n",
    "                         name = 'decoder_lstm')\n",
    "\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "\n",
    "\n",
    "decoder_dense = Dense(vocab_len+1, \n",
    "                      activation='softmax', \n",
    "                      name = 'decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs,[decoder_outputs,output_CC,output_DC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, 10, 100)      1000200     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_28 (LSTM)                  [(None, 64), (None,  42240       encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_18 (RepeatVector) (None, 10, 64)       0           lstm_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 10, 64), (No 33024       repeat_vector_18[0][0]           \n",
      "                                                                 lstm_28[0][1]                    \n",
      "                                                                 lstm_28[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "non_linear_CC (Dense)           (None, 128)          8320        lstm_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "non_linear_DC (Dense)           (None, 128)          8320        lstm_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, 10, 10002)    650130      decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "softmax_layer_CC (Dense)        (None, 2)            258         non_linear_CC[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax_layer_DC (Dense)        (None, 2)            258         non_linear_DC[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,742,750\n",
      "Trainable params: 1,742,750\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = test_generator(t_encoder_inputs,t_decoder_inputs,Y_CC,Y_DC,batch_size=1000)\n",
    "encoder_input_data,[decoder_target_data,y_cc,y_dc] = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnav1712/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/800\n",
      "900/900 [==============================] - 6s 7ms/step - loss: 0.9722 - decoder_dense_loss: 8.9584 - softmax_layer_CC_loss: 0.1470 - softmax_layer_DC_loss: 3.5608 - val_loss: 0.7560 - val_decoder_dense_loss: 8.4555 - val_softmax_layer_CC_loss: 0.0062 - val_softmax_layer_DC_loss: 9.3305\n",
      "Epoch 2/800\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 0.6702 - decoder_dense_loss: 7.9532 - softmax_layer_CC_loss: 0.0030 - softmax_layer_DC_loss: 13.3220 - val_loss: 0.6184 - val_decoder_dense_loss: 7.7126 - val_softmax_layer_CC_loss: 0.0013 - val_softmax_layer_DC_loss: 15.3631\n",
      "Epoch 3/800\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 0.5775 - decoder_dense_loss: 7.2631 - softmax_layer_CC_loss: 9.8817e-04 - softmax_layer_DC_loss: 15.3681 - val_loss: 0.5826 - val_decoder_dense_loss: 7.3597 - val_softmax_layer_CC_loss: 6.8552e-04 - val_softmax_layer_DC_loss: 15.3745\n",
      "Epoch 4/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.5407 - decoder_dense_loss: 6.9698 - softmax_layer_CC_loss: 5.5999e-04 - softmax_layer_DC_loss: 15.3770 - val_loss: 0.5631 - val_decoder_dense_loss: 7.1665 - val_softmax_layer_CC_loss: 4.1044e-04 - val_softmax_layer_DC_loss: 15.3780\n",
      "Epoch 5/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.5185 - decoder_dense_loss: 6.6894 - softmax_layer_CC_loss: 3.4341e-04 - softmax_layer_DC_loss: 15.3790 - val_loss: 0.5525 - val_decoder_dense_loss: 7.0610 - val_softmax_layer_CC_loss: 2.5865e-04 - val_softmax_layer_DC_loss: 15.3790\n",
      "Epoch 6/800\n",
      "900/900 [==============================] - 5s 5ms/step - loss: 0.5046 - decoder_dense_loss: 6.5850 - softmax_layer_CC_loss: 2.1561e-04 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5469 - val_decoder_dense_loss: 7.0055 - val_softmax_layer_CC_loss: 1.7251e-04 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 7/800\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 0.4964 - decoder_dense_loss: 6.5258 - softmax_layer_CC_loss: 1.4191e-04 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5458 - val_decoder_dense_loss: 6.9952 - val_softmax_layer_CC_loss: 1.1280e-04 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 8/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4917 - decoder_dense_loss: 6.4285 - softmax_layer_CC_loss: 9.8673e-05 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5468 - val_decoder_dense_loss: 7.0051 - val_softmax_layer_CC_loss: 7.5776e-05 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 9/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4890 - decoder_dense_loss: 6.4448 - softmax_layer_CC_loss: 6.1696e-05 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5489 - val_decoder_dense_loss: 7.0262 - val_softmax_layer_CC_loss: 4.9260e-05 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 10/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4871 - decoder_dense_loss: 6.3714 - softmax_layer_CC_loss: 4.0180e-05 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5512 - val_decoder_dense_loss: 7.0492 - val_softmax_layer_CC_loss: 3.7737e-05 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 11/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4858 - decoder_dense_loss: 6.4230 - softmax_layer_CC_loss: 3.0689e-05 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5536 - val_decoder_dense_loss: 7.0743 - val_softmax_layer_CC_loss: 2.3162e-05 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 12/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4844 - decoder_dense_loss: 6.3666 - softmax_layer_CC_loss: 1.9256e-05 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5566 - val_decoder_dense_loss: 7.1037 - val_softmax_layer_CC_loss: 1.5099e-05 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 13/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4834 - decoder_dense_loss: 6.3661 - softmax_layer_CC_loss: 1.2916e-05 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5589 - val_decoder_dense_loss: 7.1266 - val_softmax_layer_CC_loss: 9.2420e-06 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 14/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4823 - decoder_dense_loss: 6.3669 - softmax_layer_CC_loss: 8.7142e-06 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5620 - val_decoder_dense_loss: 7.1574 - val_softmax_layer_CC_loss: 6.3217e-06 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 15/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4814 - decoder_dense_loss: 6.3915 - softmax_layer_CC_loss: 6.0851e-06 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5646 - val_decoder_dense_loss: 7.1834 - val_softmax_layer_CC_loss: 4.6694e-06 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 16/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4805 - decoder_dense_loss: 6.2848 - softmax_layer_CC_loss: 7.4359e-06 - softmax_layer_DC_loss: 15.3366 - val_loss: 0.5668 - val_decoder_dense_loss: 7.2061 - val_softmax_layer_CC_loss: 2.2957e-06 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 17/800\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 0.4799 - decoder_dense_loss: 6.3367 - softmax_layer_CC_loss: 2.1665e-06 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5696 - val_decoder_dense_loss: 7.2341 - val_softmax_layer_CC_loss: 1.7706e-06 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 18/800\n",
      "900/900 [==============================] - 5s 5ms/step - loss: 0.4791 - decoder_dense_loss: 6.3481 - softmax_layer_CC_loss: 1.7599e-06 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5717 - val_decoder_dense_loss: 7.2549 - val_softmax_layer_CC_loss: 1.1697e-06 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 19/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4785 - decoder_dense_loss: 6.3789 - softmax_layer_CC_loss: 1.3596e-06 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5734 - val_decoder_dense_loss: 7.2724 - val_softmax_layer_CC_loss: 1.0571e-06 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 20/800\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 0.4777 - decoder_dense_loss: 6.2969 - softmax_layer_CC_loss: 7.7207e-07 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5761 - val_decoder_dense_loss: 7.2994 - val_softmax_layer_CC_loss: 8.2761e-07 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 21/800\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 0.4771 - decoder_dense_loss: 6.3106 - softmax_layer_CC_loss: 7.6127e-07 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5771 - val_decoder_dense_loss: 7.3086 - val_softmax_layer_CC_loss: 6.6072e-07 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 22/800\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 0.4767 - decoder_dense_loss: 6.3337 - softmax_layer_CC_loss: 4.1330e-07 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5777 - val_decoder_dense_loss: 7.3146 - val_softmax_layer_CC_loss: 6.0409e-07 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 23/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4758 - decoder_dense_loss: 6.3027 - softmax_layer_CC_loss: 2.8833e-07 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5784 - val_decoder_dense_loss: 7.3220 - val_softmax_layer_CC_loss: 1.6242e-07 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 24/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4749 - decoder_dense_loss: 6.3532 - softmax_layer_CC_loss: 2.0079e-07 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5753 - val_decoder_dense_loss: 7.2907 - val_softmax_layer_CC_loss: 5.6118e-07 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 25/800\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.4743 - decoder_dense_loss: 6.2243 - softmax_layer_CC_loss: 1.7360e-07 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5810 - val_decoder_dense_loss: 7.3477 - val_softmax_layer_CC_loss: 1.6987e-08 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 26/800\n",
      "900/900 [==============================] - 5s 5ms/step - loss: 0.4734 - decoder_dense_loss: 6.2553 - softmax_layer_CC_loss: 7.6485e-08 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5827 - val_decoder_dense_loss: 7.3649 - val_softmax_layer_CC_loss: 5.6624e-09 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 27/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/900 [==============================] - 5s 6ms/step - loss: 0.4725 - decoder_dense_loss: 6.3215 - softmax_layer_CC_loss: 5.0233e-08 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5768 - val_decoder_dense_loss: 7.3054 - val_softmax_layer_CC_loss: 4.7088e-08 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 28/800\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 0.4716 - decoder_dense_loss: 6.2598 - softmax_layer_CC_loss: 3.4168e-08 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5795 - val_decoder_dense_loss: 7.3330 - val_softmax_layer_CC_loss: 8.0466e-09 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 29/800\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 0.4709 - decoder_dense_loss: 6.2089 - softmax_layer_CC_loss: 1.4057e-08 - softmax_layer_DC_loss: 15.3791 - val_loss: 0.5795 - val_decoder_dense_loss: 7.3325 - val_softmax_layer_CC_loss: 6.8545e-09 - val_softmax_layer_DC_loss: 15.3791\n",
      "Epoch 30/800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-401-c758265fe263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     epochs=800)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss=['categorical_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],\n",
    "             loss_weights=[0.1,0.6,-0.1])\n",
    "history = model.fit(encoder_input_data, \n",
    "                    [decoder_target_data,y_cc,y_dc],\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.1,\n",
    "                    shuffle=True,\n",
    "                    epochs=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(max_encoder_len,),name=\"encoder_inputs\")\n",
    "\n",
    "encoder_embedding = Embedding(vocab_len+1,100,trainable=True,weights=[embedding_matrix],input_length=max_encoder_len,mask_zero=True,name=\"encoder_embedding\")\n",
    "\n",
    "encoder_inputs = encoder_embedding(inputs)\n",
    "\n",
    "# encoder_inputs = LSTM(64,return_sequences=True)(encoder_inputs)\n",
    "encoder = LSTM(64,return_state=True)\n",
    "\n",
    "# encoder = Bidirectional(LSTM(64, \n",
    "#                     return_state=True, \n",
    "#                     name = 'encoder'))\n",
    "\n",
    "# encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# # We discard `encoder_outputs` and only keep the states.\n",
    "\n",
    "# encoder_states = [forward_h, backward_h]\n",
    "encoder_states = [state_h,state_c]\n",
    "\n",
    "# # Set up the decoder, using `encoder_states` as initial state.\n",
    "\n",
    "\n",
    "\n",
    "decoder_inputs = RepeatVector(max_encoder_len)(encoder_outputs)\n",
    "\n",
    "# # We set up our decoder to return full output sequences,\n",
    "# # and to return internal states as well. We don't use the\n",
    "# # return states in the training model, but we will use them in inference.\n",
    "\n",
    "decoder_lstm = LSTM(64, \n",
    "                         return_state=True,\n",
    "                        return_sequences=True,\n",
    "                         name = 'decoder_lstm')\n",
    "\n",
    "\n",
    "# # The inital_state call argument, specifying the initial state(s) of a RNN. \n",
    "# # This is used to pass the encoder states to the decoder as initial states.\n",
    "# # Basically making the first memory of the decoder the encoded semantics\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "# decoder_outputs = LSTM(128,return_sequences=True)(decoder_outputs)\n",
    "\n",
    "# decoder_outputs = Dense(64,activation=\"tanh\")(decoder_outputs)\n",
    "decoder_dense = Dense(vocab_len+1, \n",
    "                      activation='softmax', \n",
    "                      name = 'decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# # Define the model that will turn\n",
    "# # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model(inputs,decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, 10, 100)      1000200     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  [(None, 64), (None,  42240       encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_11 (RepeatVector) (None, 10, 64)       0           lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 10, 64), (No 33024       repeat_vector_11[0][0]           \n",
      "                                                                 lstm_13[0][1]                    \n",
      "                                                                 lstm_13[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, 10, 10002)    650130      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,725,594\n",
      "Trainable params: 1,725,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = test_generator(t_encoder_inputs,t_decoder_inputs,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [encoder_input_data,decoder_input_data],decoder_target_data = next(generator)\n",
    "encoder_input_data,decoder_target_data = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data,decoder_target_data = all_data_generator(t_encoder_inputs,t_decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  61.,  248.,  198., 2020., 9961.,   43.,   15., 1232.,   19.,\n",
       "        848.])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [encoder_input_data,decoder_input_data],decoder_target_data = all_data_generator(t_encoder_inputs,t_decoder_inputs)\n",
    "# encoder_input_data,decoder_target_data = all_data_generator(t_encoder_inputs,t_decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 952., 1064., 2305.,    8.,  269., 4031.,   35.,   13.,  136.,\n",
       "         12.])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnav1712/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 5.5762\n",
      "Epoch 2/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.6231\n",
      "Epoch 3/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.4759\n",
      "Epoch 4/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.4262\n",
      "Epoch 5/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.3857\n",
      "Epoch 6/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.3516\n",
      "Epoch 7/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.3192\n",
      "Epoch 8/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.2938\n",
      "Epoch 9/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.2795\n",
      "Epoch 10/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.2769\n",
      "Epoch 11/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.2260\n",
      "Epoch 12/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.1987\n",
      "Epoch 13/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.1743\n",
      "Epoch 14/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.1423\n",
      "Epoch 15/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.1176\n",
      "Epoch 16/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.0899\n",
      "Epoch 17/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.0734\n",
      "Epoch 18/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.0593\n",
      "Epoch 19/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.0497\n",
      "Epoch 20/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.0374\n",
      "Epoch 21/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.0038\n",
      "Epoch 22/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.9931\n",
      "Epoch 23/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.9699\n",
      "Epoch 24/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.9323\n",
      "Epoch 25/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.9030\n",
      "Epoch 26/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8771\n",
      "Epoch 27/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8558\n",
      "Epoch 28/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8397\n",
      "Epoch 29/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8313\n",
      "Epoch 30/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8334\n",
      "Epoch 31/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8259\n",
      "Epoch 32/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.8171\n",
      "Epoch 33/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.8136\n",
      "Epoch 34/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.7631\n",
      "Epoch 35/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.7331\n",
      "Epoch 36/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6886\n",
      "Epoch 37/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6647\n",
      "Epoch 38/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6441\n",
      "Epoch 39/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6293\n",
      "Epoch 40/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6206\n",
      "Epoch 41/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.6141\n",
      "Epoch 42/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6109\n",
      "Epoch 43/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.5859\n",
      "Epoch 44/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.5816\n",
      "Epoch 45/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.5601\n",
      "Epoch 46/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.5369\n",
      "Epoch 47/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.5132\n",
      "Epoch 48/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.4805\n",
      "Epoch 49/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.4695\n",
      "Epoch 50/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.4549\n",
      "Epoch 51/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.4498\n",
      "Epoch 52/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.4244\n",
      "Epoch 53/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.4071\n",
      "Epoch 54/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3786\n",
      "Epoch 55/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3609\n",
      "Epoch 56/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3427\n",
      "Epoch 57/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3304\n",
      "Epoch 58/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3232\n",
      "Epoch 59/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3044\n",
      "Epoch 60/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2819\n",
      "Epoch 61/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2567\n",
      "Epoch 62/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2369\n",
      "Epoch 63/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2325\n",
      "Epoch 64/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2310\n",
      "Epoch 65/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2373\n",
      "Epoch 66/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2002\n",
      "Epoch 67/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.1828\n",
      "Epoch 68/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.1750\n",
      "Epoch 69/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.1797\n",
      "Epoch 70/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.1217\n",
      "Epoch 71/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0942\n",
      "Epoch 72/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0746\n",
      "Epoch 73/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0637\n",
      "Epoch 74/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0616\n",
      "Epoch 75/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0608\n",
      "Epoch 76/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0515\n",
      "Epoch 77/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0206\n",
      "Epoch 78/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9977\n",
      "Epoch 79/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9717\n",
      "Epoch 80/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9610\n",
      "Epoch 81/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9422\n",
      "Epoch 82/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9290\n",
      "Epoch 83/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9128\n",
      "Epoch 84/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9027\n",
      "Epoch 85/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8886\n",
      "Epoch 86/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8849\n",
      "Epoch 87/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8892\n",
      "Epoch 88/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8816\n",
      "Epoch 89/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8570\n",
      "Epoch 90/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8387\n",
      "Epoch 91/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8349\n",
      "Epoch 92/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8211\n",
      "Epoch 93/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8198\n",
      "Epoch 94/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.7776\n",
      "Epoch 95/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.7511\n",
      "Epoch 96/800\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 3.7408\n",
      "Epoch 97/800\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 3.7292\n",
      "Epoch 98/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.7227\n",
      "Epoch 99/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6748\n",
      "Epoch 101/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6589\n",
      "Epoch 102/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6480\n",
      "Epoch 103/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6429\n",
      "Epoch 104/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6365\n",
      "Epoch 105/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6311\n",
      "Epoch 106/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6160\n",
      "Epoch 107/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6111\n",
      "Epoch 108/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.5937\n",
      "Epoch 109/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.5853\n",
      "Epoch 110/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.5583\n",
      "Epoch 111/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.5391\n",
      "Epoch 112/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.5095\n",
      "Epoch 113/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.4937\n",
      "Epoch 114/800\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 3.4836\n",
      "Epoch 115/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4808\n",
      "Epoch 116/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4796\n",
      "Epoch 117/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.4770\n",
      "Epoch 118/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4561\n",
      "Epoch 119/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4266\n",
      "Epoch 120/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4163\n",
      "Epoch 121/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4123\n",
      "Epoch 122/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4310\n",
      "Epoch 123/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.4254\n",
      "Epoch 124/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3796\n",
      "Epoch 125/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3482\n",
      "Epoch 126/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3348\n",
      "Epoch 127/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3318\n",
      "Epoch 128/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3430\n",
      "Epoch 129/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3298\n",
      "Epoch 130/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3103\n",
      "Epoch 131/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2786\n",
      "Epoch 132/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2556\n",
      "Epoch 133/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2444\n",
      "Epoch 134/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2360\n",
      "Epoch 135/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2307\n",
      "Epoch 136/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2260\n",
      "Epoch 137/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.2195\n",
      "Epoch 138/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.2111\n",
      "Epoch 139/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.1936\n",
      "Epoch 140/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1853\n",
      "Epoch 141/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1740\n",
      "Epoch 142/800\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 3.1593\n",
      "Epoch 143/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.1418\n",
      "Epoch 144/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1272\n",
      "Epoch 145/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1056\n",
      "Epoch 146/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0952\n",
      "Epoch 147/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0854\n",
      "Epoch 148/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1039\n",
      "Epoch 149/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1005\n",
      "Epoch 150/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1031\n",
      "Epoch 151/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0516\n",
      "Epoch 152/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0335\n",
      "Epoch 153/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0118\n",
      "Epoch 154/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0007\n",
      "Epoch 155/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.9908\n",
      "Epoch 156/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9842\n",
      "Epoch 157/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.9706\n",
      "Epoch 158/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.9634\n",
      "Epoch 159/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.9616\n",
      "Epoch 160/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9718\n",
      "Epoch 161/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9678\n",
      "Epoch 162/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9491\n",
      "Epoch 163/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9195\n",
      "Epoch 164/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9087\n",
      "Epoch 165/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9014\n",
      "Epoch 166/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8880\n",
      "Epoch 167/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8680\n",
      "Epoch 168/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8436\n",
      "Epoch 169/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8295\n",
      "Epoch 170/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8149\n",
      "Epoch 171/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.8100\n",
      "Epoch 172/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8092\n",
      "Epoch 173/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8295\n",
      "Epoch 174/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8491\n",
      "Epoch 175/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8433\n",
      "Epoch 176/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7911\n",
      "Epoch 177/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7523\n",
      "Epoch 178/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7332\n",
      "Epoch 179/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7228\n",
      "Epoch 180/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7218\n",
      "Epoch 181/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7203\n",
      "Epoch 182/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7278\n",
      "Epoch 183/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7273\n",
      "Epoch 184/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7036\n",
      "Epoch 185/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.6807\n",
      "Epoch 186/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.6663\n",
      "Epoch 187/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.6721\n",
      "Epoch 188/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.6802\n",
      "Epoch 189/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.6781\n",
      "Epoch 190/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.6691\n",
      "Epoch 191/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.6326\n",
      "Epoch 192/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.6069\n",
      "Epoch 193/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5874\n",
      "Epoch 194/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5796\n",
      "Epoch 195/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5760\n",
      "Epoch 196/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5848\n",
      "Epoch 197/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5789\n",
      "Epoch 199/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5534\n",
      "Epoch 200/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5274\n",
      "Epoch 201/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5099\n",
      "Epoch 202/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5082\n",
      "Epoch 203/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5279\n",
      "Epoch 204/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5599\n",
      "Epoch 205/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5306\n",
      "Epoch 206/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4918\n",
      "Epoch 207/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4672\n",
      "Epoch 208/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4654\n",
      "Epoch 209/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4676\n",
      "Epoch 210/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4615\n",
      "Epoch 211/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4415\n",
      "Epoch 212/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4180\n",
      "Epoch 213/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4111\n",
      "Epoch 214/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4082\n",
      "Epoch 215/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.4236\n",
      "Epoch 216/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4115\n",
      "Epoch 217/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3996\n",
      "Epoch 218/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.3725\n",
      "Epoch 219/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3587\n",
      "Epoch 220/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.3536\n",
      "Epoch 221/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.3472\n",
      "Epoch 222/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3514\n",
      "Epoch 223/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3467\n",
      "Epoch 224/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3545\n",
      "Epoch 225/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3424\n",
      "Epoch 226/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3179\n",
      "Epoch 227/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3019\n",
      "Epoch 228/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2883\n",
      "Epoch 229/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2865\n",
      "Epoch 230/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2762\n",
      "Epoch 231/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2821\n",
      "Epoch 232/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2769\n",
      "Epoch 233/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2635\n",
      "Epoch 234/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2471\n",
      "Epoch 235/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2424\n",
      "Epoch 236/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2494\n",
      "Epoch 237/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2495\n",
      "Epoch 238/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2422\n",
      "Epoch 239/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2225\n",
      "Epoch 240/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2082\n",
      "Epoch 241/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1925\n",
      "Epoch 242/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1794\n",
      "Epoch 243/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1703\n",
      "Epoch 244/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1564\n",
      "Epoch 245/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1514\n",
      "Epoch 246/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1443\n",
      "Epoch 247/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1437\n",
      "Epoch 248/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1322\n",
      "Epoch 249/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1254\n",
      "Epoch 250/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1169\n",
      "Epoch 251/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1198\n",
      "Epoch 252/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1222\n",
      "Epoch 253/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1324\n",
      "Epoch 254/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1494\n",
      "Epoch 255/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1094\n",
      "Epoch 256/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.0847\n",
      "Epoch 257/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.0613\n",
      "Epoch 258/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0606\n",
      "Epoch 259/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0735\n",
      "Epoch 260/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0709\n",
      "Epoch 261/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0627\n",
      "Epoch 262/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0462\n",
      "Epoch 263/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0405\n",
      "Epoch 264/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0364\n",
      "Epoch 265/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0332\n",
      "Epoch 266/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0209\n",
      "Epoch 267/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0076\n",
      "Epoch 268/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 1.9915\n",
      "Epoch 269/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9856\n",
      "Epoch 270/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9715\n",
      "Epoch 271/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9688\n",
      "Epoch 272/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9618\n",
      "Epoch 273/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9659\n",
      "Epoch 274/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9677\n",
      "Epoch 275/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9843\n",
      "Epoch 276/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0023\n",
      "Epoch 277/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9676\n",
      "Epoch 278/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9332\n",
      "Epoch 279/800\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 1.9128\n",
      "Epoch 280/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 1.9079\n",
      "Epoch 281/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9169\n",
      "Epoch 282/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9322\n",
      "Epoch 283/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9456\n",
      "Epoch 284/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9228\n",
      "Epoch 285/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8953\n",
      "Epoch 286/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8738\n",
      "Epoch 287/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8728\n",
      "Epoch 288/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8628\n",
      "Epoch 289/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8645\n",
      "Epoch 290/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 1.8569\n",
      "Epoch 291/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8582\n",
      "Epoch 292/800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-b36b4a4925a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mdecoder_target_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     epochs=800)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#                     validation_split=0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "history = model.fit(encoder_input_data, \n",
    "                    decoder_target_data,\n",
    "                    batch_size=128,\n",
    "                    epochs=800)\n",
    "#                     validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rumour meghan markle had a secret first husband where "
     ]
    }
   ],
   "source": [
    "for idx in encoder_input_data[25]:\n",
    "    print(idx2word[idx],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict([encoder_input_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the one had had secret secret husband where where "
     ]
    }
   ],
   "source": [
    "for idx in output_array[25]:\n",
    "    lookup = np.argmax(idx)\n",
    "#     print(lookup)\n",
    "    if lookup==0:\n",
    "        break\n",
    "    else:\n",
    "        print(idx2word[lookup],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9437695e-07\n"
     ]
    }
   ],
   "source": [
    "for idx in output_array[100]:\n",
    "    print(idx[np.argmax(20)])\n",
    "    break\n",
    "    lookup = np.argmax(idx)\n",
    "#     print(lookup)\n",
    "    if lookup==0:\n",
    "        break\n",
    "    else:\n",
    "        print(idx2word[lookup],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Social",
   "language": "python",
   "name": "social"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

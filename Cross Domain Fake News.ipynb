{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, Dense,Embedding,RepeatVector\n",
    "from keras.models import Model\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "# import demoji\n",
    "import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain1_path = \"./dEFEND/gossipcop_content_no_ignore.tsv\"\n",
    "domain2_path = \"./dEFEND/politifact_content_no_ignore.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain1_frame = pd.read_csv(domain1_path,delimiter=\"\\t\").set_index('id')\n",
    "domain2_frame = pd.read_csv(domain2_path,delimiter=\"\\t\").set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts for each label for the 2 domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gossipcop-9096198130</th>\n",
       "      <td>1</td>\n",
       "      <td>Sarah Jessica Parker is getting candid about h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-6982710185</th>\n",
       "      <td>1</td>\n",
       "      <td>Many celebrities have been sharing their thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-7887456921</th>\n",
       "      <td>1</td>\n",
       "      <td>He reportedly hasn't seen her in over four yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-1594778479</th>\n",
       "      <td>1</td>\n",
       "      <td>The fashion crowd is speaking out about Kim Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-8172018375</th>\n",
       "      <td>1</td>\n",
       "      <td>What term do you want to search? Search with g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-854842</th>\n",
       "      <td>0</td>\n",
       "      <td>Aisha Tyler‘s divorce from Jeffrey Tietjens ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-843491</th>\n",
       "      <td>0</td>\n",
       "      <td>All four of Queen Elizabeth and Prince Philip'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-897778</th>\n",
       "      <td>0</td>\n",
       "      <td>Theresa Caputo is adjusting to her new life af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-899849</th>\n",
       "      <td>0</td>\n",
       "      <td>Follow Us on Twitter  Nominations for the 25th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-927093</th>\n",
       "      <td>0</td>\n",
       "      <td>Though the happy couple can’t be together in p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5816 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label                                            content\n",
       "id                                                                            \n",
       "gossipcop-9096198130      1  Sarah Jessica Parker is getting candid about h...\n",
       "gossipcop-6982710185      1  Many celebrities have been sharing their thoug...\n",
       "gossipcop-7887456921      1  He reportedly hasn't seen her in over four yea...\n",
       "gossipcop-1594778479      1  The fashion crowd is speaking out about Kim Ka...\n",
       "gossipcop-8172018375      1  What term do you want to search? Search with g...\n",
       "...                     ...                                                ...\n",
       "gossipcop-854842          0  Aisha Tyler‘s divorce from Jeffrey Tietjens ha...\n",
       "gossipcop-843491          0  All four of Queen Elizabeth and Prince Philip'...\n",
       "gossipcop-897778          0  Theresa Caputo is adjusting to her new life af...\n",
       "gossipcop-899849          0  Follow Us on Twitter  Nominations for the 25th...\n",
       "gossipcop-927093          0  Though the happy couple can’t be together in p...\n",
       "\n",
       "[5816 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain1_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "label       \n",
       "0       3586\n",
       "1       2230"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain1_frame.groupby(['label'])[['label']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "label       \n",
       "0        145\n",
       "1        270"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain2_frame.groupby(['label'])[['label']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[()\\\"_#/@;*%:<>{}`+=~|.!?,'$-\\[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"[0-9]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(decoder_input_sentence):\n",
    "    start = \"<BOS> \"\n",
    "    end = \" <EOS>\"\n",
    "    final_target = [start + text + end for text in decoder_input_sentence] \n",
    "    return final_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenizer(text_lists):\n",
    "#     return [line.split(\" \") for line in text_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(text_lists):\n",
    "    tokenizer = Tokenizer(num_words=20000,oov_token=\"<UNK>\")\n",
    "    tokenizer.fit_on_texts(text_lists)\n",
    "    \n",
    "    dictionary = tokenizer.word_index\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k,v in dictionary.items():\n",
    "        word2idx[k]=v\n",
    "        idx2word[v]=k\n",
    "    \n",
    "    return word2idx,idx2word,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = domain1_frame['content'].apply(lambda x: clean_text(x)).values.tolist()+ domain2_frame['content'].apply(lambda x: clean_text(x)).values.tolist()\n",
    "# encoder_inputs = \n",
    "decoder_inputs = tagger(encoder_inputs[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx,idx2word,tokenizer = create_vocab(encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to ambiguity with regards to Keras Tokenizer num_words, below is a good enough fix, though it changes the tokenizer word_index outside of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5000\n",
    "\n",
    "sorted_by_word_count = sorted(tokenizer.word_counts.items(), key=lambda kv: kv[1], reverse=True)\n",
    "tokenizer.word_index = {}\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "i = 0\n",
    "for word,count in sorted_by_word_count:\n",
    "    if i == num_words:\n",
    "        break\n",
    "    tokenizer.word_index[word] = i + 1    # <= because tokenizer is 1 indexed\n",
    "    word2idx[word] = i+1\n",
    "    idx2word[i+1]=word\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index[tokenizer.oov_token] = num_words+1\n",
    "word2idx[tokenizer.oov_token] = num_words+1\n",
    "idx2word[num_words+1]=tokenizer.oov_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t_encoder_inputs = tokenizer.texts_to_sequences(encoder_inputs)\n",
    "t_decoder_inputs = tokenizer.texts_to_sequences(decoder_inputs)\n",
    "\n",
    "t_encoder_inputs = pad_sequences(t_encoder_inputs,maxlen=50)\n",
    "\n",
    "max_encoder_len = max([len(val) for val in t_encoder_inputs])\n",
    "max_decoder_len = max([len(val) for val in t_decoder_inputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 952, 1064, 2305,  452,  340, 2960,  744,  881,    3,  420, 2256,\n",
       "          6,  574,    2,    1,  275,  248, 4427,  123,  952, 1064, 2305,\n",
       "        124,   21,   45, 5001,   80,  574,    2,    1,  275, 5001,  952,\n",
       "       1064, 2305,    2, 5001, 5001, 4601,  250,  248, 4427,  574,    2,\n",
       "          1,  275,  398, 1167,  353, 1613], dtype=int32)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_encoder_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(X,Y,batch_size=128,max_len=5):\n",
    "    \n",
    "        \n",
    "    \n",
    "    for idx in range(0,len(X),batch_size):\n",
    "\n",
    "        encoder_input = np.zeros((batch_size,max_encoder_len))\n",
    "#         decoder_input = np.zeros((batch_size,max_decoder_len))\n",
    "        decoder_target = np.zeros((batch_size,max_encoder_len,len(word2idx)+1))\n",
    "        for j,input_seq in enumerate(X[idx:idx+batch_size]):\n",
    "            for i,word_idx in enumerate(input_seq):\n",
    "                encoder_input[j,i]= word_idx\n",
    "                decoder_target[j,i,word_idx] = 1\n",
    "           \n",
    "        yield [encoder_input,decoder_target]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data_generator(X,Y):\n",
    "    encoder_input = np.zeros((len(X),max_encoder_len))\n",
    "    decoder_input = np.zeros((len(X),max_decoder_len))\n",
    "    decoder_target = np.zeros((len(X),max_encoder_len,len(word2idx)+1))\n",
    "    for j,(input_seq,target_seq) in enumerate(zip(X,Y)):\n",
    "        for i,word_idx in enumerate(input_seq):\n",
    "            encoder_input[j,i]= word_idx\n",
    "            decoder_target[j,i,word_idx] = 1\n",
    "\n",
    "#         for i,word_idx in enumerate(target_seq):\n",
    "#             decoder_input[j,i] = word_idx-1\n",
    "\n",
    "#             if i>0:\n",
    "#                 decoder_target[j,i-1,word_idx-1] = 1\n",
    "    \n",
    "#     return [[encoder_input,decoder_input],decoder_target]\n",
    "    return [encoder_input,decoder_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = test_generator(t_encoder_inputs,t_decoder_inputs,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len= len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5001"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 50)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generator)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6231"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5001"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5001"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(max_encoder_len,),name=\"encoder_inputs\")\n",
    "\n",
    "encoder_embedding = Embedding(vocab_len+1,100,input_length=max_encoder_len,name=\"encoder_embedding\")\n",
    "\n",
    "encoder_inputs = encoder_embedding(inputs)\n",
    "\n",
    "\n",
    "\n",
    "encoder = LSTM(128, \n",
    "                    return_state=True, \n",
    "                    name = 'encoder')\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# # We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# # Set up the decoder, using `encoder_states` as initial state.\n",
    "\n",
    "\n",
    "\n",
    "decoder_inputs = RepeatVector(max_encoder_len)(encoder_outputs)\n",
    "\n",
    "# # We set up our decoder to return full output sequences,\n",
    "# # and to return internal states as well. We don't use the\n",
    "# # return states in the training model, but we will use them in inference.\n",
    "\n",
    "decoder_lstm = LSTM(128, \n",
    "                         return_state=True,\n",
    "                        return_sequences=True,\n",
    "                         name = 'decoder_lstm')\n",
    "\n",
    "# # The inital_state call argument, specifying the initial state(s) of a RNN. \n",
    "# # This is used to pass the encoder states to the decoder as initial states.\n",
    "# # Basically making the first memory of the decoder the encoded semantics\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(vocab_len+1, \n",
    "                      activation='softmax', \n",
    "                      name = 'decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# # Define the model that will turn\n",
    "# # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model(inputs,decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, 50, 100)      500200      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder (LSTM)                  [(None, 128), (None, 117248      encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_12 (RepeatVector) (None, 50, 128)      0           encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 50, 128), (N 131584      repeat_vector_12[0][0]           \n",
      "                                                                 encoder[0][1]                    \n",
      "                                                                 encoder[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, 50, 5002)     645258      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,394,290\n",
      "Trainable params: 1,394,290\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in ./venv/lib/python3.7/site-packages (1.2.3)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in ./venv/lib/python3.7/site-packages (from pydot) (2.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-522-819c3bda4aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/University/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         raise ImportError(\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = test_generator(t_encoder_inputs,t_decoder_inputs,batch_size=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [encoder_input_data,decoder_input_data],decoder_target_data = next(generator)\n",
    "encoder_input_data,decoder_target_data = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 50)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.248e+03, 8.200e+01, 1.380e+02, 4.800e+01, 4.530e+02, 3.000e+00,\n",
       "       1.210e+02, 1.600e+01, 4.600e+01, 5.000e+00, 1.821e+03, 2.030e+03,\n",
       "       5.000e+00, 5.460e+02, 2.100e+01, 4.500e+01, 5.001e+03, 1.500e+01,\n",
       "       8.800e+01, 2.015e+03, 9.000e+00, 6.490e+02, 2.985e+03, 1.740e+03,\n",
       "       2.100e+01, 4.500e+01, 5.001e+03, 6.300e+01, 9.000e+00, 5.001e+03,\n",
       "       3.000e+00, 1.150e+03, 5.001e+03, 3.000e+00, 2.853e+03, 4.600e+01,\n",
       "       5.001e+03, 4.300e+01, 1.600e+01, 5.590e+02, 5.001e+03, 3.840e+02,\n",
       "       1.250e+02, 5.200e+01, 3.550e+02, 3.787e+03, 3.000e+00, 2.576e+03,\n",
       "       5.001e+03, 4.930e+02])"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarah jessica parker wants sexual beast ellen degeneres to play samantha in sex and the city kim cattrall says sarah jessica parker could have been <UNK> over sex and the city <UNK> sarah jessica parker and <UNK> <UNK> reunite following kim cattrall sex and the city drama pics related gallery "
     ]
    }
   ],
   "source": [
    "for idx in encoder_input_data[0]:\n",
    "    print(idx2word[idx],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict([encoder_input_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 50, 5002)"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarah jessica parker wants sexual beast ellen degeneres to play samantha in sex and the city kim cattrall says sarah jessica parker could have been <UNK> over sex and the city <UNK> sarah jessica parker and <UNK> <UNK> reunite following kim cattrall sex and the city drama pics related gallery "
     ]
    }
   ],
   "source": [
    "for idx in decoder_target_data[0]:\n",
    "    lookup = np.argmax(idx)\n",
    "#     print(lookup)\n",
    "    if lookup==0:\n",
    "        break\n",
    "    else:\n",
    "        print(idx2word[lookup],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007085485\n"
     ]
    }
   ],
   "source": [
    "for idx in output_array[100]:\n",
    "    print(idx[np.argmax(20)])\n",
    "    break\n",
    "    lookup = np.argmax(idx)\n",
    "#     print(lookup)\n",
    "    if lookup==0:\n",
    "        break\n",
    "    else:\n",
    "        print(idx2word[lookup],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [encoder_input_data,decoder_input_data],decoder_target_data = all_data_generator(t_encoder_inputs,t_decoder_inputs)\n",
    "encoder_input_data,decoder_target_data = all_data_generator(t_encoder_inputs,t_decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6231, 50, 5002)"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnav/Desktop/University/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "6231/6231 [==============================] - 75s 12ms/step - loss: 6.2222\n",
      "Epoch 2/4\n",
      "6231/6231 [==============================] - 75s 12ms/step - loss: 6.0389\n",
      "Epoch 3/4\n",
      "6231/6231 [==============================] - 79s 13ms/step - loss: 5.9325\n",
      "Epoch 4/4\n",
      "6231/6231 [==============================] - 76s 12ms/step - loss: 5.8496\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "history = model.fit(encoder_input_data, \n",
    "                    decoder_target_data,\n",
    "                    epochs=4)\n",
    "#                     validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, 1), \n",
    "                       name = 'encoder_inputs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMM",
   "language": "python",
   "name": "smm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, Dense,Embedding,RepeatVector,Bidirectional\n",
    "from keras.models import Model\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "# import demoji\n",
    "import tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_list = list(set(stopwords.words('english')))\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain1_path = \"./dEFEND/gossipcop_content_no_ignore.tsv\"\n",
    "domain2_path = \"./dEFEND/politifact_content_no_ignore.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain1_frame = pd.read_csv(domain1_path,delimiter=\"\\t\").set_index('id')\n",
    "domain2_frame = pd.read_csv(domain2_path,delimiter=\"\\t\").set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts for each label for the 2 domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gossipcop-9096198130</th>\n",
       "      <td>1</td>\n",
       "      <td>Sarah Jessica Parker is getting candid about h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-6982710185</th>\n",
       "      <td>1</td>\n",
       "      <td>Many celebrities have been sharing their thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-7887456921</th>\n",
       "      <td>1</td>\n",
       "      <td>He reportedly hasn't seen her in over four yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-1594778479</th>\n",
       "      <td>1</td>\n",
       "      <td>The fashion crowd is speaking out about Kim Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-8172018375</th>\n",
       "      <td>1</td>\n",
       "      <td>What term do you want to search? Search with g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-854842</th>\n",
       "      <td>0</td>\n",
       "      <td>Aisha Tyler‘s divorce from Jeffrey Tietjens ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-843491</th>\n",
       "      <td>0</td>\n",
       "      <td>All four of Queen Elizabeth and Prince Philip'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-897778</th>\n",
       "      <td>0</td>\n",
       "      <td>Theresa Caputo is adjusting to her new life af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-899849</th>\n",
       "      <td>0</td>\n",
       "      <td>Follow Us on Twitter  Nominations for the 25th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gossipcop-927093</th>\n",
       "      <td>0</td>\n",
       "      <td>Though the happy couple can’t be together in p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5816 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label                                            content\n",
       "id                                                                            \n",
       "gossipcop-9096198130      1  Sarah Jessica Parker is getting candid about h...\n",
       "gossipcop-6982710185      1  Many celebrities have been sharing their thoug...\n",
       "gossipcop-7887456921      1  He reportedly hasn't seen her in over four yea...\n",
       "gossipcop-1594778479      1  The fashion crowd is speaking out about Kim Ka...\n",
       "gossipcop-8172018375      1  What term do you want to search? Search with g...\n",
       "...                     ...                                                ...\n",
       "gossipcop-854842          0  Aisha Tyler‘s divorce from Jeffrey Tietjens ha...\n",
       "gossipcop-843491          0  All four of Queen Elizabeth and Prince Philip'...\n",
       "gossipcop-897778          0  Theresa Caputo is adjusting to her new life af...\n",
       "gossipcop-899849          0  Follow Us on Twitter  Nominations for the 25th...\n",
       "gossipcop-927093          0  Though the happy couple can’t be together in p...\n",
       "\n",
       "[5816 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain1_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "label       \n",
       "0       3586\n",
       "1       2230"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain1_frame.groupby(['label'])[['label']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "label       \n",
       "0        145\n",
       "1        270"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain2_frame.groupby(['label'])[['label']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[()\\\"_#/@;*%:{}<>`+=~|.!?,'$-\\[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"[0-9]\", \"\", text)\n",
    "    \n",
    "#     for words in stopwords_list:\n",
    "#         text = re.sub(r\"\\b{}\\b\".format(words),\"\",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(decoder_input_sentence):\n",
    "    start = \"<BOS> \"\n",
    "    end = \" <EOS>\"\n",
    "    final_target = [start + text + end for text in decoder_input_sentence] \n",
    "    return final_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenizer(text_lists):\n",
    "#     return [line.split(\" \") for line in text_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(text_lists):\n",
    "    tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "    tokenizer.fit_on_texts(text_lists)\n",
    "    \n",
    "    dictionary = tokenizer.word_index\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k,v in dictionary.items():\n",
    "        word2idx[k]=v\n",
    "        idx2word[v]=k\n",
    "    \n",
    "    return word2idx,idx2word,tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = domain1_frame['content'].apply(lambda x: clean_text(x)).values.tolist()+ domain2_frame['content'].apply(lambda x: clean_text(x)).values.tolist()\n",
    "# encoder_inputs = \n",
    "decoder_inputs = tagger(encoder_inputs[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake news detection training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CC = domain1_frame['content'].apply(lambda x: clean_text(x)).values.tolist()+ domain2_frame['content'].apply(lambda x: clean_text(x)).values.tolist()\n",
    "Y_CC = np.array(domain1_frame['label'].apply(lambda x:int(x)).values.tolist()+ domain2_frame['label'].apply(lambda x:int(x)).values.tolist())\n",
    "Y_CC_oh = to_categorical(Y_CC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DC = domain1_frame['content'].apply(lambda x: clean_text(x)).values.tolist()+ domain2_frame['content'].apply(lambda x: clean_text(x)).values.tolist()\n",
    "Y_DC = np.array([0]*len(domain1_frame) + [1]*len(domain2_frame))\n",
    "Y_DC_oh = to_categorical(Y_DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(encoder_inputs))\n",
    "np.random.shuffle(idx)\n",
    "encoder_inputs = np.array(encoder_inputs)[idx]\n",
    "decodeer_inputs = np.array(decoder_inputs)[idx]\n",
    "Y_CC = Y_CC[idx]\n",
    "Y_DC = Y_DC[idx]\n",
    "Y_CC_oh = Y_CC_oh[idx]\n",
    "Y_DC_oh = Y_DC_oh[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-8189b08258f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx,idx2word,tokenizer = create_vocab(encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to ambiguity with regards to Keras Tokenizer num_words, below is a good enough fix, though it changes the tokenizer word_index outside of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "\n",
    "sorted_by_word_count = sorted(tokenizer.word_counts.items(), key=lambda kv: kv[1], reverse=True)\n",
    "tokenizer.word_index = {}\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "i = 0\n",
    "for word,count in sorted_by_word_count:\n",
    "    if i == num_words:\n",
    "        break\n",
    "\n",
    "    tokenizer.word_index[word] = i + 1    # <= because tokenizer is 1 indexed\n",
    "    word2idx[word] = i+1\n",
    "    idx2word[i+1]=word\n",
    "    i += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index[tokenizer.oov_token] = num_words+1\n",
    "word2idx[tokenizer.oov_token] = num_words+1\n",
    "idx2word[num_words+1]=tokenizer.oov_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = len(tokenizer.word_index)+1\n",
    "# tokenizer.word_index[\"<BOS>\"] = idx\n",
    "# word2idx[\"<BOS>\"] = idx\n",
    "# idx2word[idx] = \"<BOS>\"\n",
    "\n",
    "# idx = len(tokenizer.word_index)+1\n",
    "# tokenizer.word_index[\"<EOS>\"] = idx\n",
    "# word2idx[\"<EOS>\"] = idx\n",
    "# idx2word[idx] = \"<EOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t_encoder_inputs = tokenizer.texts_to_sequences(encoder_inputs)\n",
    "t_decoder_inputs = tokenizer.texts_to_sequences(decoder_inputs)\n",
    "\n",
    "\n",
    "# \n",
    "t_encoder_inputs = pad_sequences(t_encoder_inputs,maxlen=10,padding='post', truncating='post')\n",
    "\n",
    "# t_decoder_inputs = pad_sequences(t_encoder_inputs,maxlen=100,padding='post', truncating='post')\n",
    "# t_decoder_inputs = np.insert(t_decoder_inputs,0,word2idx[\"<BOS>\"],axis=1)\n",
    "# t_decoder_inputs = np.insert(t_decoder_inputs,t_decoder_inputs.shape[1],word2idx[\"<EOS>\"],axis=1)\n",
    "\n",
    "\n",
    "max_encoder_len = max([len(val) for val in t_encoder_inputs])\n",
    "max_decoder_len = max([len(val) for val in t_decoder_inputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(X,Y,Y_CC,Y_DC,batch_size=128,max_len=5):\n",
    "    \n",
    "        \n",
    "    y_cc = to_categorical(Y_CC)\n",
    "    y_dc = to_categorical(Y_DC)\n",
    "    for idx in range(0,len(X),batch_size):\n",
    "\n",
    "        encoder_input = np.zeros((batch_size,max_encoder_len))\n",
    "#         decoder_input = np.zeros((batch_size,max_decoder_len))\n",
    "        decoder_target = np.zeros((batch_size,max_encoder_len,len(word2idx)+1))\n",
    "        for j,input_seq in enumerate(X[idx:idx+batch_size]):\n",
    "            for i,word_idx in enumerate(input_seq):\n",
    "                encoder_input[j,i]= word_idx\n",
    "                decoder_target[j,i,word_idx] = 1\n",
    "           \n",
    "        yield [encoder_input,[decoder_target,y_cc[idx:idx+batch_size],y_dc[idx:idx+batch_size]]]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data_generator(X,Y,Y_CC,Y_DC):\n",
    "    encoder_input = np.zeros((len(X),max_encoder_len))\n",
    "    decoder_input = np.zeros((len(X),max_decoder_len))\n",
    "    decoder_target = np.zeros((len(X),max_encoder_len,len(word2idx)+1)) ## Extra index for padding, word2idx is 1 indexed\n",
    "    for j,(input_seq,target_seq) in enumerate(zip(X,Y)):\n",
    "        for i,word_idx in enumerate(input_seq):\n",
    "            encoder_input[j,i]= word_idx\n",
    "            decoder_target[j,i,word_idx] = 1\n",
    "    \n",
    "    y_cc = to_categorical(Y_CC)\n",
    "    y_dc = to_categorical(Y_DC)\n",
    "\n",
    "    return [encoder_input,[decoder_target,y_cc,y_dc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len= len(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Glove Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_len+1, 100))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i+1] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(max_encoder_len,),name=\"encoder_inputs\")\n",
    "\n",
    "encoder_embedding = Embedding(vocab_len+1,100,trainable=True,input_length=max_encoder_len,mask_zero=True,name=\"encoder_embedding\")\n",
    "\n",
    "encoder_inputs = encoder_embedding(inputs)\n",
    "\n",
    "\n",
    "encoder = LSTM(64,return_state=True)\n",
    "\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "\n",
    "encoder_states = [state_h,state_c]\n",
    "\n",
    "\n",
    "########## Content Classification Part ###############\n",
    "\n",
    "fully_connected_CC = Dense(128,\n",
    "                       activation=\"tanh\",\n",
    "                       name=\"non_linear_CC\")\n",
    "\n",
    "logits_CC = fully_connected_CC(encoder_outputs)\n",
    "\n",
    "softmax_layer_CC = Dense(2,\n",
    "                     activation=\"softmax\",\n",
    "                     name=\"softmax_layer_CC\")\n",
    "\n",
    "output_CC = softmax_layer_CC(logits_CC)\n",
    "\n",
    "\n",
    "######### Domain Classification Part ##############\n",
    "\n",
    "fully_connected_DC = Dense(128,\n",
    "                       activation=\"tanh\",\n",
    "                       name=\"non_linear_DC\")\n",
    "\n",
    "logits_DC = fully_connected_DC(encoder_outputs)\n",
    "\n",
    "softmax_layer_DC = Dense(2,\n",
    "                     activation=\"softmax\",\n",
    "                     name=\"softmax_layer_DC\")\n",
    "\n",
    "output_DC = softmax_layer_DC(logits_DC)\n",
    "\n",
    "\n",
    "########### Autoencoder PART #############\n",
    "decoder_inputs = RepeatVector(max_encoder_len)(encoder_outputs)\n",
    "\n",
    "\n",
    "decoder_lstm = LSTM(64, \n",
    "                         return_state=True,\n",
    "                        return_sequences=True,\n",
    "                         name = 'decoder_lstm')\n",
    "\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "\n",
    "\n",
    "decoder_dense = Dense(vocab_len+1, \n",
    "                      activation='softmax', \n",
    "                      name = 'decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs,[decoder_outputs,output_CC,output_DC])\n",
    "# model = Model(inputs,output_CC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, 10, 100)      1000200     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, 64), (None,  42240       encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 10, 64)       0           lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 10, 64), (No 33024       repeat_vector_7[0][0]            \n",
      "                                                                 lstm_7[0][1]                     \n",
      "                                                                 lstm_7[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "non_linear_CC (Dense)           (None, 128)          8320        lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "non_linear_DC (Dense)           (None, 128)          8320        lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, 10, 10002)    650130      decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "softmax_layer_CC (Dense)        (None, 2)            258         non_linear_CC[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax_layer_DC (Dense)        (None, 2)            258         non_linear_DC[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,742,750\n",
      "Trainable params: 1,742,750\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"547pt\" viewBox=\"0.00 0.00 583.94 410.00\" width=\"779pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(0.75 0.75) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-406 579.94,-406 579.94,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 5671532584 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>5671532584</title>\n",
       "<polygon fill=\"none\" points=\"180.93,-365.5 180.93,-401.5 352.04,-401.5 352.04,-365.5 180.93,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.48\" y=\"-379.3\">encoder_inputs: InputLayer</text>\n",
       "</g>\n",
       "<!-- 5671534376 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>5671534376</title>\n",
       "<polygon fill=\"none\" points=\"165.76,-292.5 165.76,-328.5 367.21,-328.5 367.21,-292.5 165.76,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.48\" y=\"-306.3\">encoder_embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 5671532584&#45;&gt;5671534376 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>5671532584-&gt;5671534376</title>\n",
       "<path d=\"M266.48,-365.31C266.48,-357.29 266.48,-347.55 266.48,-338.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"269.98,-338.53 266.48,-328.53 262.98,-338.53 269.98,-338.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5671534152 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>5671534152</title>\n",
       "<polygon fill=\"none\" points=\"217.06,-219.5 217.06,-255.5 315.91,-255.5 315.91,-219.5 217.06,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.48\" y=\"-233.3\">lstm_7: LSTM</text>\n",
       "</g>\n",
       "<!-- 5671534376&#45;&gt;5671534152 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>5671534376-&gt;5671534152</title>\n",
       "<path d=\"M266.48,-292.31C266.48,-284.29 266.48,-274.55 266.48,-265.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"269.98,-265.53 266.48,-255.53 262.98,-265.53 269.98,-265.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6422823712 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>6422823712</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 188.97,-182.5 188.97,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94.48\" y=\"-160.3\">repeat_vector_7: RepeatVector</text>\n",
       "</g>\n",
       "<!-- 5671534152&#45;&gt;6422823712 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>5671534152-&gt;6422823712</title>\n",
       "<path d=\"M225.29,-219.49C201.32,-209.6 170.9,-197.04 145.44,-186.53\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"146.52,-183.19 135.94,-182.61 143.85,-189.66 146.52,-183.19\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5673424544 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>5673424544</title>\n",
       "<polygon fill=\"none\" points=\"53.41,-73.5 53.41,-109.5 189.56,-109.5 189.56,-73.5 53.41,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.48\" y=\"-87.3\">decoder_lstm: LSTM</text>\n",
       "</g>\n",
       "<!-- 5671534152&#45;&gt;5673424544 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>5671534152-&gt;5673424544</title>\n",
       "<path d=\"M255.66,-219.24C243.11,-200.05 221.11,-168.75 197.48,-146 185.87,-134.82 171.74,-124.21 158.83,-115.44\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"160.4,-112.28 150.13,-109.67 156.53,-118.11 160.4,-112.28\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5672674248 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>5672674248</title>\n",
       "<polygon fill=\"none\" points=\"244.52,-146.5 244.52,-182.5 388.45,-182.5 388.45,-146.5 244.52,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"316.48\" y=\"-160.3\">non_linear_CC: Dense</text>\n",
       "</g>\n",
       "<!-- 5671534152&#45;&gt;5672674248 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>5671534152-&gt;5672674248</title>\n",
       "<path d=\"M278.59,-219.31C284.61,-210.77 292,-200.27 298.65,-190.82\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"301.6,-192.72 304.49,-182.53 295.87,-188.69 301.6,-192.72\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5662336616 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>5662336616</title>\n",
       "<polygon fill=\"none\" points=\"416.14,-146.5 416.14,-182.5 560.83,-182.5 560.83,-146.5 416.14,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488.48\" y=\"-160.3\">non_linear_DC: Dense</text>\n",
       "</g>\n",
       "<!-- 5671534152&#45;&gt;5662336616 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>5671534152-&gt;5662336616</title>\n",
       "<path d=\"M316.03,-220.65C348.39,-210.31 390.91,-196.71 425.64,-185.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"426.76,-188.92 435.21,-182.54 424.62,-182.25 426.76,-188.92\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6422823712&#45;&gt;5673424544 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>6422823712-&gt;5673424544</title>\n",
       "<path d=\"M101.02,-146.31C104.14,-138.12 107.94,-128.12 111.41,-118.98\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.73,-120.12 115.01,-109.53 108.18,-117.63 114.73,-120.12\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6407301384 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>6407301384</title>\n",
       "<polygon fill=\"none\" points=\"50.71,-0.5 50.71,-36.5 192.26,-36.5 192.26,-0.5 50.71,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.48\" y=\"-14.3\">decoder_dense: Dense</text>\n",
       "</g>\n",
       "<!-- 5673424544&#45;&gt;6407301384 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>5673424544-&gt;6407301384</title>\n",
       "<path d=\"M121.48,-73.31C121.48,-65.29 121.48,-55.55 121.48,-46.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"124.98,-46.53 121.48,-36.53 117.98,-46.53 124.98,-46.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5671533816 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>5671533816</title>\n",
       "<polygon fill=\"none\" points=\"228.42,-73.5 228.42,-109.5 392.55,-109.5 392.55,-73.5 228.42,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"310.48\" y=\"-87.3\">softmax_layer_CC: Dense</text>\n",
       "</g>\n",
       "<!-- 5672674248&#45;&gt;5671533816 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>5672674248-&gt;5671533816</title>\n",
       "<path d=\"M315.03,-146.31C314.35,-138.29 313.53,-128.55 312.77,-119.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"316.25,-119.2 311.92,-109.53 309.28,-119.79 316.25,-119.2\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6422816080 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>6422816080</title>\n",
       "<polygon fill=\"none\" points=\"411.03,-73.5 411.03,-109.5 575.94,-109.5 575.94,-73.5 411.03,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"493.48\" y=\"-87.3\">softmax_layer_DC: Dense</text>\n",
       "</g>\n",
       "<!-- 5662336616&#45;&gt;6422816080 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>5662336616-&gt;6422816080</title>\n",
       "<path d=\"M489.7,-146.31C490.26,-138.29 490.95,-128.55 491.58,-119.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"495.07,-119.75 492.29,-109.53 488.09,-119.26 495.07,-119.75\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = test_generator(t_encoder_inputs,t_decoder_inputs,Y_CC,Y_DC,batch_size=len(t_encoder_inputs))\n",
    "encoder_input_data,[decoder_target_data,y_cc,y_dc] = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6231"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnav1712/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4984 samples, validate on 1247 samples\n",
      "Epoch 1/800\n",
      "4984/4984 [==============================] - 41s 8ms/step - loss: 0.0801 - decoder_dense_loss: 7.8643 - softmax_layer_CC_loss: 0.6839 - softmax_layer_DC_loss: 11.1746 - decoder_dense_accuracy: 0.0702 - softmax_layer_CC_accuracy: 0.5925 - softmax_layer_DC_accuracy: 0.0732 - val_loss: -0.3182 - val_decoder_dense_loss: 6.9387 - val_softmax_layer_CC_loss: 0.6763 - val_softmax_layer_DC_loss: 14.1881 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.5710 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 2/800\n",
      "4984/4984 [==============================] - 40s 8ms/step - loss: -0.3755 - decoder_dense_loss: 6.7737 - softmax_layer_CC_loss: 0.6425 - softmax_layer_DC_loss: 14.3839 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.6336 - softmax_layer_DC_accuracy: 0.0638 - val_loss: -0.3296 - val_decoder_dense_loss: 6.8159 - val_softmax_layer_CC_loss: 0.6790 - val_softmax_layer_DC_loss: 14.1919 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6071 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 3/800\n",
      "4984/4984 [==============================] - 41s 8ms/step - loss: -0.4606 - decoder_dense_loss: 6.7075 - softmax_layer_CC_loss: 0.5127 - softmax_layer_DC_loss: 14.3902 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.7628 - softmax_layer_DC_accuracy: 0.0638 - val_loss: -0.3469 - val_decoder_dense_loss: 6.8366 - val_softmax_layer_CC_loss: 0.6475 - val_softmax_layer_DC_loss: 14.1945 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6712 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 4/800\n",
      "4984/4984 [==============================] - 44s 9ms/step - loss: -0.5196 - decoder_dense_loss: 6.6909 - softmax_layer_CC_loss: 0.4184 - softmax_layer_DC_loss: 14.3961 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.8204 - softmax_layer_DC_accuracy: 0.0638 - val_loss: -0.3203 - val_decoder_dense_loss: 6.8516 - val_softmax_layer_CC_loss: 0.6868 - val_softmax_layer_DC_loss: 14.1949 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6191 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 5/800\n",
      "4984/4984 [==============================] - 45s 9ms/step - loss: -0.5610 - decoder_dense_loss: 6.6706 - softmax_layer_CC_loss: 0.3529 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.8477 - softmax_layer_DC_accuracy: 0.0638 - val_loss: -0.3088 - val_decoder_dense_loss: 6.8488 - val_softmax_layer_CC_loss: 0.7100 - val_softmax_layer_DC_loss: 14.1964 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6993 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 6/800\n",
      "4984/4984 [==============================] - 51s 10ms/step - loss: -0.5962 - decoder_dense_loss: 6.6437 - softmax_layer_CC_loss: 0.2987 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.8768 - softmax_layer_DC_accuracy: 0.0638 - val_loss: -0.2947 - val_decoder_dense_loss: 6.8404 - val_softmax_layer_CC_loss: 0.7321 - val_softmax_layer_DC_loss: 14.1956 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6600 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 7/800\n",
      "4984/4984 [==============================] - 52s 10ms/step - loss: -0.6262 - decoder_dense_loss: 6.6164 - softmax_layer_CC_loss: 0.2532 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.9009 - softmax_layer_DC_accuracy: 0.0638 - val_loss: -0.2625 - val_decoder_dense_loss: 6.8205 - val_softmax_layer_CC_loss: 0.7908 - val_softmax_layer_DC_loss: 14.1937 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6977 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 8/800\n",
      "4984/4984 [==============================] - 66s 13ms/step - loss: -0.6512 - decoder_dense_loss: 6.5897 - softmax_layer_CC_loss: 0.2164 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.9153 - softmax_layer_DC_accuracy: 0.0638 - val_loss: -0.0874 - val_decoder_dense_loss: 6.8082 - val_softmax_layer_CC_loss: 1.0865 - val_softmax_layer_DC_loss: 14.1916 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6712 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 9/800\n",
      "4984/4984 [==============================] - 57s 11ms/step - loss: -0.6780 - decoder_dense_loss: 6.5635 - softmax_layer_CC_loss: 0.1755 - softmax_layer_DC_loss: 14.3967 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.9368 - softmax_layer_DC_accuracy: 0.0638 - val_loss: -0.2135 - val_decoder_dense_loss: 6.7977 - val_softmax_layer_CC_loss: 0.8736 - val_softmax_layer_DC_loss: 14.1936 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6792 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 10/800\n",
      "4984/4984 [==============================] - 53s 11ms/step - loss: -0.6988 - decoder_dense_loss: 6.5389 - softmax_layer_CC_loss: 0.1447 - softmax_layer_DC_loss: 14.3946 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.9450 - softmax_layer_DC_accuracy: 0.0638 - val_loss: -0.1088 - val_decoder_dense_loss: 6.7837 - val_softmax_layer_CC_loss: 1.0531 - val_softmax_layer_DC_loss: 14.1954 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6872 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 11/800\n",
      "4984/4984 [==============================] - 52s 10ms/step - loss: -0.7129 - decoder_dense_loss: 6.5143 - softmax_layer_CC_loss: 0.1258 - softmax_layer_DC_loss: 14.3981 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.9555 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.0311 - val_decoder_dense_loss: 6.7705 - val_softmax_layer_CC_loss: 1.2893 - val_softmax_layer_DC_loss: 14.1925 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6840 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 12/800\n",
      "4984/4984 [==============================] - 51s 10ms/step - loss: -0.7288 - decoder_dense_loss: 6.4889 - softmax_layer_CC_loss: 0.1033 - softmax_layer_DC_loss: 14.3979 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.9605 - softmax_layer_DC_accuracy: 0.0638 - val_loss: -0.0149 - val_decoder_dense_loss: 6.7634 - val_softmax_layer_CC_loss: 1.2112 - val_softmax_layer_DC_loss: 14.1914 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6905 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 13/800\n",
      "4984/4984 [==============================] - 53s 11ms/step - loss: -0.7451 - decoder_dense_loss: 6.4646 - softmax_layer_CC_loss: 0.0802 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.9695 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.1009 - val_decoder_dense_loss: 6.7571 - val_softmax_layer_CC_loss: 1.4050 - val_softmax_layer_DC_loss: 14.1919 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6832 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 14/800\n",
      "4984/4984 [==============================] - 50s 10ms/step - loss: -0.7559 - decoder_dense_loss: 6.4403 - softmax_layer_CC_loss: 0.0662 - softmax_layer_DC_loss: 14.3949 - decoder_dense_accuracy: 0.0732 - softmax_layer_CC_accuracy: 0.9751 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.1183 - val_decoder_dense_loss: 6.7573 - val_softmax_layer_CC_loss: 1.4278 - val_softmax_layer_DC_loss: 14.1893 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6496 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 15/800\n",
      "4984/4984 [==============================] - 51s 10ms/step - loss: -0.7607 - decoder_dense_loss: 6.4156 - softmax_layer_CC_loss: 0.0623 - softmax_layer_DC_loss: 14.3971 - decoder_dense_accuracy: 0.0731 - softmax_layer_CC_accuracy: 0.9757 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.1142 - val_decoder_dense_loss: 6.7413 - val_softmax_layer_CC_loss: 1.4256 - val_softmax_layer_DC_loss: 14.1880 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6608 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 16/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 54s 11ms/step - loss: -0.7686 - decoder_dense_loss: 6.3930 - softmax_layer_CC_loss: 0.0528 - softmax_layer_DC_loss: 14.3954 - decoder_dense_accuracy: 0.0732 - softmax_layer_CC_accuracy: 0.9797 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.1540 - val_decoder_dense_loss: 6.7435 - val_softmax_layer_CC_loss: 1.4921 - val_softmax_layer_DC_loss: 14.1959 - val_decoder_dense_accuracy: 0.0719 - val_softmax_layer_CC_accuracy: 0.6616 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 17/800\n",
      "4984/4984 [==============================] - 53s 11ms/step - loss: -0.7735 - decoder_dense_loss: 6.3717 - softmax_layer_CC_loss: 0.0485 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.0732 - softmax_layer_CC_accuracy: 0.9841 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.3011 - val_decoder_dense_loss: 6.7302 - val_softmax_layer_CC_loss: 1.7430 - val_softmax_layer_DC_loss: 14.1954 - val_decoder_dense_accuracy: 0.0721 - val_softmax_layer_CC_accuracy: 0.6848 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 18/800\n",
      "4984/4984 [==============================] - 50s 10ms/step - loss: -0.7768 - decoder_dense_loss: 6.3508 - softmax_layer_CC_loss: 0.0466 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.0735 - softmax_layer_CC_accuracy: 0.9831 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2418 - val_decoder_dense_loss: 6.7329 - val_softmax_layer_CC_loss: 1.6402 - val_softmax_layer_DC_loss: 14.1861 - val_decoder_dense_accuracy: 0.0722 - val_softmax_layer_CC_accuracy: 0.6704 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 19/800\n",
      "4984/4984 [==============================] - 59s 12ms/step - loss: -0.7812 - decoder_dense_loss: 6.3292 - softmax_layer_CC_loss: 0.0425 - softmax_layer_DC_loss: 14.3963 - decoder_dense_accuracy: 0.0740 - softmax_layer_CC_accuracy: 0.9872 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2078 - val_decoder_dense_loss: 6.7245 - val_softmax_layer_CC_loss: 1.5866 - val_softmax_layer_DC_loss: 14.1969 - val_decoder_dense_accuracy: 0.0724 - val_softmax_layer_CC_accuracy: 0.6640 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 20/800\n",
      "4984/4984 [==============================] - 4129s 828ms/step - loss: -0.7823 - decoder_dense_loss: 6.3084 - softmax_layer_CC_loss: 0.0444 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.0744 - softmax_layer_CC_accuracy: 0.9835 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2264 - val_decoder_dense_loss: 6.7220 - val_softmax_layer_CC_loss: 1.6169 - val_softmax_layer_DC_loss: 14.1910 - val_decoder_dense_accuracy: 0.0725 - val_softmax_layer_CC_accuracy: 0.6728 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 21/800\n",
      "4984/4984 [==============================] - 34s 7ms/step - loss: -0.7879 - decoder_dense_loss: 6.2881 - softmax_layer_CC_loss: 0.0384 - softmax_layer_DC_loss: 14.3971 - decoder_dense_accuracy: 0.0743 - softmax_layer_CC_accuracy: 0.9860 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2349 - val_decoder_dense_loss: 6.7218 - val_softmax_layer_CC_loss: 1.6319 - val_softmax_layer_DC_loss: 14.1990 - val_decoder_dense_accuracy: 0.0731 - val_softmax_layer_CC_accuracy: 0.6736 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 22/800\n",
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.7900 - decoder_dense_loss: 6.2683 - softmax_layer_CC_loss: 0.0383 - softmax_layer_DC_loss: 14.3988 - decoder_dense_accuracy: 0.0751 - softmax_layer_CC_accuracy: 0.9868 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.1947 - val_decoder_dense_loss: 6.7158 - val_softmax_layer_CC_loss: 1.5657 - val_softmax_layer_DC_loss: 14.2037 - val_decoder_dense_accuracy: 0.0739 - val_softmax_layer_CC_accuracy: 0.6736 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 23/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.7921 - decoder_dense_loss: 6.2484 - softmax_layer_CC_loss: 0.0381 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.0759 - softmax_layer_CC_accuracy: 0.9856 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.1643 - val_decoder_dense_loss: 6.7173 - val_softmax_layer_CC_loss: 1.5119 - val_softmax_layer_DC_loss: 14.1990 - val_decoder_dense_accuracy: 0.0733 - val_softmax_layer_CC_accuracy: 0.6576 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 24/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.7951 - decoder_dense_loss: 6.2259 - softmax_layer_CC_loss: 0.0368 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.0759 - softmax_layer_CC_accuracy: 0.9870 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2354 - val_decoder_dense_loss: 6.7038 - val_softmax_layer_CC_loss: 1.6355 - val_softmax_layer_DC_loss: 14.1992 - val_decoder_dense_accuracy: 0.0731 - val_softmax_layer_CC_accuracy: 0.6768 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 25/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.7972 - decoder_dense_loss: 6.2038 - softmax_layer_CC_loss: 0.0370 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.0770 - softmax_layer_CC_accuracy: 0.9872 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.1844 - val_decoder_dense_loss: 6.7127 - val_softmax_layer_CC_loss: 1.5465 - val_softmax_layer_DC_loss: 14.1963 - val_decoder_dense_accuracy: 0.0733 - val_softmax_layer_CC_accuracy: 0.6736 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 26/800\n",
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.8019 - decoder_dense_loss: 6.1798 - softmax_layer_CC_loss: 0.0333 - softmax_layer_DC_loss: 14.3985 - decoder_dense_accuracy: 0.0766 - softmax_layer_CC_accuracy: 0.9874 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.1848 - val_decoder_dense_loss: 6.7000 - val_softmax_layer_CC_loss: 1.5503 - val_softmax_layer_DC_loss: 14.1930 - val_decoder_dense_accuracy: 0.0739 - val_softmax_layer_CC_accuracy: 0.6712 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 27/800\n",
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.8046 - decoder_dense_loss: 6.1567 - softmax_layer_CC_loss: 0.0325 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.0781 - softmax_layer_CC_accuracy: 0.9882 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2126 - val_decoder_dense_loss: 6.6977 - val_softmax_layer_CC_loss: 1.5950 - val_softmax_layer_DC_loss: 14.1893 - val_decoder_dense_accuracy: 0.0750 - val_softmax_layer_CC_accuracy: 0.6728 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 28/800\n",
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.8069 - decoder_dense_loss: 6.1339 - softmax_layer_CC_loss: 0.0325 - softmax_layer_DC_loss: 14.3966 - decoder_dense_accuracy: 0.0787 - softmax_layer_CC_accuracy: 0.9892 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2471 - val_decoder_dense_loss: 6.6826 - val_softmax_layer_CC_loss: 1.6558 - val_softmax_layer_DC_loss: 14.1823 - val_decoder_dense_accuracy: 0.0747 - val_softmax_layer_CC_accuracy: 0.6776 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 29/800\n",
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.8112 - decoder_dense_loss: 6.1088 - softmax_layer_CC_loss: 0.0297 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.0811 - softmax_layer_CC_accuracy: 0.9878 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2406 - val_decoder_dense_loss: 6.6745 - val_softmax_layer_CC_loss: 1.6439 - val_softmax_layer_DC_loss: 14.1743 - val_decoder_dense_accuracy: 0.0763 - val_softmax_layer_CC_accuracy: 0.6760 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 30/800\n",
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.8137 - decoder_dense_loss: 6.0832 - softmax_layer_CC_loss: 0.0294 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.0814 - softmax_layer_CC_accuracy: 0.9884 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2993 - val_decoder_dense_loss: 6.6654 - val_softmax_layer_CC_loss: 1.7449 - val_softmax_layer_DC_loss: 14.1811 - val_decoder_dense_accuracy: 0.0779 - val_softmax_layer_CC_accuracy: 0.6648 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 31/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.8167 - decoder_dense_loss: 6.0571 - softmax_layer_CC_loss: 0.0290 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.0836 - softmax_layer_CC_accuracy: 0.9888 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2880 - val_decoder_dense_loss: 6.6577 - val_softmax_layer_CC_loss: 1.7257 - val_softmax_layer_DC_loss: 14.1740 - val_decoder_dense_accuracy: 0.0798 - val_softmax_layer_CC_accuracy: 0.6728 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 32/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.8201 - decoder_dense_loss: 6.0342 - softmax_layer_CC_loss: 0.0270 - softmax_layer_DC_loss: 14.3967 - decoder_dense_accuracy: 0.0851 - softmax_layer_CC_accuracy: 0.9890 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2572 - val_decoder_dense_loss: 6.6552 - val_softmax_layer_CC_loss: 1.6742 - val_softmax_layer_DC_loss: 14.1643 - val_decoder_dense_accuracy: 0.0791 - val_softmax_layer_CC_accuracy: 0.6648 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 33/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.8232 - decoder_dense_loss: 6.0094 - softmax_layer_CC_loss: 0.0260 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.0869 - softmax_layer_CC_accuracy: 0.9892 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2768 - val_decoder_dense_loss: 6.6603 - val_softmax_layer_CC_loss: 1.7052 - val_softmax_layer_DC_loss: 14.1658 - val_decoder_dense_accuracy: 0.0809 - val_softmax_layer_CC_accuracy: 0.6576 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 34/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.8242 - decoder_dense_loss: 5.9857 - softmax_layer_CC_loss: 0.0283 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.0885 - softmax_layer_CC_accuracy: 0.9884 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2784 - val_decoder_dense_loss: 6.6477 - val_softmax_layer_CC_loss: 1.7148 - val_softmax_layer_DC_loss: 14.1734 - val_decoder_dense_accuracy: 0.0842 - val_softmax_layer_CC_accuracy: 0.6656 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 35/800\n",
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.8280 - decoder_dense_loss: 5.9603 - softmax_layer_CC_loss: 0.0263 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.0909 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2746 - val_decoder_dense_loss: 6.6408 - val_softmax_layer_CC_loss: 1.7032 - val_softmax_layer_DC_loss: 14.1651 - val_decoder_dense_accuracy: 0.0859 - val_softmax_layer_CC_accuracy: 0.6648 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 36/800\n",
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.8302 - decoder_dense_loss: 5.9367 - softmax_layer_CC_loss: 0.0264 - softmax_layer_DC_loss: 14.3971 - decoder_dense_accuracy: 0.0927 - softmax_layer_CC_accuracy: 0.9882 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.3485 - val_decoder_dense_loss: 6.6328 - val_softmax_layer_CC_loss: 1.8295 - val_softmax_layer_DC_loss: 14.1672 - val_decoder_dense_accuracy: 0.0863 - val_softmax_layer_CC_accuracy: 0.6712 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 37/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.8333 - decoder_dense_loss: 5.9113 - softmax_layer_CC_loss: 0.0256 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.0945 - softmax_layer_CC_accuracy: 0.9888 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.3758 - val_decoder_dense_loss: 6.6333 - val_softmax_layer_CC_loss: 1.8723 - val_softmax_layer_DC_loss: 14.1666 - val_decoder_dense_accuracy: 0.0868 - val_softmax_layer_CC_accuracy: 0.6640 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 38/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.8372 - decoder_dense_loss: 5.8871 - softmax_layer_CC_loss: 0.0232 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.0971 - softmax_layer_CC_accuracy: 0.9890 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.3481 - val_decoder_dense_loss: 6.6204 - val_softmax_layer_CC_loss: 1.8294 - val_softmax_layer_DC_loss: 14.1719 - val_decoder_dense_accuracy: 0.0908 - val_softmax_layer_CC_accuracy: 0.6664 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 39/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.8393 - decoder_dense_loss: 5.8613 - softmax_layer_CC_loss: 0.0239 - softmax_layer_DC_loss: 14.3975 - decoder_dense_accuracy: 0.0997 - softmax_layer_CC_accuracy: 0.9884 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.4600 - val_decoder_dense_loss: 6.6132 - val_softmax_layer_CC_loss: 2.0221 - val_softmax_layer_DC_loss: 14.1750 - val_decoder_dense_accuracy: 0.0919 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 40/800\n",
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.8418 - decoder_dense_loss: 5.8387 - softmax_layer_CC_loss: 0.0237 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.1014 - softmax_layer_CC_accuracy: 0.9884 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.3455 - val_decoder_dense_loss: 6.6103 - val_softmax_layer_CC_loss: 1.8274 - val_softmax_layer_DC_loss: 14.1749 - val_decoder_dense_accuracy: 0.0945 - val_softmax_layer_CC_accuracy: 0.6656 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 41/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -0.8453 - decoder_dense_loss: 5.8095 - softmax_layer_CC_loss: 0.0227 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.1039 - softmax_layer_CC_accuracy: 0.9890 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.3474 - val_decoder_dense_loss: 6.6112 - val_softmax_layer_CC_loss: 1.8246 - val_softmax_layer_DC_loss: 14.1566 - val_decoder_dense_accuracy: 0.0930 - val_softmax_layer_CC_accuracy: 0.6624 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 42/800\n",
      "4984/4984 [==============================] - 32s 6ms/step - loss: -0.8474 - decoder_dense_loss: 5.7852 - softmax_layer_CC_loss: 0.0231 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.1054 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.3628 - val_decoder_dense_loss: 6.5922 - val_softmax_layer_CC_loss: 1.8590 - val_softmax_layer_DC_loss: 14.1644 - val_decoder_dense_accuracy: 0.0959 - val_softmax_layer_CC_accuracy: 0.6688 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 43/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -0.8504 - decoder_dense_loss: 5.7570 - softmax_layer_CC_loss: 0.0226 - softmax_layer_DC_loss: 14.3966 - decoder_dense_accuracy: 0.1077 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.4466 - val_decoder_dense_loss: 6.5923 - val_softmax_layer_CC_loss: 1.9975 - val_softmax_layer_DC_loss: 14.1665 - val_decoder_dense_accuracy: 0.0989 - val_softmax_layer_CC_accuracy: 0.6656 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 44/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.8535 - decoder_dense_loss: 5.7300 - softmax_layer_CC_loss: 0.0222 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.1092 - softmax_layer_CC_accuracy: 0.9892 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.2826 - val_decoder_dense_loss: 6.5815 - val_softmax_layer_CC_loss: 1.7247 - val_softmax_layer_DC_loss: 14.1604 - val_decoder_dense_accuracy: 0.0949 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 45/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -0.8566 - decoder_dense_loss: 5.7028 - softmax_layer_CC_loss: 0.0216 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.1121 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.3860 - val_decoder_dense_loss: 6.5699 - val_softmax_layer_CC_loss: 1.9013 - val_softmax_layer_DC_loss: 14.1685 - val_decoder_dense_accuracy: 0.1025 - val_softmax_layer_CC_accuracy: 0.6640 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 46/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.8603 - decoder_dense_loss: 5.6736 - softmax_layer_CC_loss: 0.0203 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.1138 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.4472 - val_decoder_dense_loss: 6.5594 - val_softmax_layer_CC_loss: 2.0057 - val_softmax_layer_DC_loss: 14.1713 - val_decoder_dense_accuracy: 0.1012 - val_softmax_layer_CC_accuracy: 0.6592 - val_softmax_layer_DC_accuracy: 0.0778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -0.8617 - decoder_dense_loss: 5.6459 - softmax_layer_CC_loss: 0.0225 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.1153 - softmax_layer_CC_accuracy: 0.9886 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.3951 - val_decoder_dense_loss: 6.5392 - val_softmax_layer_CC_loss: 1.9241 - val_softmax_layer_DC_loss: 14.1704 - val_decoder_dense_accuracy: 0.1026 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 48/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.8664 - decoder_dense_loss: 5.6141 - softmax_layer_CC_loss: 0.0199 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.1184 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.4494 - val_decoder_dense_loss: 6.5312 - val_softmax_layer_CC_loss: 2.0138 - val_softmax_layer_DC_loss: 14.1673 - val_decoder_dense_accuracy: 0.1025 - val_softmax_layer_CC_accuracy: 0.6608 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 49/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.8681 - decoder_dense_loss: 5.5865 - softmax_layer_CC_loss: 0.0218 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.1211 - softmax_layer_CC_accuracy: 0.9876 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.4631 - val_decoder_dense_loss: 6.5172 - val_softmax_layer_CC_loss: 2.0416 - val_softmax_layer_DC_loss: 14.1668 - val_decoder_dense_accuracy: 0.1043 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 50/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.8724 - decoder_dense_loss: 5.5573 - softmax_layer_CC_loss: 0.0195 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.1244 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5471 - val_decoder_dense_loss: 6.5082 - val_softmax_layer_CC_loss: 2.1803 - val_softmax_layer_DC_loss: 14.1626 - val_decoder_dense_accuracy: 0.1048 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 51/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.8742 - decoder_dense_loss: 5.5269 - softmax_layer_CC_loss: 0.0214 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.1258 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5070 - val_decoder_dense_loss: 6.4887 - val_softmax_layer_CC_loss: 2.1198 - val_softmax_layer_DC_loss: 14.1719 - val_decoder_dense_accuracy: 0.1081 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 52/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.8778 - decoder_dense_loss: 5.4961 - softmax_layer_CC_loss: 0.0207 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.1282 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.4957 - val_decoder_dense_loss: 6.4842 - val_softmax_layer_CC_loss: 2.0989 - val_softmax_layer_DC_loss: 14.1779 - val_decoder_dense_accuracy: 0.1115 - val_softmax_layer_CC_accuracy: 0.6472 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 53/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.8811 - decoder_dense_loss: 5.4654 - softmax_layer_CC_loss: 0.0203 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.1314 - softmax_layer_CC_accuracy: 0.9890 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5584 - val_decoder_dense_loss: 6.4758 - val_softmax_layer_CC_loss: 2.2046 - val_softmax_layer_DC_loss: 14.1698 - val_decoder_dense_accuracy: 0.1123 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 54/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -0.8845 - decoder_dense_loss: 5.4320 - softmax_layer_CC_loss: 0.0201 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.1339 - softmax_layer_CC_accuracy: 0.9886 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.4960 - val_decoder_dense_loss: 6.4689 - val_softmax_layer_CC_loss: 2.1048 - val_softmax_layer_DC_loss: 14.1744 - val_decoder_dense_accuracy: 0.1162 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 55/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -0.8875 - decoder_dense_loss: 5.4029 - softmax_layer_CC_loss: 0.0201 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.1352 - softmax_layer_CC_accuracy: 0.9890 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.4452 - val_decoder_dense_loss: 6.4658 - val_softmax_layer_CC_loss: 2.0135 - val_softmax_layer_DC_loss: 14.1505 - val_decoder_dense_accuracy: 0.1143 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 56/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.8906 - decoder_dense_loss: 5.3727 - softmax_layer_CC_loss: 0.0197 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.1392 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5567 - val_decoder_dense_loss: 6.4424 - val_softmax_layer_CC_loss: 2.2074 - val_softmax_layer_DC_loss: 14.1625 - val_decoder_dense_accuracy: 0.1160 - val_softmax_layer_CC_accuracy: 0.6496 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 57/800\n",
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.8937 - decoder_dense_loss: 5.3408 - softmax_layer_CC_loss: 0.0201 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.1413 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5668 - val_decoder_dense_loss: 6.4208 - val_softmax_layer_CC_loss: 2.2304 - val_softmax_layer_DC_loss: 14.1667 - val_decoder_dense_accuracy: 0.1176 - val_softmax_layer_CC_accuracy: 0.6472 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 58/800\n",
      "4984/4984 [==============================] - 25s 5ms/step - loss: -0.8968 - decoder_dense_loss: 5.3091 - softmax_layer_CC_loss: 0.0202 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.1444 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5499 - val_decoder_dense_loss: 6.4277 - val_softmax_layer_CC_loss: 2.1997 - val_softmax_layer_DC_loss: 14.1593 - val_decoder_dense_accuracy: 0.1184 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 59/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.9001 - decoder_dense_loss: 5.2787 - softmax_layer_CC_loss: 0.0197 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.1458 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6161 - val_decoder_dense_loss: 6.4080 - val_softmax_layer_CC_loss: 2.3158 - val_softmax_layer_DC_loss: 14.1671 - val_decoder_dense_accuracy: 0.1194 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 60/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.9022 - decoder_dense_loss: 5.2504 - softmax_layer_CC_loss: 0.0209 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.1481 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5348 - val_decoder_dense_loss: 6.3893 - val_softmax_layer_CC_loss: 2.1809 - val_softmax_layer_DC_loss: 14.1639 - val_decoder_dense_accuracy: 0.1208 - val_softmax_layer_CC_accuracy: 0.6439 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 61/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.9066 - decoder_dense_loss: 5.2155 - softmax_layer_CC_loss: 0.0195 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.1506 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.4626 - val_decoder_dense_loss: 6.3986 - val_softmax_layer_CC_loss: 2.0570 - val_softmax_layer_DC_loss: 14.1595 - val_decoder_dense_accuracy: 0.1199 - val_softmax_layer_CC_accuracy: 0.6439 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 62/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 27s 5ms/step - loss: -0.9086 - decoder_dense_loss: 5.1898 - softmax_layer_CC_loss: 0.0204 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.1526 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5938 - val_decoder_dense_loss: 6.3716 - val_softmax_layer_CC_loss: 2.2866 - val_softmax_layer_DC_loss: 14.1616 - val_decoder_dense_accuracy: 0.1246 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 63/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -0.9128 - decoder_dense_loss: 5.1544 - softmax_layer_CC_loss: 0.0193 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.1572 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5058 - val_decoder_dense_loss: 6.3684 - val_softmax_layer_CC_loss: 2.1379 - val_softmax_layer_DC_loss: 14.1551 - val_decoder_dense_accuracy: 0.1241 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 64/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -0.9164 - decoder_dense_loss: 5.1223 - softmax_layer_CC_loss: 0.0187 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.1582 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5426 - val_decoder_dense_loss: 6.3513 - val_softmax_layer_CC_loss: 2.2034 - val_softmax_layer_DC_loss: 14.1572 - val_decoder_dense_accuracy: 0.1255 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 65/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.9178 - decoder_dense_loss: 5.0949 - softmax_layer_CC_loss: 0.0209 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.1619 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5167 - val_decoder_dense_loss: 6.3461 - val_softmax_layer_CC_loss: 2.1597 - val_softmax_layer_DC_loss: 14.1581 - val_decoder_dense_accuracy: 0.1241 - val_softmax_layer_CC_accuracy: 0.6584 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 66/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.9223 - decoder_dense_loss: 5.0636 - softmax_layer_CC_loss: 0.0184 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.1634 - softmax_layer_CC_accuracy: 0.9886 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6034 - val_decoder_dense_loss: 6.3287 - val_softmax_layer_CC_loss: 2.3061 - val_softmax_layer_DC_loss: 14.1581 - val_decoder_dense_accuracy: 0.1273 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 67/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.9248 - decoder_dense_loss: 5.0347 - softmax_layer_CC_loss: 0.0191 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.1660 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6212 - val_decoder_dense_loss: 6.3169 - val_softmax_layer_CC_loss: 2.3427 - val_softmax_layer_DC_loss: 14.1708 - val_decoder_dense_accuracy: 0.1294 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 68/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -0.9278 - decoder_dense_loss: 5.0066 - softmax_layer_CC_loss: 0.0190 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.1670 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5970 - val_decoder_dense_loss: 6.3087 - val_softmax_layer_CC_loss: 2.3043 - val_softmax_layer_DC_loss: 14.1618 - val_decoder_dense_accuracy: 0.1311 - val_softmax_layer_CC_accuracy: 0.6600 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 69/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.9310 - decoder_dense_loss: 4.9760 - softmax_layer_CC_loss: 0.0186 - softmax_layer_DC_loss: 14.3990 - decoder_dense_accuracy: 0.1706 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5955 - val_decoder_dense_loss: 6.3171 - val_softmax_layer_CC_loss: 2.2964 - val_softmax_layer_DC_loss: 14.1502 - val_decoder_dense_accuracy: 0.1301 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 70/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -0.9337 - decoder_dense_loss: 4.9487 - softmax_layer_CC_loss: 0.0187 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.1718 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6773 - val_decoder_dense_loss: 6.3033 - val_softmax_layer_CC_loss: 2.4367 - val_softmax_layer_DC_loss: 14.1642 - val_decoder_dense_accuracy: 0.1301 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 71/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.9368 - decoder_dense_loss: 4.9199 - softmax_layer_CC_loss: 0.0182 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.1754 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7101 - val_decoder_dense_loss: 6.2813 - val_softmax_layer_CC_loss: 2.4939 - val_softmax_layer_DC_loss: 14.1590 - val_decoder_dense_accuracy: 0.1344 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 72/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.9390 - decoder_dense_loss: 4.8910 - softmax_layer_CC_loss: 0.0195 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.1768 - softmax_layer_CC_accuracy: 0.9886 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6203 - val_decoder_dense_loss: 6.2758 - val_softmax_layer_CC_loss: 2.3465 - val_softmax_layer_DC_loss: 14.1541 - val_decoder_dense_accuracy: 0.1326 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 73/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -0.9426 - decoder_dense_loss: 4.8573 - softmax_layer_CC_loss: 0.0191 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.1792 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6548 - val_decoder_dense_loss: 6.2690 - val_softmax_layer_CC_loss: 2.4038 - val_softmax_layer_DC_loss: 14.1522 - val_decoder_dense_accuracy: 0.1346 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 74/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.9460 - decoder_dense_loss: 4.8294 - softmax_layer_CC_loss: 0.0181 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.1811 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7089 - val_decoder_dense_loss: 6.2565 - val_softmax_layer_CC_loss: 2.4957 - val_softmax_layer_DC_loss: 14.1660 - val_decoder_dense_accuracy: 0.1352 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 75/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -0.9482 - decoder_dense_loss: 4.8007 - softmax_layer_CC_loss: 0.0192 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.1839 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6707 - val_decoder_dense_loss: 6.2442 - val_softmax_layer_CC_loss: 2.4379 - val_softmax_layer_DC_loss: 14.1705 - val_decoder_dense_accuracy: 0.1351 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 76/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.9517 - decoder_dense_loss: 4.7744 - softmax_layer_CC_loss: 0.0177 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.1863 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6957 - val_decoder_dense_loss: 6.2471 - val_softmax_layer_CC_loss: 2.4786 - val_softmax_layer_DC_loss: 14.1591 - val_decoder_dense_accuracy: 0.1360 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 77/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -0.9538 - decoder_dense_loss: 4.7456 - softmax_layer_CC_loss: 0.0190 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.1889 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6247 - val_decoder_dense_loss: 6.2357 - val_softmax_layer_CC_loss: 2.3603 - val_softmax_layer_DC_loss: 14.1555 - val_decoder_dense_accuracy: 0.1380 - val_softmax_layer_CC_accuracy: 0.6584 - val_softmax_layer_DC_accuracy: 0.0786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -0.9562 - decoder_dense_loss: 4.7177 - softmax_layer_CC_loss: 0.0197 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.1915 - softmax_layer_CC_accuracy: 0.9888 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6190 - val_decoder_dense_loss: 6.2241 - val_softmax_layer_CC_loss: 2.3517 - val_softmax_layer_DC_loss: 14.1541 - val_decoder_dense_accuracy: 0.1403 - val_softmax_layer_CC_accuracy: 0.6496 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 79/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -0.9603 - decoder_dense_loss: 4.6882 - softmax_layer_CC_loss: 0.0179 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.1914 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6558 - val_decoder_dense_loss: 6.2188 - val_softmax_layer_CC_loss: 2.4146 - val_softmax_layer_DC_loss: 14.1608 - val_decoder_dense_accuracy: 0.1409 - val_softmax_layer_CC_accuracy: 0.6496 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 80/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -0.9622 - decoder_dense_loss: 4.6654 - softmax_layer_CC_loss: 0.0185 - softmax_layer_DC_loss: 14.3965 - decoder_dense_accuracy: 0.1941 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.5932 - val_decoder_dense_loss: 6.2244 - val_softmax_layer_CC_loss: 2.3097 - val_softmax_layer_DC_loss: 14.1485 - val_decoder_dense_accuracy: 0.1380 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 81/800\n",
      "4984/4984 [==============================] - 35s 7ms/step - loss: -0.9654 - decoder_dense_loss: 4.6368 - softmax_layer_CC_loss: 0.0178 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.1966 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7033 - val_decoder_dense_loss: 6.1993 - val_softmax_layer_CC_loss: 2.4994 - val_softmax_layer_DC_loss: 14.1607 - val_decoder_dense_accuracy: 0.1418 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 82/800\n",
      "4984/4984 [==============================] - 36s 7ms/step - loss: -0.9674 - decoder_dense_loss: 4.6113 - softmax_layer_CC_loss: 0.0188 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.1980 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6514 - val_decoder_dense_loss: 6.1972 - val_softmax_layer_CC_loss: 2.4091 - val_softmax_layer_DC_loss: 14.1455 - val_decoder_dense_accuracy: 0.1438 - val_softmax_layer_CC_accuracy: 0.6496 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 83/800\n",
      "4984/4984 [==============================] - 50s 10ms/step - loss: -0.9705 - decoder_dense_loss: 4.5811 - softmax_layer_CC_loss: 0.0186 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.2018 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6989 - val_decoder_dense_loss: 6.1909 - val_softmax_layer_CC_loss: 2.4933 - val_softmax_layer_DC_loss: 14.1590 - val_decoder_dense_accuracy: 0.1431 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 84/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -0.9734 - decoder_dense_loss: 4.5554 - softmax_layer_CC_loss: 0.0181 - softmax_layer_DC_loss: 14.3969 - decoder_dense_accuracy: 0.2034 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7070 - val_decoder_dense_loss: 6.1823 - val_softmax_layer_CC_loss: 2.5079 - val_softmax_layer_DC_loss: 14.1551 - val_decoder_dense_accuracy: 0.1447 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 85/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -0.9761 - decoder_dense_loss: 4.5291 - softmax_layer_CC_loss: 0.0180 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.2043 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6733 - val_decoder_dense_loss: 6.1753 - val_softmax_layer_CC_loss: 2.4518 - val_softmax_layer_DC_loss: 14.1466 - val_decoder_dense_accuracy: 0.1465 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 86/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.9789 - decoder_dense_loss: 4.5050 - softmax_layer_CC_loss: 0.0174 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.2072 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6710 - val_decoder_dense_loss: 6.1745 - val_softmax_layer_CC_loss: 2.4483 - val_softmax_layer_DC_loss: 14.1393 - val_decoder_dense_accuracy: 0.1464 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 87/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -0.9814 - decoder_dense_loss: 4.4740 - softmax_layer_CC_loss: 0.0183 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.2106 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6920 - val_decoder_dense_loss: 6.1809 - val_softmax_layer_CC_loss: 2.4842 - val_softmax_layer_DC_loss: 14.1402 - val_decoder_dense_accuracy: 0.1472 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 88/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.9842 - decoder_dense_loss: 4.4484 - softmax_layer_CC_loss: 0.0179 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.2122 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6950 - val_decoder_dense_loss: 6.1695 - val_softmax_layer_CC_loss: 2.4880 - val_softmax_layer_DC_loss: 14.1467 - val_decoder_dense_accuracy: 0.1492 - val_softmax_layer_CC_accuracy: 0.6431 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 89/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -0.9862 - decoder_dense_loss: 4.4247 - softmax_layer_CC_loss: 0.0184 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.2136 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6843 - val_decoder_dense_loss: 6.1578 - val_softmax_layer_CC_loss: 2.4746 - val_softmax_layer_DC_loss: 14.1536 - val_decoder_dense_accuracy: 0.1491 - val_softmax_layer_CC_accuracy: 0.6447 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 90/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.9893 - decoder_dense_loss: 4.4002 - softmax_layer_CC_loss: 0.0174 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.2172 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8015 - val_decoder_dense_loss: 6.1575 - val_softmax_layer_CC_loss: 2.6722 - val_softmax_layer_DC_loss: 14.1616 - val_decoder_dense_accuracy: 0.1464 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 91/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -0.9912 - decoder_dense_loss: 4.3737 - softmax_layer_CC_loss: 0.0187 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.2189 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6299 - val_decoder_dense_loss: 6.1596 - val_softmax_layer_CC_loss: 2.3833 - val_softmax_layer_DC_loss: 14.1402 - val_decoder_dense_accuracy: 0.1483 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 92/800\n",
      "4984/4984 [==============================] - 32s 6ms/step - loss: -0.9944 - decoder_dense_loss: 4.3446 - softmax_layer_CC_loss: 0.0183 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.2218 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6300 - val_decoder_dense_loss: 6.1588 - val_softmax_layer_CC_loss: 2.3820 - val_softmax_layer_DC_loss: 14.1307 - val_decoder_dense_accuracy: 0.1479 - val_softmax_layer_CC_accuracy: 0.6608 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 93/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 40s 8ms/step - loss: -0.9969 - decoder_dense_loss: 4.3231 - softmax_layer_CC_loss: 0.0176 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.2229 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6573 - val_decoder_dense_loss: 6.1485 - val_softmax_layer_CC_loss: 2.4275 - val_softmax_layer_DC_loss: 14.1350 - val_decoder_dense_accuracy: 0.1522 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 94/800\n",
      "4984/4984 [==============================] - 36s 7ms/step - loss: -0.9986 - decoder_dense_loss: 4.3037 - softmax_layer_CC_loss: 0.0180 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.2254 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7536 - val_decoder_dense_loss: 6.1472 - val_softmax_layer_CC_loss: 2.5899 - val_softmax_layer_DC_loss: 14.1419 - val_decoder_dense_accuracy: 0.1518 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 95/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.0012 - decoder_dense_loss: 4.2729 - softmax_layer_CC_loss: 0.0189 - softmax_layer_DC_loss: 14.3969 - decoder_dense_accuracy: 0.2282 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7037 - val_decoder_dense_loss: 6.1329 - val_softmax_layer_CC_loss: 2.5079 - val_softmax_layer_DC_loss: 14.1430 - val_decoder_dense_accuracy: 0.1549 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 96/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.0048 - decoder_dense_loss: 4.2473 - softmax_layer_CC_loss: 0.0172 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.2310 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7590 - val_decoder_dense_loss: 6.1278 - val_softmax_layer_CC_loss: 2.6010 - val_softmax_layer_DC_loss: 14.1463 - val_decoder_dense_accuracy: 0.1558 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 97/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.0065 - decoder_dense_loss: 4.2246 - softmax_layer_CC_loss: 0.0180 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.2332 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6959 - val_decoder_dense_loss: 6.1429 - val_softmax_layer_CC_loss: 2.4939 - val_softmax_layer_DC_loss: 14.1435 - val_decoder_dense_accuracy: 0.1574 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 98/800\n",
      "4984/4984 [==============================] - 32s 6ms/step - loss: -1.0091 - decoder_dense_loss: 4.1983 - softmax_layer_CC_loss: 0.0181 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.2360 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6748 - val_decoder_dense_loss: 6.1366 - val_softmax_layer_CC_loss: 2.4565 - val_softmax_layer_DC_loss: 14.1352 - val_decoder_dense_accuracy: 0.1569 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 99/800\n",
      "4984/4984 [==============================] - 42s 8ms/step - loss: -1.0112 - decoder_dense_loss: 4.1772 - softmax_layer_CC_loss: 0.0181 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.2369 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6607 - val_decoder_dense_loss: 6.1286 - val_softmax_layer_CC_loss: 2.4360 - val_softmax_layer_DC_loss: 14.1359 - val_decoder_dense_accuracy: 0.1558 - val_softmax_layer_CC_accuracy: 0.6592 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 100/800\n",
      "4984/4984 [==============================] - 32s 6ms/step - loss: -1.0141 - decoder_dense_loss: 4.1526 - softmax_layer_CC_loss: 0.0173 - softmax_layer_DC_loss: 14.3979 - decoder_dense_accuracy: 0.2398 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7715 - val_decoder_dense_loss: 6.1148 - val_softmax_layer_CC_loss: 2.6270 - val_softmax_layer_DC_loss: 14.1554 - val_decoder_dense_accuracy: 0.1577 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 101/800\n",
      "4984/4984 [==============================] - 33s 7ms/step - loss: -1.0158 - decoder_dense_loss: 4.1327 - softmax_layer_CC_loss: 0.0179 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.2408 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6848 - val_decoder_dense_loss: 6.1270 - val_softmax_layer_CC_loss: 2.4759 - val_softmax_layer_DC_loss: 14.1352 - val_decoder_dense_accuracy: 0.1561 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 102/800\n",
      "4984/4984 [==============================] - 33s 7ms/step - loss: -1.0185 - decoder_dense_loss: 4.1040 - softmax_layer_CC_loss: 0.0181 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.2450 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7733 - val_decoder_dense_loss: 6.1148 - val_softmax_layer_CC_loss: 2.6315 - val_softmax_layer_DC_loss: 14.1559 - val_decoder_dense_accuracy: 0.1595 - val_softmax_layer_CC_accuracy: 0.6648 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 103/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.0215 - decoder_dense_loss: 4.0793 - softmax_layer_CC_loss: 0.0173 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.2467 - softmax_layer_CC_accuracy: 0.9914 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7409 - val_decoder_dense_loss: 6.1000 - val_softmax_layer_CC_loss: 2.5752 - val_softmax_layer_DC_loss: 14.1529 - val_decoder_dense_accuracy: 0.1606 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 104/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.0225 - decoder_dense_loss: 4.0633 - softmax_layer_CC_loss: 0.0183 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.2484 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6260 - val_decoder_dense_loss: 6.1221 - val_softmax_layer_CC_loss: 2.3784 - val_softmax_layer_DC_loss: 14.1320 - val_decoder_dense_accuracy: 0.1570 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 105/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.0261 - decoder_dense_loss: 4.0347 - softmax_layer_CC_loss: 0.0171 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.2497 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7507 - val_decoder_dense_loss: 6.0965 - val_softmax_layer_CC_loss: 2.5938 - val_softmax_layer_DC_loss: 14.1494 - val_decoder_dense_accuracy: 0.1602 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 106/800\n",
      "4984/4984 [==============================] - 34s 7ms/step - loss: -1.0280 - decoder_dense_loss: 4.0114 - softmax_layer_CC_loss: 0.0177 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.2529 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7002 - val_decoder_dense_loss: 6.1110 - val_softmax_layer_CC_loss: 2.5079 - val_softmax_layer_DC_loss: 14.1413 - val_decoder_dense_accuracy: 0.1583 - val_softmax_layer_CC_accuracy: 0.6600 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 107/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.0306 - decoder_dense_loss: 3.9882 - softmax_layer_CC_loss: 0.0173 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.2557 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6337 - val_decoder_dense_loss: 6.1121 - val_softmax_layer_CC_loss: 2.3941 - val_softmax_layer_DC_loss: 14.1331 - val_decoder_dense_accuracy: 0.1600 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 108/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 32s 6ms/step - loss: -1.0324 - decoder_dense_loss: 3.9673 - softmax_layer_CC_loss: 0.0178 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.2580 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7592 - val_decoder_dense_loss: 6.0998 - val_softmax_layer_CC_loss: 2.6083 - val_softmax_layer_DC_loss: 14.1478 - val_decoder_dense_accuracy: 0.1621 - val_softmax_layer_CC_accuracy: 0.6632 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 109/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.0353 - decoder_dense_loss: 3.9388 - softmax_layer_CC_loss: 0.0177 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.2602 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7931 - val_decoder_dense_loss: 6.0966 - val_softmax_layer_CC_loss: 2.6617 - val_softmax_layer_DC_loss: 14.1492 - val_decoder_dense_accuracy: 0.1622 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 110/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.0371 - decoder_dense_loss: 3.9158 - softmax_layer_CC_loss: 0.0186 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.2631 - softmax_layer_CC_accuracy: 0.9890 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8097 - val_decoder_dense_loss: 6.0994 - val_softmax_layer_CC_loss: 2.6901 - val_softmax_layer_DC_loss: 14.1525 - val_decoder_dense_accuracy: 0.1626 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 111/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.0398 - decoder_dense_loss: 3.8956 - softmax_layer_CC_loss: 0.0173 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.2652 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8057 - val_decoder_dense_loss: 6.0959 - val_softmax_layer_CC_loss: 2.6842 - val_softmax_layer_DC_loss: 14.1602 - val_decoder_dense_accuracy: 0.1613 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 112/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.0414 - decoder_dense_loss: 3.8766 - softmax_layer_CC_loss: 0.0178 - softmax_layer_DC_loss: 14.3961 - decoder_dense_accuracy: 0.2663 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7785 - val_decoder_dense_loss: 6.0899 - val_softmax_layer_CC_loss: 2.6401 - val_softmax_layer_DC_loss: 14.1581 - val_decoder_dense_accuracy: 0.1647 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 113/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.0438 - decoder_dense_loss: 3.8542 - softmax_layer_CC_loss: 0.0178 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.2691 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6584 - val_decoder_dense_loss: 6.1072 - val_softmax_layer_CC_loss: 2.4344 - val_softmax_layer_DC_loss: 14.1381 - val_decoder_dense_accuracy: 0.1617 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 114/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.0459 - decoder_dense_loss: 3.8340 - softmax_layer_CC_loss: 0.0175 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.2702 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7579 - val_decoder_dense_loss: 6.0812 - val_softmax_layer_CC_loss: 2.6089 - val_softmax_layer_DC_loss: 14.1519 - val_decoder_dense_accuracy: 0.1650 - val_softmax_layer_CC_accuracy: 0.6632 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 115/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.0479 - decoder_dense_loss: 3.8112 - softmax_layer_CC_loss: 0.0179 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.2724 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7317 - val_decoder_dense_loss: 6.0834 - val_softmax_layer_CC_loss: 2.5630 - val_softmax_layer_DC_loss: 14.1514 - val_decoder_dense_accuracy: 0.1643 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 116/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.0511 - decoder_dense_loss: 3.7850 - softmax_layer_CC_loss: 0.0170 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.2762 - softmax_layer_CC_accuracy: 0.9890 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7460 - val_decoder_dense_loss: 6.0863 - val_softmax_layer_CC_loss: 2.5834 - val_softmax_layer_DC_loss: 14.1475 - val_decoder_dense_accuracy: 0.1657 - val_softmax_layer_CC_accuracy: 0.6576 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 117/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.0525 - decoder_dense_loss: 3.7671 - softmax_layer_CC_loss: 0.0176 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.2775 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7503 - val_decoder_dense_loss: 6.0871 - val_softmax_layer_CC_loss: 2.5916 - val_softmax_layer_DC_loss: 14.1417 - val_decoder_dense_accuracy: 0.1666 - val_softmax_layer_CC_accuracy: 0.6592 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 118/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.0551 - decoder_dense_loss: 3.7443 - softmax_layer_CC_loss: 0.0170 - softmax_layer_DC_loss: 14.3968 - decoder_dense_accuracy: 0.2810 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7508 - val_decoder_dense_loss: 6.0871 - val_softmax_layer_CC_loss: 2.5946 - val_softmax_layer_DC_loss: 14.1480 - val_decoder_dense_accuracy: 0.1691 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 119/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.0574 - decoder_dense_loss: 3.7222 - softmax_layer_CC_loss: 0.0169 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.2828 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7369 - val_decoder_dense_loss: 6.0987 - val_softmax_layer_CC_loss: 2.5672 - val_softmax_layer_DC_loss: 14.1430 - val_decoder_dense_accuracy: 0.1630 - val_softmax_layer_CC_accuracy: 0.6608 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 120/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.0595 - decoder_dense_loss: 3.7017 - softmax_layer_CC_loss: 0.0169 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.2847 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7625 - val_decoder_dense_loss: 6.0749 - val_softmax_layer_CC_loss: 2.6171 - val_softmax_layer_DC_loss: 14.1583 - val_decoder_dense_accuracy: 0.1685 - val_softmax_layer_CC_accuracy: 0.6600 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 121/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.0615 - decoder_dense_loss: 3.6800 - softmax_layer_CC_loss: 0.0171 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.2874 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7257 - val_decoder_dense_loss: 6.0923 - val_softmax_layer_CC_loss: 2.5473 - val_softmax_layer_DC_loss: 14.1428 - val_decoder_dense_accuracy: 0.1695 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 122/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.0634 - decoder_dense_loss: 3.6583 - softmax_layer_CC_loss: 0.0176 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.2890 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7221 - val_decoder_dense_loss: 6.0869 - val_softmax_layer_CC_loss: 2.5460 - val_softmax_layer_DC_loss: 14.1491 - val_decoder_dense_accuracy: 0.1688 - val_softmax_layer_CC_accuracy: 0.6592 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 123/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.0658 - decoder_dense_loss: 3.6391 - softmax_layer_CC_loss: 0.0169 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.2914 - softmax_layer_CC_accuracy: 0.9916 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7502 - val_decoder_dense_loss: 6.0745 - val_softmax_layer_CC_loss: 2.5940 - val_softmax_layer_DC_loss: 14.1556 - val_decoder_dense_accuracy: 0.1707 - val_softmax_layer_CC_accuracy: 0.6632 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 124/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.0681 - decoder_dense_loss: 3.6140 - softmax_layer_CC_loss: 0.0171 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.2945 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.6867 - val_decoder_dense_loss: 6.0850 - val_softmax_layer_CC_loss: 2.4840 - val_softmax_layer_DC_loss: 14.1409 - val_decoder_dense_accuracy: 0.1682 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 125/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.0698 - decoder_dense_loss: 3.5989 - softmax_layer_CC_loss: 0.0170 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.2952 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7629 - val_decoder_dense_loss: 6.0819 - val_softmax_layer_CC_loss: 2.6131 - val_softmax_layer_DC_loss: 14.1453 - val_decoder_dense_accuracy: 0.1711 - val_softmax_layer_CC_accuracy: 0.6608 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 126/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.0720 - decoder_dense_loss: 3.5711 - softmax_layer_CC_loss: 0.0178 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.2988 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7401 - val_decoder_dense_loss: 6.0895 - val_softmax_layer_CC_loss: 2.5741 - val_softmax_layer_DC_loss: 14.1437 - val_decoder_dense_accuracy: 0.1694 - val_softmax_layer_CC_accuracy: 0.6624 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 127/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.0740 - decoder_dense_loss: 3.5553 - softmax_layer_CC_loss: 0.0171 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.2995 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7908 - val_decoder_dense_loss: 6.0817 - val_softmax_layer_CC_loss: 2.6627 - val_softmax_layer_DC_loss: 14.1505 - val_decoder_dense_accuracy: 0.1696 - val_softmax_layer_CC_accuracy: 0.6600 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 128/800\n",
      "4984/4984 [==============================] - 32s 7ms/step - loss: -1.0758 - decoder_dense_loss: 3.5344 - softmax_layer_CC_loss: 0.0176 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.3026 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7841 - val_decoder_dense_loss: 6.0790 - val_softmax_layer_CC_loss: 2.6529 - val_softmax_layer_DC_loss: 14.1595 - val_decoder_dense_accuracy: 0.1729 - val_softmax_layer_CC_accuracy: 0.6632 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 129/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.0785 - decoder_dense_loss: 3.5090 - softmax_layer_CC_loss: 0.0173 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.3072 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8552 - val_decoder_dense_loss: 6.0714 - val_softmax_layer_CC_loss: 2.7701 - val_softmax_layer_DC_loss: 14.1611 - val_decoder_dense_accuracy: 0.1735 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 130/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.0804 - decoder_dense_loss: 3.4911 - softmax_layer_CC_loss: 0.0170 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.3081 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8037 - val_decoder_dense_loss: 6.0846 - val_softmax_layer_CC_loss: 2.6827 - val_softmax_layer_DC_loss: 14.1496 - val_decoder_dense_accuracy: 0.1716 - val_softmax_layer_CC_accuracy: 0.6616 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 131/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.0815 - decoder_dense_loss: 3.4735 - softmax_layer_CC_loss: 0.0183 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.3104 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7971 - val_decoder_dense_loss: 6.0781 - val_softmax_layer_CC_loss: 2.6714 - val_softmax_layer_DC_loss: 14.1525 - val_decoder_dense_accuracy: 0.1718 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 132/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.0852 - decoder_dense_loss: 3.4413 - softmax_layer_CC_loss: 0.0175 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.3136 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7985 - val_decoder_dense_loss: 6.0917 - val_softmax_layer_CC_loss: 2.6735 - val_softmax_layer_DC_loss: 14.1561 - val_decoder_dense_accuracy: 0.1724 - val_softmax_layer_CC_accuracy: 0.6648 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 133/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.0867 - decoder_dense_loss: 3.4298 - softmax_layer_CC_loss: 0.0169 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.3157 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7964 - val_decoder_dense_loss: 6.0777 - val_softmax_layer_CC_loss: 2.6728 - val_softmax_layer_DC_loss: 14.1526 - val_decoder_dense_accuracy: 0.1743 - val_softmax_layer_CC_accuracy: 0.6608 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 134/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.0882 - decoder_dense_loss: 3.4127 - softmax_layer_CC_loss: 0.0172 - softmax_layer_DC_loss: 14.3992 - decoder_dense_accuracy: 0.3163 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8013 - val_decoder_dense_loss: 6.0755 - val_softmax_layer_CC_loss: 2.6795 - val_softmax_layer_DC_loss: 14.1551 - val_decoder_dense_accuracy: 0.1732 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 135/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.0904 - decoder_dense_loss: 3.3955 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.3192 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7853 - val_decoder_dense_loss: 6.1004 - val_softmax_layer_CC_loss: 2.6450 - val_softmax_layer_DC_loss: 14.1469 - val_decoder_dense_accuracy: 0.1726 - val_softmax_layer_CC_accuracy: 0.6592 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 136/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.0921 - decoder_dense_loss: 3.3737 - softmax_layer_CC_loss: 0.0173 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.3225 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7430 - val_decoder_dense_loss: 6.0980 - val_softmax_layer_CC_loss: 2.5767 - val_softmax_layer_DC_loss: 14.1451 - val_decoder_dense_accuracy: 0.1736 - val_softmax_layer_CC_accuracy: 0.6640 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 137/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.0938 - decoder_dense_loss: 3.3576 - softmax_layer_CC_loss: 0.0170 - softmax_layer_DC_loss: 14.3963 - decoder_dense_accuracy: 0.3234 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8268 - val_decoder_dense_loss: 6.0802 - val_softmax_layer_CC_loss: 2.7187 - val_softmax_layer_DC_loss: 14.1552 - val_decoder_dense_accuracy: 0.1753 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 138/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.0970 - decoder_dense_loss: 3.3282 - softmax_layer_CC_loss: 0.0166 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.3290 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8196 - val_decoder_dense_loss: 6.0938 - val_softmax_layer_CC_loss: 2.7062 - val_softmax_layer_DC_loss: 14.1495 - val_decoder_dense_accuracy: 0.1734 - val_softmax_layer_CC_accuracy: 0.6656 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 139/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.0982 - decoder_dense_loss: 3.3153 - softmax_layer_CC_loss: 0.0167 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.3288 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7885 - val_decoder_dense_loss: 6.0894 - val_softmax_layer_CC_loss: 2.6562 - val_softmax_layer_DC_loss: 14.1487 - val_decoder_dense_accuracy: 0.1767 - val_softmax_layer_CC_accuracy: 0.6656 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 140/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.0989 - decoder_dense_loss: 3.3018 - softmax_layer_CC_loss: 0.0177 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.3292 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8252 - val_decoder_dense_loss: 6.0839 - val_softmax_layer_CC_loss: 2.7172 - val_softmax_layer_DC_loss: 14.1501 - val_decoder_dense_accuracy: 0.1773 - val_softmax_layer_CC_accuracy: 0.6608 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 141/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1026 - decoder_dense_loss: 3.2709 - softmax_layer_CC_loss: 0.0168 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.3356 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8009 - val_decoder_dense_loss: 6.1018 - val_softmax_layer_CC_loss: 2.6732 - val_softmax_layer_DC_loss: 14.1467 - val_decoder_dense_accuracy: 0.1762 - val_softmax_layer_CC_accuracy: 0.6632 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 142/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1046 - decoder_dense_loss: 3.2539 - softmax_layer_CC_loss: 0.0164 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.3384 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8554 - val_decoder_dense_loss: 6.0905 - val_softmax_layer_CC_loss: 2.7676 - val_softmax_layer_DC_loss: 14.1477 - val_decoder_dense_accuracy: 0.1763 - val_softmax_layer_CC_accuracy: 0.6608 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 143/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1059 - decoder_dense_loss: 3.2387 - softmax_layer_CC_loss: 0.0168 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.3389 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8732 - val_decoder_dense_loss: 6.0965 - val_softmax_layer_CC_loss: 2.7932 - val_softmax_layer_DC_loss: 14.1575 - val_decoder_dense_accuracy: 0.1771 - val_softmax_layer_CC_accuracy: 0.6584 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 144/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1070 - decoder_dense_loss: 3.2232 - softmax_layer_CC_loss: 0.0174 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.3405 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8390 - val_decoder_dense_loss: 6.0847 - val_softmax_layer_CC_loss: 2.7437 - val_softmax_layer_DC_loss: 14.1595 - val_decoder_dense_accuracy: 0.1774 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 145/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1092 - decoder_dense_loss: 3.2053 - softmax_layer_CC_loss: 0.0168 - softmax_layer_DC_loss: 14.3988 - decoder_dense_accuracy: 0.3451 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8194 - val_decoder_dense_loss: 6.1017 - val_softmax_layer_CC_loss: 2.7033 - val_softmax_layer_DC_loss: 14.1516 - val_decoder_dense_accuracy: 0.1777 - val_softmax_layer_CC_accuracy: 0.6632 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 146/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.1112 - decoder_dense_loss: 3.1858 - softmax_layer_CC_loss: 0.0167 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.3451 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7750 - val_decoder_dense_loss: 6.1103 - val_softmax_layer_CC_loss: 2.6276 - val_softmax_layer_DC_loss: 14.1487 - val_decoder_dense_accuracy: 0.1768 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 147/800\n",
      "4984/4984 [==============================] - 32s 6ms/step - loss: -1.1128 - decoder_dense_loss: 3.1670 - softmax_layer_CC_loss: 0.0171 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.3500 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8672 - val_decoder_dense_loss: 6.0883 - val_softmax_layer_CC_loss: 2.7909 - val_softmax_layer_DC_loss: 14.1578 - val_decoder_dense_accuracy: 0.1795 - val_softmax_layer_CC_accuracy: 0.6616 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 148/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.1144 - decoder_dense_loss: 3.1410 - softmax_layer_CC_loss: 0.0188 - softmax_layer_DC_loss: 14.3988 - decoder_dense_accuracy: 0.3517 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8283 - val_decoder_dense_loss: 6.1022 - val_softmax_layer_CC_loss: 2.7187 - val_softmax_layer_DC_loss: 14.1478 - val_decoder_dense_accuracy: 0.1775 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 149/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.1175 - decoder_dense_loss: 3.1219 - softmax_layer_CC_loss: 0.0168 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.3558 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8377 - val_decoder_dense_loss: 6.0945 - val_softmax_layer_CC_loss: 2.7393 - val_softmax_layer_DC_loss: 14.1617 - val_decoder_dense_accuracy: 0.1776 - val_softmax_layer_CC_accuracy: 0.6584 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 150/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1182 - decoder_dense_loss: 3.1148 - softmax_layer_CC_loss: 0.0168 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.3575 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7901 - val_decoder_dense_loss: 6.1112 - val_softmax_layer_CC_loss: 2.6554 - val_softmax_layer_DC_loss: 14.1538 - val_decoder_dense_accuracy: 0.1759 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 151/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1204 - decoder_dense_loss: 3.0933 - softmax_layer_CC_loss: 0.0168 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.3593 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8244 - val_decoder_dense_loss: 6.0959 - val_softmax_layer_CC_loss: 2.7141 - val_softmax_layer_DC_loss: 14.1519 - val_decoder_dense_accuracy: 0.1806 - val_softmax_layer_CC_accuracy: 0.6624 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 152/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.1219 - decoder_dense_loss: 3.0784 - softmax_layer_CC_loss: 0.0167 - softmax_layer_DC_loss: 14.3988 - decoder_dense_accuracy: 0.3608 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8539 - val_decoder_dense_loss: 6.1100 - val_softmax_layer_CC_loss: 2.7623 - val_softmax_layer_DC_loss: 14.1518 - val_decoder_dense_accuracy: 0.1771 - val_softmax_layer_CC_accuracy: 0.6600 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 153/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1234 - decoder_dense_loss: 3.0595 - softmax_layer_CC_loss: 0.0173 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.3641 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8411 - val_decoder_dense_loss: 6.1176 - val_softmax_layer_CC_loss: 2.7360 - val_softmax_layer_DC_loss: 14.1496 - val_decoder_dense_accuracy: 0.1776 - val_softmax_layer_CC_accuracy: 0.6576 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 154/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.1257 - decoder_dense_loss: 3.0404 - softmax_layer_CC_loss: 0.0168 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.3672 - softmax_layer_CC_accuracy: 0.9914 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8035 - val_decoder_dense_loss: 6.1181 - val_softmax_layer_CC_loss: 2.6753 - val_softmax_layer_DC_loss: 14.1470 - val_decoder_dense_accuracy: 0.1774 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 155/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1273 - decoder_dense_loss: 3.0253 - softmax_layer_CC_loss: 0.0166 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.3680 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8063 - val_decoder_dense_loss: 6.1309 - val_softmax_layer_CC_loss: 2.6743 - val_softmax_layer_DC_loss: 14.1495 - val_decoder_dense_accuracy: 0.1772 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 156/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1284 - decoder_dense_loss: 3.0143 - softmax_layer_CC_loss: 0.0167 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.3711 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8211 - val_decoder_dense_loss: 6.1114 - val_softmax_layer_CC_loss: 2.7102 - val_softmax_layer_DC_loss: 14.1531 - val_decoder_dense_accuracy: 0.1791 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 157/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.1304 - decoder_dense_loss: 2.9853 - softmax_layer_CC_loss: 0.0182 - softmax_layer_DC_loss: 14.3965 - decoder_dense_accuracy: 0.3754 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.7938 - val_decoder_dense_loss: 6.1183 - val_softmax_layer_CC_loss: 2.6591 - val_softmax_layer_DC_loss: 14.1508 - val_decoder_dense_accuracy: 0.1787 - val_softmax_layer_CC_accuracy: 0.6608 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 158/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1317 - decoder_dense_loss: 2.9738 - softmax_layer_CC_loss: 0.0179 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.3767 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8262 - val_decoder_dense_loss: 6.1366 - val_softmax_layer_CC_loss: 2.7083 - val_softmax_layer_DC_loss: 14.1457 - val_decoder_dense_accuracy: 0.1768 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 159/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1354 - decoder_dense_loss: 2.9457 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.3819 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8588 - val_decoder_dense_loss: 6.1167 - val_softmax_layer_CC_loss: 2.7671 - val_softmax_layer_DC_loss: 14.1517 - val_decoder_dense_accuracy: 0.1808 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 160/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1360 - decoder_dense_loss: 2.9382 - softmax_layer_CC_loss: 0.0166 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.3803 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8602 - val_decoder_dense_loss: 6.1094 - val_softmax_layer_CC_loss: 2.7748 - val_softmax_layer_DC_loss: 14.1572 - val_decoder_dense_accuracy: 0.1801 - val_softmax_layer_CC_accuracy: 0.6616 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 161/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.1371 - decoder_dense_loss: 2.9250 - softmax_layer_CC_loss: 0.0170 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.3830 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8044 - val_decoder_dense_loss: 6.1267 - val_softmax_layer_CC_loss: 2.6760 - val_softmax_layer_DC_loss: 14.1515 - val_decoder_dense_accuracy: 0.1766 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0794\n",
      "Epoch 162/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1393 - decoder_dense_loss: 2.9073 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3969 - decoder_dense_accuracy: 0.3859 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8131 - val_decoder_dense_loss: 6.1285 - val_softmax_layer_CC_loss: 2.6897 - val_softmax_layer_DC_loss: 14.1549 - val_decoder_dense_accuracy: 0.1812 - val_softmax_layer_CC_accuracy: 0.6584 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 163/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1408 - decoder_dense_loss: 2.8910 - softmax_layer_CC_loss: 0.0165 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.3882 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8528 - val_decoder_dense_loss: 6.1310 - val_softmax_layer_CC_loss: 2.7567 - val_softmax_layer_DC_loss: 14.1515 - val_decoder_dense_accuracy: 0.1803 - val_softmax_layer_CC_accuracy: 0.6576 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 164/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1422 - decoder_dense_loss: 2.8759 - softmax_layer_CC_loss: 0.0166 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.3903 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8739 - val_decoder_dense_loss: 6.1281 - val_softmax_layer_CC_loss: 2.7915 - val_softmax_layer_DC_loss: 14.1601 - val_decoder_dense_accuracy: 0.1815 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 165/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1439 - decoder_dense_loss: 2.8614 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.3927 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8473 - val_decoder_dense_loss: 6.1352 - val_softmax_layer_CC_loss: 2.7527 - val_softmax_layer_DC_loss: 14.1595 - val_decoder_dense_accuracy: 0.1789 - val_softmax_layer_CC_accuracy: 0.6600 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 166/800\n",
      "4984/4984 [==============================] - 36s 7ms/step - loss: -1.1457 - decoder_dense_loss: 2.8428 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.3970 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8705 - val_decoder_dense_loss: 6.1493 - val_softmax_layer_CC_loss: 2.7828 - val_softmax_layer_DC_loss: 14.1573 - val_decoder_dense_accuracy: 0.1771 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 167/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.1467 - decoder_dense_loss: 2.8172 - softmax_layer_CC_loss: 0.0189 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.4002 - softmax_layer_CC_accuracy: 0.9884 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8674 - val_decoder_dense_loss: 6.1374 - val_softmax_layer_CC_loss: 2.7805 - val_softmax_layer_DC_loss: 14.1637 - val_decoder_dense_accuracy: 0.1816 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 168/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 32s 7ms/step - loss: -1.1491 - decoder_dense_loss: 2.8075 - softmax_layer_CC_loss: 0.0166 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.4017 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8539 - val_decoder_dense_loss: 6.1512 - val_softmax_layer_CC_loss: 2.7566 - val_softmax_layer_DC_loss: 14.1558 - val_decoder_dense_accuracy: 0.1807 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 169/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1507 - decoder_dense_loss: 2.7925 - softmax_layer_CC_loss: 0.0165 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.4036 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8348 - val_decoder_dense_loss: 6.1448 - val_softmax_layer_CC_loss: 2.7226 - val_softmax_layer_DC_loss: 14.1598 - val_decoder_dense_accuracy: 0.1817 - val_softmax_layer_CC_accuracy: 0.6600 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 170/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1521 - decoder_dense_loss: 2.7808 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.4057 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8852 - val_decoder_dense_loss: 6.1393 - val_softmax_layer_CC_loss: 2.8088 - val_softmax_layer_DC_loss: 14.1641 - val_decoder_dense_accuracy: 0.1813 - val_softmax_layer_CC_accuracy: 0.6584 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 171/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.1534 - decoder_dense_loss: 2.7651 - softmax_layer_CC_loss: 0.0165 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.4088 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8557 - val_decoder_dense_loss: 6.1405 - val_softmax_layer_CC_loss: 2.7610 - val_softmax_layer_DC_loss: 14.1615 - val_decoder_dense_accuracy: 0.1824 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 172/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1549 - decoder_dense_loss: 2.7478 - softmax_layer_CC_loss: 0.0167 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.4095 - softmax_layer_CC_accuracy: 0.9892 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8306 - val_decoder_dense_loss: 6.1641 - val_softmax_layer_CC_loss: 2.7157 - val_softmax_layer_DC_loss: 14.1608 - val_decoder_dense_accuracy: 0.1790 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 173/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1556 - decoder_dense_loss: 2.7371 - softmax_layer_CC_loss: 0.0175 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.4127 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8381 - val_decoder_dense_loss: 6.1486 - val_softmax_layer_CC_loss: 2.7282 - val_softmax_layer_DC_loss: 14.1590 - val_decoder_dense_accuracy: 0.1826 - val_softmax_layer_CC_accuracy: 0.6592 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 174/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1588 - decoder_dense_loss: 2.7128 - softmax_layer_CC_loss: 0.0162 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.4167 - softmax_layer_CC_accuracy: 0.9890 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8794 - val_decoder_dense_loss: 6.1481 - val_softmax_layer_CC_loss: 2.8011 - val_softmax_layer_DC_loss: 14.1632 - val_decoder_dense_accuracy: 0.1787 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 175/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1596 - decoder_dense_loss: 2.7042 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.4170 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8851 - val_decoder_dense_loss: 6.1652 - val_softmax_layer_CC_loss: 2.8038 - val_softmax_layer_DC_loss: 14.1605 - val_decoder_dense_accuracy: 0.1806 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 176/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1609 - decoder_dense_loss: 2.6900 - softmax_layer_CC_loss: 0.0165 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.4206 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8871 - val_decoder_dense_loss: 6.1699 - val_softmax_layer_CC_loss: 2.8076 - val_softmax_layer_DC_loss: 14.1608 - val_decoder_dense_accuracy: 0.1803 - val_softmax_layer_CC_accuracy: 0.6576 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 177/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1624 - decoder_dense_loss: 2.6744 - softmax_layer_CC_loss: 0.0166 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.4241 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8616 - val_decoder_dense_loss: 6.1690 - val_softmax_layer_CC_loss: 2.7656 - val_softmax_layer_DC_loss: 14.1585 - val_decoder_dense_accuracy: 0.1833 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 178/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1641 - decoder_dense_loss: 2.6587 - softmax_layer_CC_loss: 0.0164 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.4272 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9164 - val_decoder_dense_loss: 6.1716 - val_softmax_layer_CC_loss: 2.8580 - val_softmax_layer_DC_loss: 14.1690 - val_decoder_dense_accuracy: 0.1807 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 179/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.1656 - decoder_dense_loss: 2.6444 - softmax_layer_CC_loss: 0.0162 - softmax_layer_DC_loss: 14.3965 - decoder_dense_accuracy: 0.4292 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8434 - val_decoder_dense_loss: 6.1842 - val_softmax_layer_CC_loss: 2.7348 - val_softmax_layer_DC_loss: 14.1627 - val_decoder_dense_accuracy: 0.1824 - val_softmax_layer_CC_accuracy: 0.6496 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 180/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.1672 - decoder_dense_loss: 2.6268 - softmax_layer_CC_loss: 0.0166 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.4318 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8652 - val_decoder_dense_loss: 6.1737 - val_softmax_layer_CC_loss: 2.7723 - val_softmax_layer_DC_loss: 14.1667 - val_decoder_dense_accuracy: 0.1824 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 181/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.1682 - decoder_dense_loss: 2.6117 - softmax_layer_CC_loss: 0.0173 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.4322 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8435 - val_decoder_dense_loss: 6.1823 - val_softmax_layer_CC_loss: 2.7362 - val_softmax_layer_DC_loss: 14.1604 - val_decoder_dense_accuracy: 0.1805 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 182/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1702 - decoder_dense_loss: 2.5993 - softmax_layer_CC_loss: 0.0161 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.4351 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8457 - val_decoder_dense_loss: 6.1897 - val_softmax_layer_CC_loss: 2.7355 - val_softmax_layer_DC_loss: 14.1678 - val_decoder_dense_accuracy: 0.1829 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 183/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1713 - decoder_dense_loss: 2.5878 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.4369 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9291 - val_decoder_dense_loss: 6.1807 - val_softmax_layer_CC_loss: 2.8796 - val_softmax_layer_DC_loss: 14.1734 - val_decoder_dense_accuracy: 0.1824 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 184/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.1731 - decoder_dense_loss: 2.5691 - softmax_layer_CC_loss: 0.0164 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.4411 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8370 - val_decoder_dense_loss: 6.2014 - val_softmax_layer_CC_loss: 2.7186 - val_softmax_layer_DC_loss: 14.1650 - val_decoder_dense_accuracy: 0.1807 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 185/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.1739 - decoder_dense_loss: 2.5608 - softmax_layer_CC_loss: 0.0164 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.4413 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8690 - val_decoder_dense_loss: 6.2052 - val_softmax_layer_CC_loss: 2.7691 - val_softmax_layer_DC_loss: 14.1637 - val_decoder_dense_accuracy: 0.1802 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 186/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.1758 - decoder_dense_loss: 2.5415 - softmax_layer_CC_loss: 0.0164 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.4471 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9135 - val_decoder_dense_loss: 6.1917 - val_softmax_layer_CC_loss: 2.8501 - val_softmax_layer_DC_loss: 14.1729 - val_decoder_dense_accuracy: 0.1836 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 187/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1772 - decoder_dense_loss: 2.5294 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.4482 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8926 - val_decoder_dense_loss: 6.2082 - val_softmax_layer_CC_loss: 2.8097 - val_softmax_layer_DC_loss: 14.1656 - val_decoder_dense_accuracy: 0.1816 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 188/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1782 - decoder_dense_loss: 2.5171 - softmax_layer_CC_loss: 0.0164 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.4492 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9332 - val_decoder_dense_loss: 6.2002 - val_softmax_layer_CC_loss: 2.8837 - val_softmax_layer_DC_loss: 14.1719 - val_decoder_dense_accuracy: 0.1834 - val_softmax_layer_CC_accuracy: 0.6592 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 189/800\n",
      "4984/4984 [==============================] - 37s 7ms/step - loss: -1.1792 - decoder_dense_loss: 2.5059 - softmax_layer_CC_loss: 0.0166 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.4520 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8066 - val_decoder_dense_loss: 6.2236 - val_softmax_layer_CC_loss: 2.6617 - val_softmax_layer_DC_loss: 14.1560 - val_decoder_dense_accuracy: 0.1819 - val_softmax_layer_CC_accuracy: 0.6624 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 190/800\n",
      "4984/4984 [==============================] - 33s 7ms/step - loss: -1.1813 - decoder_dense_loss: 2.4884 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3969 - decoder_dense_accuracy: 0.4526 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9109 - val_decoder_dense_loss: 6.2121 - val_softmax_layer_CC_loss: 2.8425 - val_softmax_layer_DC_loss: 14.1708 - val_decoder_dense_accuracy: 0.1813 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 191/800\n",
      "4984/4984 [==============================] - 33s 7ms/step - loss: -1.1827 - decoder_dense_loss: 2.4724 - softmax_layer_CC_loss: 0.0164 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.4574 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9428 - val_decoder_dense_loss: 6.2238 - val_softmax_layer_CC_loss: 2.8976 - val_softmax_layer_DC_loss: 14.1763 - val_decoder_dense_accuracy: 0.1824 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 192/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1838 - decoder_dense_loss: 2.4624 - softmax_layer_CC_loss: 0.0162 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.4600 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9285 - val_decoder_dense_loss: 6.2118 - val_softmax_layer_CC_loss: 2.8725 - val_softmax_layer_DC_loss: 14.1730 - val_decoder_dense_accuracy: 0.1824 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 193/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1851 - decoder_dense_loss: 2.4495 - softmax_layer_CC_loss: 0.0162 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.4597 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8533 - val_decoder_dense_loss: 6.2330 - val_softmax_layer_CC_loss: 2.7441 - val_softmax_layer_DC_loss: 14.1650 - val_decoder_dense_accuracy: 0.1833 - val_softmax_layer_CC_accuracy: 0.6584 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 194/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.1856 - decoder_dense_loss: 2.4317 - softmax_layer_CC_loss: 0.0184 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.4657 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8645 - val_decoder_dense_loss: 6.2413 - val_softmax_layer_CC_loss: 2.7607 - val_softmax_layer_DC_loss: 14.1633 - val_decoder_dense_accuracy: 0.1819 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 195/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1881 - decoder_dense_loss: 2.4206 - softmax_layer_CC_loss: 0.0161 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.4665 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9084 - val_decoder_dense_loss: 6.2391 - val_softmax_layer_CC_loss: 2.8321 - val_softmax_layer_DC_loss: 14.1663 - val_decoder_dense_accuracy: 0.1810 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 196/800\n",
      "4984/4984 [==============================] - 27s 6ms/step - loss: -1.1891 - decoder_dense_loss: 2.4105 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.4687 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9022 - val_decoder_dense_loss: 6.2589 - val_softmax_layer_CC_loss: 2.8166 - val_softmax_layer_DC_loss: 14.1648 - val_decoder_dense_accuracy: 0.1816 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 197/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.1904 - decoder_dense_loss: 2.3961 - softmax_layer_CC_loss: 0.0162 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.4711 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8673 - val_decoder_dense_loss: 6.2466 - val_softmax_layer_CC_loss: 2.7624 - val_softmax_layer_DC_loss: 14.1676 - val_decoder_dense_accuracy: 0.1822 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 198/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1920 - decoder_dense_loss: 2.3823 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.4730 - softmax_layer_CC_accuracy: 0.9916 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9302 - val_decoder_dense_loss: 6.2535 - val_softmax_layer_CC_loss: 2.8661 - val_softmax_layer_DC_loss: 14.1688 - val_decoder_dense_accuracy: 0.1839 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 199/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.1931 - decoder_dense_loss: 2.3695 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3992 - decoder_dense_accuracy: 0.4767 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9401 - val_decoder_dense_loss: 6.2532 - val_softmax_layer_CC_loss: 2.8837 - val_softmax_layer_DC_loss: 14.1769 - val_decoder_dense_accuracy: 0.1820 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 200/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.1942 - decoder_dense_loss: 2.3609 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.4777 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9013 - val_decoder_dense_loss: 6.2664 - val_softmax_layer_CC_loss: 2.8193 - val_softmax_layer_DC_loss: 14.1728 - val_decoder_dense_accuracy: 0.1830 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 201/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.1953 - decoder_dense_loss: 2.3478 - softmax_layer_CC_loss: 0.0162 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.4803 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8510 - val_decoder_dense_loss: 6.2676 - val_softmax_layer_CC_loss: 2.7349 - val_softmax_layer_DC_loss: 14.1686 - val_decoder_dense_accuracy: 0.1815 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0786\n",
      "Epoch 202/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.1969 - decoder_dense_loss: 2.3327 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.4833 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9473 - val_decoder_dense_loss: 6.2691 - val_softmax_layer_CC_loss: 2.8947 - val_softmax_layer_DC_loss: 14.1764 - val_decoder_dense_accuracy: 0.1820 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 203/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1979 - decoder_dense_loss: 2.3213 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.4842 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9076 - val_decoder_dense_loss: 6.2775 - val_softmax_layer_CC_loss: 2.8259 - val_softmax_layer_DC_loss: 14.1702 - val_decoder_dense_accuracy: 0.1840 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 204/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.1995 - decoder_dense_loss: 2.3060 - softmax_layer_CC_loss: 0.0161 - softmax_layer_DC_loss: 14.3967 - decoder_dense_accuracy: 0.4877 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8978 - val_decoder_dense_loss: 6.2639 - val_softmax_layer_CC_loss: 2.8112 - val_softmax_layer_DC_loss: 14.1730 - val_decoder_dense_accuracy: 0.1846 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 205/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2004 - decoder_dense_loss: 2.2966 - softmax_layer_CC_loss: 0.0162 - softmax_layer_DC_loss: 14.3988 - decoder_dense_accuracy: 0.4902 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9245 - val_decoder_dense_loss: 6.2701 - val_softmax_layer_CC_loss: 2.8567 - val_softmax_layer_DC_loss: 14.1750 - val_decoder_dense_accuracy: 0.1864 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 206/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2019 - decoder_dense_loss: 2.2845 - softmax_layer_CC_loss: 0.0158 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.4908 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9089 - val_decoder_dense_loss: 6.2824 - val_softmax_layer_CC_loss: 2.8251 - val_softmax_layer_DC_loss: 14.1707 - val_decoder_dense_accuracy: 0.1823 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 207/800\n",
      "4984/4984 [==============================] - 33s 7ms/step - loss: -1.2027 - decoder_dense_loss: 2.2727 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.4939 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9369 - val_decoder_dense_loss: 6.2829 - val_softmax_layer_CC_loss: 2.8789 - val_softmax_layer_DC_loss: 14.1820 - val_decoder_dense_accuracy: 0.1872 - val_softmax_layer_CC_accuracy: 0.6496 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 208/800\n",
      "4984/4984 [==============================] - 33s 7ms/step - loss: -1.2043 - decoder_dense_loss: 2.2591 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.4986 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8967 - val_decoder_dense_loss: 6.3036 - val_softmax_layer_CC_loss: 2.8025 - val_softmax_layer_DC_loss: 14.1697 - val_decoder_dense_accuracy: 0.1831 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 209/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2061 - decoder_dense_loss: 2.2471 - softmax_layer_CC_loss: 0.0149 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.4995 - softmax_layer_CC_accuracy: 0.9932 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9239 - val_decoder_dense_loss: 6.2991 - val_softmax_layer_CC_loss: 2.8533 - val_softmax_layer_DC_loss: 14.1762 - val_decoder_dense_accuracy: 0.1838 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 210/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2063 - decoder_dense_loss: 2.2370 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.5026 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9458 - val_decoder_dense_loss: 6.3090 - val_softmax_layer_CC_loss: 2.8871 - val_softmax_layer_DC_loss: 14.1785 - val_decoder_dense_accuracy: 0.1852 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 211/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2086 - decoder_dense_loss: 2.2199 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.5047 - softmax_layer_CC_accuracy: 0.9930 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9516 - val_decoder_dense_loss: 6.3009 - val_softmax_layer_CC_loss: 2.8986 - val_softmax_layer_DC_loss: 14.1750 - val_decoder_dense_accuracy: 0.1865 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 212/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2088 - decoder_dense_loss: 2.2143 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.5074 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9165 - val_decoder_dense_loss: 6.3137 - val_softmax_layer_CC_loss: 2.8355 - val_softmax_layer_DC_loss: 14.1737 - val_decoder_dense_accuracy: 0.1853 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 213/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2102 - decoder_dense_loss: 2.2019 - softmax_layer_CC_loss: 0.0158 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.5090 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9286 - val_decoder_dense_loss: 6.3182 - val_softmax_layer_CC_loss: 2.8531 - val_softmax_layer_DC_loss: 14.1781 - val_decoder_dense_accuracy: 0.1849 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 214/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2113 - decoder_dense_loss: 2.1890 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.5113 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9386 - val_decoder_dense_loss: 6.3219 - val_softmax_layer_CC_loss: 2.8709 - val_softmax_layer_DC_loss: 14.1762 - val_decoder_dense_accuracy: 0.1834 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 215/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2121 - decoder_dense_loss: 2.1807 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3967 - decoder_dense_accuracy: 0.5134 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9206 - val_decoder_dense_loss: 6.3236 - val_softmax_layer_CC_loss: 2.8393 - val_softmax_layer_DC_loss: 14.1790 - val_decoder_dense_accuracy: 0.1852 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 216/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2133 - decoder_dense_loss: 2.1684 - softmax_layer_CC_loss: 0.0161 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.5137 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8961 - val_decoder_dense_loss: 6.3254 - val_softmax_layer_CC_loss: 2.7996 - val_softmax_layer_DC_loss: 14.1772 - val_decoder_dense_accuracy: 0.1846 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 217/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2149 - decoder_dense_loss: 2.1526 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.5185 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8752 - val_decoder_dense_loss: 6.3508 - val_softmax_layer_CC_loss: 2.7596 - val_softmax_layer_DC_loss: 14.1777 - val_decoder_dense_accuracy: 0.1832 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 218/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2140 - decoder_dense_loss: 2.1522 - softmax_layer_CC_loss: 0.0177 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.5187 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9523 - val_decoder_dense_loss: 6.3296 - val_softmax_layer_CC_loss: 2.8949 - val_softmax_layer_DC_loss: 14.1812 - val_decoder_dense_accuracy: 0.1840 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 219/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2187 - decoder_dense_loss: 2.1161 - softmax_layer_CC_loss: 0.0158 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.5265 - softmax_layer_CC_accuracy: 0.9914 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9229 - val_decoder_dense_loss: 6.3483 - val_softmax_layer_CC_loss: 2.8435 - val_softmax_layer_DC_loss: 14.1795 - val_decoder_dense_accuracy: 0.1866 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 220/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2179 - decoder_dense_loss: 2.1213 - softmax_layer_CC_loss: 0.0162 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.5245 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8759 - val_decoder_dense_loss: 6.3616 - val_softmax_layer_CC_loss: 2.7598 - val_softmax_layer_DC_loss: 14.1781 - val_decoder_dense_accuracy: 0.1824 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 221/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2178 - decoder_dense_loss: 2.1075 - softmax_layer_CC_loss: 0.0187 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.5285 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8964 - val_decoder_dense_loss: 6.3663 - val_softmax_layer_CC_loss: 2.7921 - val_softmax_layer_DC_loss: 14.1788 - val_decoder_dense_accuracy: 0.1833 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 222/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2209 - decoder_dense_loss: 2.0932 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3988 - decoder_dense_accuracy: 0.5330 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8957 - val_decoder_dense_loss: 6.3774 - val_softmax_layer_CC_loss: 2.7904 - val_softmax_layer_DC_loss: 14.1753 - val_decoder_dense_accuracy: 0.1829 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 223/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2211 - decoder_dense_loss: 2.0918 - softmax_layer_CC_loss: 0.0158 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.5311 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8917 - val_decoder_dense_loss: 6.3693 - val_softmax_layer_CC_loss: 2.7853 - val_softmax_layer_DC_loss: 14.1776 - val_decoder_dense_accuracy: 0.1837 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 224/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2223 - decoder_dense_loss: 2.0772 - softmax_layer_CC_loss: 0.0162 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.5329 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9221 - val_decoder_dense_loss: 6.3638 - val_softmax_layer_CC_loss: 2.8403 - val_softmax_layer_DC_loss: 14.1833 - val_decoder_dense_accuracy: 0.1853 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 225/800\n",
      "4984/4984 [==============================] - 35s 7ms/step - loss: -1.2228 - decoder_dense_loss: 2.0734 - softmax_layer_CC_loss: 0.0161 - softmax_layer_DC_loss: 14.3965 - decoder_dense_accuracy: 0.5353 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9379 - val_decoder_dense_loss: 6.3748 - val_softmax_layer_CC_loss: 2.8627 - val_softmax_layer_DC_loss: 14.1828 - val_decoder_dense_accuracy: 0.1878 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 226/800\n",
      "4984/4984 [==============================] - 32s 6ms/step - loss: -1.2248 - decoder_dense_loss: 2.0552 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.5394 - softmax_layer_CC_accuracy: 0.9918 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8921 - val_decoder_dense_loss: 6.3944 - val_softmax_layer_CC_loss: 2.7838 - val_softmax_layer_DC_loss: 14.1790 - val_decoder_dense_accuracy: 0.1844 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 227/800\n",
      "4984/4984 [==============================] - 32s 6ms/step - loss: -1.2256 - decoder_dense_loss: 2.0456 - softmax_layer_CC_loss: 0.0161 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.5416 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9308 - val_decoder_dense_loss: 6.3826 - val_softmax_layer_CC_loss: 2.8470 - val_softmax_layer_DC_loss: 14.1816 - val_decoder_dense_accuracy: 0.1864 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 228/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 39s 8ms/step - loss: -1.2264 - decoder_dense_loss: 2.0382 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3988 - decoder_dense_accuracy: 0.5426 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8976 - val_decoder_dense_loss: 6.3733 - val_softmax_layer_CC_loss: 2.7997 - val_softmax_layer_DC_loss: 14.1809 - val_decoder_dense_accuracy: 0.1879 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 229/800\n",
      "4984/4984 [==============================] - 38s 8ms/step - loss: -1.2275 - decoder_dense_loss: 2.0273 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.5445 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9502 - val_decoder_dense_loss: 6.3977 - val_softmax_layer_CC_loss: 2.8794 - val_softmax_layer_DC_loss: 14.1821 - val_decoder_dense_accuracy: 0.1860 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 230/800\n",
      "4984/4984 [==============================] - 36s 7ms/step - loss: -1.2290 - decoder_dense_loss: 2.0115 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.5485 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9717 - val_decoder_dense_loss: 6.4031 - val_softmax_layer_CC_loss: 2.9090 - val_softmax_layer_DC_loss: 14.1816 - val_decoder_dense_accuracy: 0.1806 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 231/800\n",
      "4984/4984 [==============================] - 32s 6ms/step - loss: -1.2286 - decoder_dense_loss: 2.0097 - softmax_layer_CC_loss: 0.0170 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.5477 - softmax_layer_CC_accuracy: 0.9892 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9118 - val_decoder_dense_loss: 6.4072 - val_softmax_layer_CC_loss: 2.8126 - val_softmax_layer_DC_loss: 14.1752 - val_decoder_dense_accuracy: 0.1868 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 232/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2319 - decoder_dense_loss: 1.9853 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.5530 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9487 - val_decoder_dense_loss: 6.4165 - val_softmax_layer_CC_loss: 2.8711 - val_softmax_layer_DC_loss: 14.1793 - val_decoder_dense_accuracy: 0.1856 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 233/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2317 - decoder_dense_loss: 1.9849 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.5529 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8910 - val_decoder_dense_loss: 6.4123 - val_softmax_layer_CC_loss: 2.7773 - val_softmax_layer_DC_loss: 14.1763 - val_decoder_dense_accuracy: 0.1853 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 234/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2322 - decoder_dense_loss: 1.9790 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.5554 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9103 - val_decoder_dense_loss: 6.4133 - val_softmax_layer_CC_loss: 2.8140 - val_softmax_layer_DC_loss: 14.1842 - val_decoder_dense_accuracy: 0.1894 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 235/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2340 - decoder_dense_loss: 1.9636 - softmax_layer_CC_loss: 0.0158 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.5592 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9656 - val_decoder_dense_loss: 6.4286 - val_softmax_layer_CC_loss: 2.8969 - val_softmax_layer_DC_loss: 14.1793 - val_decoder_dense_accuracy: 0.1887 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 236/800\n",
      "4984/4984 [==============================] - 34s 7ms/step - loss: -1.2346 - decoder_dense_loss: 1.9569 - softmax_layer_CC_loss: 0.0157 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.5602 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9296 - val_decoder_dense_loss: 6.4310 - val_softmax_layer_CC_loss: 2.8357 - val_softmax_layer_DC_loss: 14.1777 - val_decoder_dense_accuracy: 0.1877 - val_softmax_layer_CC_accuracy: 0.6584 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 237/800\n",
      "4984/4984 [==============================] - 33s 7ms/step - loss: -1.2355 - decoder_dense_loss: 1.9418 - softmax_layer_CC_loss: 0.0169 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.5630 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9216 - val_decoder_dense_loss: 6.4398 - val_softmax_layer_CC_loss: 2.8231 - val_softmax_layer_DC_loss: 14.1781 - val_decoder_dense_accuracy: 0.1891 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 238/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2371 - decoder_dense_loss: 1.9317 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.5632 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9402 - val_decoder_dense_loss: 6.4496 - val_softmax_layer_CC_loss: 2.8538 - val_softmax_layer_DC_loss: 14.1844 - val_decoder_dense_accuracy: 0.1877 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 239/800\n",
      "4984/4984 [==============================] - 33s 7ms/step - loss: -1.2376 - decoder_dense_loss: 1.9265 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3967 - decoder_dense_accuracy: 0.5665 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8929 - val_decoder_dense_loss: 6.4585 - val_softmax_layer_CC_loss: 2.7705 - val_softmax_layer_DC_loss: 14.1784 - val_decoder_dense_accuracy: 0.1864 - val_softmax_layer_CC_accuracy: 0.6496 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 240/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2388 - decoder_dense_loss: 1.9164 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.5695 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9154 - val_decoder_dense_loss: 6.4584 - val_softmax_layer_CC_loss: 2.8150 - val_softmax_layer_DC_loss: 14.1795 - val_decoder_dense_accuracy: 0.1872 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 241/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2394 - decoder_dense_loss: 1.9058 - softmax_layer_CC_loss: 0.0163 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.5697 - softmax_layer_CC_accuracy: 0.9890 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9335 - val_decoder_dense_loss: 6.4928 - val_softmax_layer_CC_loss: 2.8343 - val_softmax_layer_DC_loss: 14.1755 - val_decoder_dense_accuracy: 0.1825 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 242/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2408 - decoder_dense_loss: 1.8953 - softmax_layer_CC_loss: 0.0158 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.5735 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9498 - val_decoder_dense_loss: 6.4784 - val_softmax_layer_CC_loss: 2.8609 - val_softmax_layer_DC_loss: 14.1806 - val_decoder_dense_accuracy: 0.1852 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 243/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2418 - decoder_dense_loss: 1.8858 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.5750 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9376 - val_decoder_dense_loss: 6.4780 - val_softmax_layer_CC_loss: 2.8434 - val_softmax_layer_DC_loss: 14.1767 - val_decoder_dense_accuracy: 0.1894 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 244/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2426 - decoder_dense_loss: 1.8763 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.5773 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9207 - val_decoder_dense_loss: 6.4863 - val_softmax_layer_CC_loss: 2.8102 - val_softmax_layer_DC_loss: 14.1774 - val_decoder_dense_accuracy: 0.1882 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 245/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2435 - decoder_dense_loss: 1.8674 - softmax_layer_CC_loss: 0.0160 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.5790 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9449 - val_decoder_dense_loss: 6.4833 - val_softmax_layer_CC_loss: 2.8565 - val_softmax_layer_DC_loss: 14.1795 - val_decoder_dense_accuracy: 0.1872 - val_softmax_layer_CC_accuracy: 0.6455 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 246/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2445 - decoder_dense_loss: 1.8579 - softmax_layer_CC_loss: 0.0158 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.5807 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9341 - val_decoder_dense_loss: 6.4873 - val_softmax_layer_CC_loss: 2.8342 - val_softmax_layer_DC_loss: 14.1803 - val_decoder_dense_accuracy: 0.1877 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 247/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2457 - decoder_dense_loss: 1.8472 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.5849 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9590 - val_decoder_dense_loss: 6.4806 - val_softmax_layer_CC_loss: 2.8790 - val_softmax_layer_DC_loss: 14.1794 - val_decoder_dense_accuracy: 0.1917 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 248/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2464 - decoder_dense_loss: 1.8401 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.5835 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9703 - val_decoder_dense_loss: 6.5024 - val_softmax_layer_CC_loss: 2.8957 - val_softmax_layer_DC_loss: 14.1805 - val_decoder_dense_accuracy: 0.1877 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 249/800\n",
      "4984/4984 [==============================] - 34s 7ms/step - loss: -1.2466 - decoder_dense_loss: 1.8319 - softmax_layer_CC_loss: 0.0167 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.5874 - softmax_layer_CC_accuracy: 0.9914 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9211 - val_decoder_dense_loss: 6.4992 - val_softmax_layer_CC_loss: 2.8100 - val_softmax_layer_DC_loss: 14.1811 - val_decoder_dense_accuracy: 0.1870 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 250/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2484 - decoder_dense_loss: 1.8183 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.5888 - softmax_layer_CC_accuracy: 0.9914 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9596 - val_decoder_dense_loss: 6.5003 - val_softmax_layer_CC_loss: 2.8770 - val_softmax_layer_DC_loss: 14.1851 - val_decoder_dense_accuracy: 0.1896 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 251/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.2487 - decoder_dense_loss: 1.8168 - softmax_layer_CC_loss: 0.0157 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.5912 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9302 - val_decoder_dense_loss: 6.5218 - val_softmax_layer_CC_loss: 2.8213 - val_softmax_layer_DC_loss: 14.1789 - val_decoder_dense_accuracy: 0.1847 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 252/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2502 - decoder_dense_loss: 1.8021 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.5949 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9689 - val_decoder_dense_loss: 6.5195 - val_softmax_layer_CC_loss: 2.8888 - val_softmax_layer_DC_loss: 14.1818 - val_decoder_dense_accuracy: 0.1894 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 253/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2510 - decoder_dense_loss: 1.7940 - softmax_layer_CC_loss: 0.0157 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.5931 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9936 - val_decoder_dense_loss: 6.5143 - val_softmax_layer_CC_loss: 2.9303 - val_softmax_layer_DC_loss: 14.1793 - val_decoder_dense_accuracy: 0.1893 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 254/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2472 - decoder_dense_loss: 1.7998 - softmax_layer_CC_loss: 0.0209 - softmax_layer_DC_loss: 14.3967 - decoder_dense_accuracy: 0.5929 - softmax_layer_CC_accuracy: 0.9880 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9211 - val_decoder_dense_loss: 6.5187 - val_softmax_layer_CC_loss: 2.8088 - val_softmax_layer_DC_loss: 14.1798 - val_decoder_dense_accuracy: 0.1895 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 255/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2552 - decoder_dense_loss: 1.7542 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3988 - decoder_dense_accuracy: 0.6073 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9269 - val_decoder_dense_loss: 6.5533 - val_softmax_layer_CC_loss: 2.8122 - val_softmax_layer_DC_loss: 14.1762 - val_decoder_dense_accuracy: 0.1871 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 256/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2542 - decoder_dense_loss: 1.7612 - softmax_layer_CC_loss: 0.0158 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.6030 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9053 - val_decoder_dense_loss: 6.5401 - val_softmax_layer_CC_loss: 2.7779 - val_softmax_layer_DC_loss: 14.1770 - val_decoder_dense_accuracy: 0.1869 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 257/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2542 - decoder_dense_loss: 1.7613 - softmax_layer_CC_loss: 0.0157 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.6025 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8954 - val_decoder_dense_loss: 6.5491 - val_softmax_layer_CC_loss: 2.7575 - val_softmax_layer_DC_loss: 14.1776 - val_decoder_dense_accuracy: 0.1878 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 258/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2550 - decoder_dense_loss: 1.7540 - softmax_layer_CC_loss: 0.0157 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.6037 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8933 - val_decoder_dense_loss: 6.5683 - val_softmax_layer_CC_loss: 2.7527 - val_softmax_layer_DC_loss: 14.1782 - val_decoder_dense_accuracy: 0.1872 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 259/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2556 - decoder_dense_loss: 1.7477 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.6054 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9277 - val_decoder_dense_loss: 6.5578 - val_softmax_layer_CC_loss: 2.8141 - val_softmax_layer_DC_loss: 14.1797 - val_decoder_dense_accuracy: 0.1877 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 260/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.2577 - decoder_dense_loss: 1.7287 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.6102 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9659 - val_decoder_dense_loss: 6.5546 - val_softmax_layer_CC_loss: 2.8792 - val_softmax_layer_DC_loss: 14.1798 - val_decoder_dense_accuracy: 0.1885 - val_softmax_layer_CC_accuracy: 0.6496 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 261/800\n",
      "4984/4984 [==============================] - 26s 5ms/step - loss: -1.2577 - decoder_dense_loss: 1.7272 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3965 - decoder_dense_accuracy: 0.6108 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9100 - val_decoder_dense_loss: 6.5695 - val_softmax_layer_CC_loss: 2.7813 - val_softmax_layer_DC_loss: 14.1784 - val_decoder_dense_accuracy: 0.1866 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 262/800\n",
      "4984/4984 [==============================] - 33s 7ms/step - loss: -1.2581 - decoder_dense_loss: 1.7222 - softmax_layer_CC_loss: 0.0158 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.6112 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9701 - val_decoder_dense_loss: 6.5695 - val_softmax_layer_CC_loss: 2.8792 - val_softmax_layer_DC_loss: 14.1777 - val_decoder_dense_accuracy: 0.1864 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 263/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2596 - decoder_dense_loss: 1.7080 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.6159 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9373 - val_decoder_dense_loss: 6.5753 - val_softmax_layer_CC_loss: 2.8284 - val_softmax_layer_DC_loss: 14.1792 - val_decoder_dense_accuracy: 0.1904 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 264/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2586 - decoder_dense_loss: 1.6938 - softmax_layer_CC_loss: 0.0198 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.6187 - softmax_layer_CC_accuracy: 0.9886 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8710 - val_decoder_dense_loss: 6.5867 - val_softmax_layer_CC_loss: 2.7122 - val_softmax_layer_DC_loss: 14.1766 - val_decoder_dense_accuracy: 0.1872 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 265/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2626 - decoder_dense_loss: 1.6783 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.6236 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9165 - val_decoder_dense_loss: 6.5908 - val_softmax_layer_CC_loss: 2.7860 - val_softmax_layer_DC_loss: 14.1771 - val_decoder_dense_accuracy: 0.1888 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 266/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2619 - decoder_dense_loss: 1.6855 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.6186 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9311 - val_decoder_dense_loss: 6.5998 - val_softmax_layer_CC_loss: 2.8097 - val_softmax_layer_DC_loss: 14.1751 - val_decoder_dense_accuracy: 0.1872 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 267/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2633 - decoder_dense_loss: 1.6720 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3977 - decoder_dense_accuracy: 0.6244 - softmax_layer_CC_accuracy: 0.9918 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9796 - val_decoder_dense_loss: 6.6062 - val_softmax_layer_CC_loss: 2.8927 - val_softmax_layer_DC_loss: 14.1854 - val_decoder_dense_accuracy: 0.1874 - val_softmax_layer_CC_accuracy: 0.6472 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 268/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2638 - decoder_dense_loss: 1.6652 - softmax_layer_CC_loss: 0.0157 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.6247 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9701 - val_decoder_dense_loss: 6.5976 - val_softmax_layer_CC_loss: 2.8787 - val_softmax_layer_DC_loss: 14.1848 - val_decoder_dense_accuracy: 0.1902 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 269/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2639 - decoder_dense_loss: 1.6608 - softmax_layer_CC_loss: 0.0164 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.6242 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9252 - val_decoder_dense_loss: 6.6154 - val_softmax_layer_CC_loss: 2.8007 - val_softmax_layer_DC_loss: 14.1814 - val_decoder_dense_accuracy: 0.1866 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 270/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2656 - decoder_dense_loss: 1.6467 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.6291 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9276 - val_decoder_dense_loss: 6.6238 - val_softmax_layer_CC_loss: 2.8010 - val_softmax_layer_DC_loss: 14.1724 - val_decoder_dense_accuracy: 0.1901 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 271/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2666 - decoder_dense_loss: 1.6384 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.6332 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9605 - val_decoder_dense_loss: 6.6254 - val_softmax_layer_CC_loss: 2.8590 - val_softmax_layer_DC_loss: 14.1834 - val_decoder_dense_accuracy: 0.1888 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 272/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2666 - decoder_dense_loss: 1.6376 - softmax_layer_CC_loss: 0.0157 - softmax_layer_DC_loss: 14.3965 - decoder_dense_accuracy: 0.6309 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9402 - val_decoder_dense_loss: 6.6211 - val_softmax_layer_CC_loss: 2.8240 - val_softmax_layer_DC_loss: 14.1834 - val_decoder_dense_accuracy: 0.1890 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 273/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2674 - decoder_dense_loss: 1.6298 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.6344 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9632 - val_decoder_dense_loss: 6.6325 - val_softmax_layer_CC_loss: 2.8629 - val_softmax_layer_DC_loss: 14.1826 - val_decoder_dense_accuracy: 0.1889 - val_softmax_layer_CC_accuracy: 0.6464 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 274/800\n",
      "4984/4984 [==============================] - 36s 7ms/step - loss: -1.2686 - decoder_dense_loss: 1.6171 - softmax_layer_CC_loss: 0.0158 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.6354 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9531 - val_decoder_dense_loss: 6.6387 - val_softmax_layer_CC_loss: 2.8436 - val_softmax_layer_DC_loss: 14.1821 - val_decoder_dense_accuracy: 0.1868 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 275/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2685 - decoder_dense_loss: 1.6101 - softmax_layer_CC_loss: 0.0170 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.6381 - softmax_layer_CC_accuracy: 0.9886 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9447 - val_decoder_dense_loss: 6.6409 - val_softmax_layer_CC_loss: 2.8290 - val_softmax_layer_DC_loss: 14.1805 - val_decoder_dense_accuracy: 0.1900 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 276/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2707 - decoder_dense_loss: 1.5966 - softmax_layer_CC_loss: 0.0158 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.6402 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9315 - val_decoder_dense_loss: 6.6477 - val_softmax_layer_CC_loss: 2.8069 - val_softmax_layer_DC_loss: 14.1839 - val_decoder_dense_accuracy: 0.1894 - val_softmax_layer_CC_accuracy: 0.6464 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 277/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2709 - decoder_dense_loss: 1.5961 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.6402 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9136 - val_decoder_dense_loss: 6.6530 - val_softmax_layer_CC_loss: 2.7756 - val_softmax_layer_DC_loss: 14.1825 - val_decoder_dense_accuracy: 0.1893 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 278/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2713 - decoder_dense_loss: 1.5880 - softmax_layer_CC_loss: 0.0162 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.6427 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9108 - val_decoder_dense_loss: 6.6725 - val_softmax_layer_CC_loss: 2.7655 - val_softmax_layer_DC_loss: 14.1767 - val_decoder_dense_accuracy: 0.1889 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 279/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2709 - decoder_dense_loss: 1.5743 - softmax_layer_CC_loss: 0.0191 - softmax_layer_DC_loss: 14.3963 - decoder_dense_accuracy: 0.6455 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8996 - val_decoder_dense_loss: 6.6563 - val_softmax_layer_CC_loss: 2.7501 - val_softmax_layer_DC_loss: 14.1801 - val_decoder_dense_accuracy: 0.1897 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 280/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2737 - decoder_dense_loss: 1.5682 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.6467 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9180 - val_decoder_dense_loss: 6.6723 - val_softmax_layer_CC_loss: 2.7794 - val_softmax_layer_DC_loss: 14.1838 - val_decoder_dense_accuracy: 0.1881 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 281/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2734 - decoder_dense_loss: 1.5708 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.6458 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.8833 - val_decoder_dense_loss: 6.6854 - val_softmax_layer_CC_loss: 2.7180 - val_softmax_layer_DC_loss: 14.1747 - val_decoder_dense_accuracy: 0.1850 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 282/800\n",
      "4984/4984 [==============================] - 33s 7ms/step - loss: -1.2748 - decoder_dense_loss: 1.5586 - softmax_layer_CC_loss: 0.0153 - softmax_layer_DC_loss: 14.3970 - decoder_dense_accuracy: 0.6514 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9734 - val_decoder_dense_loss: 6.6799 - val_softmax_layer_CC_loss: 2.8715 - val_softmax_layer_DC_loss: 14.1827 - val_decoder_dense_accuracy: 0.1870 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 283/800\n",
      "4984/4984 [==============================] - 32s 6ms/step - loss: -1.2750 - decoder_dense_loss: 1.5526 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.6516 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9536 - val_decoder_dense_loss: 6.6938 - val_softmax_layer_CC_loss: 2.8377 - val_softmax_layer_DC_loss: 14.1836 - val_decoder_dense_accuracy: 0.1886 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 284/800\n",
      "4984/4984 [==============================] - 34s 7ms/step - loss: -1.2766 - decoder_dense_loss: 1.5371 - softmax_layer_CC_loss: 0.0157 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.6564 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9138 - val_decoder_dense_loss: 6.7004 - val_softmax_layer_CC_loss: 2.7665 - val_softmax_layer_DC_loss: 14.1781 - val_decoder_dense_accuracy: 0.1893 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 285/800\n",
      "4984/4984 [==============================] - 35s 7ms/step - loss: -1.2770 - decoder_dense_loss: 1.5357 - softmax_layer_CC_loss: 0.0153 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.6542 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9317 - val_decoder_dense_loss: 6.6871 - val_softmax_layer_CC_loss: 2.7976 - val_softmax_layer_DC_loss: 14.1783 - val_decoder_dense_accuracy: 0.1900 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 286/800\n",
      "4984/4984 [==============================] - 34s 7ms/step - loss: -1.2769 - decoder_dense_loss: 1.5348 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.6544 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9684 - val_decoder_dense_loss: 6.7058 - val_softmax_layer_CC_loss: 2.8556 - val_softmax_layer_DC_loss: 14.1786 - val_decoder_dense_accuracy: 0.1884 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 287/800\n",
      "4984/4984 [==============================] - 37s 7ms/step - loss: -1.2781 - decoder_dense_loss: 1.5229 - softmax_layer_CC_loss: 0.0157 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.6584 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9659 - val_decoder_dense_loss: 6.7167 - val_softmax_layer_CC_loss: 2.8519 - val_softmax_layer_DC_loss: 14.1764 - val_decoder_dense_accuracy: 0.1868 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 288/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 32s 6ms/step - loss: -1.2792 - decoder_dense_loss: 1.5135 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.6611 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9484 - val_decoder_dense_loss: 6.7276 - val_softmax_layer_CC_loss: 2.8203 - val_softmax_layer_DC_loss: 14.1804 - val_decoder_dense_accuracy: 0.1865 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 289/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2794 - decoder_dense_loss: 1.5103 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.6605 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9375 - val_decoder_dense_loss: 6.7167 - val_softmax_layer_CC_loss: 2.8038 - val_softmax_layer_DC_loss: 14.1797 - val_decoder_dense_accuracy: 0.1885 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 290/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2804 - decoder_dense_loss: 1.5037 - softmax_layer_CC_loss: 0.0151 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.6631 - softmax_layer_CC_accuracy: 0.9920 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9824 - val_decoder_dense_loss: 6.7234 - val_softmax_layer_CC_loss: 2.8805 - val_softmax_layer_DC_loss: 14.1839 - val_decoder_dense_accuracy: 0.1885 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 291/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2811 - decoder_dense_loss: 1.4923 - softmax_layer_CC_loss: 0.0157 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.6663 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9284 - val_decoder_dense_loss: 6.7281 - val_softmax_layer_CC_loss: 2.7872 - val_softmax_layer_DC_loss: 14.1743 - val_decoder_dense_accuracy: 0.1883 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 292/800\n",
      "4984/4984 [==============================] - 37s 7ms/step - loss: -1.2814 - decoder_dense_loss: 1.4916 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.6633 - softmax_layer_CC_accuracy: 0.9914 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9423 - val_decoder_dense_loss: 6.7496 - val_softmax_layer_CC_loss: 2.8051 - val_softmax_layer_DC_loss: 14.1749 - val_decoder_dense_accuracy: 0.1884 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 293/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2825 - decoder_dense_loss: 1.4802 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.6694 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9707 - val_decoder_dense_loss: 6.7531 - val_softmax_layer_CC_loss: 2.8492 - val_softmax_layer_DC_loss: 14.1776 - val_decoder_dense_accuracy: 0.1879 - val_softmax_layer_CC_accuracy: 0.6576 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 294/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2828 - decoder_dense_loss: 1.4766 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.6693 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9532 - val_decoder_dense_loss: 6.7587 - val_softmax_layer_CC_loss: 2.8208 - val_softmax_layer_DC_loss: 14.1735 - val_decoder_dense_accuracy: 0.1861 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 295/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2841 - decoder_dense_loss: 1.4632 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.6724 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9808 - val_decoder_dense_loss: 6.7446 - val_softmax_layer_CC_loss: 2.8725 - val_softmax_layer_DC_loss: 14.1869 - val_decoder_dense_accuracy: 0.1883 - val_softmax_layer_CC_accuracy: 0.6584 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 296/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2842 - decoder_dense_loss: 1.4622 - softmax_layer_CC_loss: 0.0157 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.6727 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9834 - val_decoder_dense_loss: 6.7439 - val_softmax_layer_CC_loss: 2.8763 - val_softmax_layer_DC_loss: 14.1800 - val_decoder_dense_accuracy: 0.1901 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 297/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2856 - decoder_dense_loss: 1.4491 - softmax_layer_CC_loss: 0.0156 - softmax_layer_DC_loss: 14.3963 - decoder_dense_accuracy: 0.6766 - softmax_layer_CC_accuracy: 0.9918 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9789 - val_decoder_dense_loss: 6.7541 - val_softmax_layer_CC_loss: 2.8678 - val_softmax_layer_DC_loss: 14.1861 - val_decoder_dense_accuracy: 0.1906 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 298/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2853 - decoder_dense_loss: 1.4485 - softmax_layer_CC_loss: 0.0161 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.6774 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9601 - val_decoder_dense_loss: 6.7637 - val_softmax_layer_CC_loss: 2.8329 - val_softmax_layer_DC_loss: 14.1836 - val_decoder_dense_accuracy: 0.1910 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 299/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2877 - decoder_dense_loss: 1.4278 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3988 - decoder_dense_accuracy: 0.6829 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0169 - val_decoder_dense_loss: 6.7998 - val_softmax_layer_CC_loss: 2.9144 - val_softmax_layer_DC_loss: 14.1771 - val_decoder_dense_accuracy: 0.1832 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0770\n",
      "Epoch 300/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2874 - decoder_dense_loss: 1.4310 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.6799 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9499 - val_decoder_dense_loss: 6.7862 - val_softmax_layer_CC_loss: 2.8104 - val_softmax_layer_DC_loss: 14.1771 - val_decoder_dense_accuracy: 0.1912 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 301/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2884 - decoder_dense_loss: 1.4216 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.6813 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9614 - val_decoder_dense_loss: 6.7920 - val_softmax_layer_CC_loss: 2.8289 - val_softmax_layer_DC_loss: 14.1775 - val_decoder_dense_accuracy: 0.1885 - val_softmax_layer_CC_accuracy: 0.6568 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 302/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2882 - decoder_dense_loss: 1.4228 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.6827 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9735 - val_decoder_dense_loss: 6.7905 - val_softmax_layer_CC_loss: 2.8514 - val_softmax_layer_DC_loss: 14.1792 - val_decoder_dense_accuracy: 0.1877 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 303/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2894 - decoder_dense_loss: 1.4112 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.6832 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9815 - val_decoder_dense_loss: 6.7936 - val_softmax_layer_CC_loss: 2.8653 - val_softmax_layer_DC_loss: 14.1764 - val_decoder_dense_accuracy: 0.1905 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 304/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2902 - decoder_dense_loss: 1.4028 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.6879 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9801 - val_decoder_dense_loss: 6.7843 - val_softmax_layer_CC_loss: 2.8641 - val_softmax_layer_DC_loss: 14.1843 - val_decoder_dense_accuracy: 0.1899 - val_softmax_layer_CC_accuracy: 0.6472 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 305/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2904 - decoder_dense_loss: 1.4008 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.6860 - softmax_layer_CC_accuracy: 0.9894 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9444 - val_decoder_dense_loss: 6.8040 - val_softmax_layer_CC_loss: 2.8005 - val_softmax_layer_DC_loss: 14.1757 - val_decoder_dense_accuracy: 0.1862 - val_softmax_layer_CC_accuracy: 0.6488 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 306/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2913 - decoder_dense_loss: 1.3929 - softmax_layer_CC_loss: 0.0153 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.6905 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9657 - val_decoder_dense_loss: 6.8285 - val_softmax_layer_CC_loss: 2.8317 - val_softmax_layer_DC_loss: 14.1759 - val_decoder_dense_accuracy: 0.1884 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 307/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2911 - decoder_dense_loss: 1.3879 - softmax_layer_CC_loss: 0.0164 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.6918 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9566 - val_decoder_dense_loss: 6.8080 - val_softmax_layer_CC_loss: 2.8198 - val_softmax_layer_DC_loss: 14.1839 - val_decoder_dense_accuracy: 0.1917 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 308/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2932 - decoder_dense_loss: 1.3735 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.6945 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9817 - val_decoder_dense_loss: 6.8246 - val_softmax_layer_CC_loss: 2.8552 - val_softmax_layer_DC_loss: 14.1815 - val_decoder_dense_accuracy: 0.1889 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 309/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2935 - decoder_dense_loss: 1.3706 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.6956 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9613 - val_decoder_dense_loss: 6.8278 - val_softmax_layer_CC_loss: 2.8257 - val_softmax_layer_DC_loss: 14.1832 - val_decoder_dense_accuracy: 0.1901 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 310/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.2939 - decoder_dense_loss: 1.3662 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.6962 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9627 - val_decoder_dense_loss: 6.8329 - val_softmax_layer_CC_loss: 2.8309 - val_softmax_layer_DC_loss: 14.1815 - val_decoder_dense_accuracy: 0.1893 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 311/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2943 - decoder_dense_loss: 1.3629 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3972 - decoder_dense_accuracy: 0.6963 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9594 - val_decoder_dense_loss: 6.8549 - val_softmax_layer_CC_loss: 2.8142 - val_softmax_layer_DC_loss: 14.1791 - val_decoder_dense_accuracy: 0.1893 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 312/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2950 - decoder_dense_loss: 1.3551 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.6986 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9888 - val_decoder_dense_loss: 6.8493 - val_softmax_layer_CC_loss: 2.8688 - val_softmax_layer_DC_loss: 14.1822 - val_decoder_dense_accuracy: 0.1916 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 313/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2957 - decoder_dense_loss: 1.3482 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.7016 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9563 - val_decoder_dense_loss: 6.8530 - val_softmax_layer_CC_loss: 2.8134 - val_softmax_layer_DC_loss: 14.1827 - val_decoder_dense_accuracy: 0.1900 - val_softmax_layer_CC_accuracy: 0.6464 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 314/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.2966 - decoder_dense_loss: 1.3398 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.7030 - softmax_layer_CC_accuracy: 0.9900 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9587 - val_decoder_dense_loss: 6.8640 - val_softmax_layer_CC_loss: 2.8133 - val_softmax_layer_DC_loss: 14.1790 - val_decoder_dense_accuracy: 0.1913 - val_softmax_layer_CC_accuracy: 0.6472 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 315/800\n",
      "4984/4984 [==============================] - 27s 6ms/step - loss: -1.2970 - decoder_dense_loss: 1.3371 - softmax_layer_CC_loss: 0.0151 - softmax_layer_DC_loss: 14.3963 - decoder_dense_accuracy: 0.7032 - softmax_layer_CC_accuracy: 0.9914 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0350 - val_decoder_dense_loss: 6.8678 - val_softmax_layer_CC_loss: 2.9434 - val_softmax_layer_DC_loss: 14.1886 - val_decoder_dense_accuracy: 0.1895 - val_softmax_layer_CC_accuracy: 0.6464 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 316/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2977 - decoder_dense_loss: 1.3299 - softmax_layer_CC_loss: 0.0152 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.7040 - softmax_layer_CC_accuracy: 0.9914 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0129 - val_decoder_dense_loss: 6.8776 - val_softmax_layer_CC_loss: 2.9007 - val_softmax_layer_DC_loss: 14.1824 - val_decoder_dense_accuracy: 0.1897 - val_softmax_layer_CC_accuracy: 0.6512 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 317/800\n",
      "4984/4984 [==============================] - 31s 6ms/step - loss: -1.2979 - decoder_dense_loss: 1.3272 - softmax_layer_CC_loss: 0.0152 - softmax_layer_DC_loss: 14.3988 - decoder_dense_accuracy: 0.7034 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0145 - val_decoder_dense_loss: 6.8811 - val_softmax_layer_CC_loss: 2.9028 - val_softmax_layer_DC_loss: 14.1815 - val_decoder_dense_accuracy: 0.1891 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 318/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.2984 - decoder_dense_loss: 1.3210 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.7062 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9864 - val_decoder_dense_loss: 6.8822 - val_softmax_layer_CC_loss: 2.8573 - val_softmax_layer_DC_loss: 14.1837 - val_decoder_dense_accuracy: 0.1893 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 319/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2993 - decoder_dense_loss: 1.3134 - softmax_layer_CC_loss: 0.0153 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.7100 - softmax_layer_CC_accuracy: 0.9902 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0033 - val_decoder_dense_loss: 6.9024 - val_softmax_layer_CC_loss: 2.8808 - val_softmax_layer_DC_loss: 14.1807 - val_decoder_dense_accuracy: 0.1899 - val_softmax_layer_CC_accuracy: 0.6536 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 320/800\n",
      "4984/4984 [==============================] - 27s 5ms/step - loss: -1.3000 - decoder_dense_loss: 1.3052 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.7114 - softmax_layer_CC_accuracy: 0.9918 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0214 - val_decoder_dense_loss: 6.9100 - val_softmax_layer_CC_loss: 2.9098 - val_softmax_layer_DC_loss: 14.1807 - val_decoder_dense_accuracy: 0.1900 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 321/800\n",
      "4984/4984 [==============================] - 30s 6ms/step - loss: -1.3010 - decoder_dense_loss: 1.2961 - softmax_layer_CC_loss: 0.0153 - softmax_layer_DC_loss: 14.3986 - decoder_dense_accuracy: 0.7142 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 0.9847 - val_decoder_dense_loss: 6.9101 - val_softmax_layer_CC_loss: 2.8495 - val_softmax_layer_DC_loss: 14.1841 - val_decoder_dense_accuracy: 0.1876 - val_softmax_layer_CC_accuracy: 0.6560 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 322/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.2995 - decoder_dense_loss: 1.3033 - softmax_layer_CC_loss: 0.0166 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.7103 - softmax_layer_CC_accuracy: 0.9904 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0111 - val_decoder_dense_loss: 6.9140 - val_softmax_layer_CC_loss: 2.8934 - val_softmax_layer_DC_loss: 14.1826 - val_decoder_dense_accuracy: 0.1926 - val_softmax_layer_CC_accuracy: 0.6592 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 323/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.3027 - decoder_dense_loss: 1.2801 - softmax_layer_CC_loss: 0.0153 - softmax_layer_DC_loss: 14.3984 - decoder_dense_accuracy: 0.7178 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0328 - val_decoder_dense_loss: 6.9182 - val_softmax_layer_CC_loss: 2.9316 - val_softmax_layer_DC_loss: 14.1893 - val_decoder_dense_accuracy: 0.1912 - val_softmax_layer_CC_accuracy: 0.6520 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 324/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.3024 - decoder_dense_loss: 1.2826 - softmax_layer_CC_loss: 0.0153 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.7162 - softmax_layer_CC_accuracy: 0.9908 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0192 - val_decoder_dense_loss: 6.9344 - val_softmax_layer_CC_loss: 2.9014 - val_softmax_layer_DC_loss: 14.1781 - val_decoder_dense_accuracy: 0.1881 - val_softmax_layer_CC_accuracy: 0.6528 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 325/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.3029 - decoder_dense_loss: 1.2776 - softmax_layer_CC_loss: 0.0153 - softmax_layer_DC_loss: 14.3976 - decoder_dense_accuracy: 0.7167 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0129 - val_decoder_dense_loss: 6.9238 - val_softmax_layer_CC_loss: 2.8945 - val_softmax_layer_DC_loss: 14.1893 - val_decoder_dense_accuracy: 0.1909 - val_softmax_layer_CC_accuracy: 0.6480 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 326/800\n",
      "4984/4984 [==============================] - 28s 6ms/step - loss: -1.3035 - decoder_dense_loss: 1.2698 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3982 - decoder_dense_accuracy: 0.7194 - softmax_layer_CC_accuracy: 0.9896 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0193 - val_decoder_dense_loss: 6.9296 - val_softmax_layer_CC_loss: 2.9030 - val_softmax_layer_DC_loss: 14.1856 - val_decoder_dense_accuracy: 0.1889 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 327/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.3041 - decoder_dense_loss: 1.2647 - softmax_layer_CC_loss: 0.0154 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.7223 - softmax_layer_CC_accuracy: 0.9898 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0003 - val_decoder_dense_loss: 6.9577 - val_softmax_layer_CC_loss: 2.8658 - val_softmax_layer_DC_loss: 14.1796 - val_decoder_dense_accuracy: 0.1878 - val_softmax_layer_CC_accuracy: 0.6552 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 328/800\n",
      "4984/4984 [==============================] - 29s 6ms/step - loss: -1.3048 - decoder_dense_loss: 1.2566 - softmax_layer_CC_loss: 0.0155 - softmax_layer_DC_loss: 14.3974 - decoder_dense_accuracy: 0.7243 - softmax_layer_CC_accuracy: 0.9916 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0068 - val_decoder_dense_loss: 6.9474 - val_softmax_layer_CC_loss: 2.8799 - val_softmax_layer_DC_loss: 14.1851 - val_decoder_dense_accuracy: 0.1905 - val_softmax_layer_CC_accuracy: 0.6544 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 329/800\n",
      "4984/4984 [==============================] - 36s 7ms/step - loss: -1.3052 - decoder_dense_loss: 1.2551 - softmax_layer_CC_loss: 0.0151 - softmax_layer_DC_loss: 14.3978 - decoder_dense_accuracy: 0.7221 - softmax_layer_CC_accuracy: 0.9906 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0309 - val_decoder_dense_loss: 6.9658 - val_softmax_layer_CC_loss: 2.9170 - val_softmax_layer_DC_loss: 14.1816 - val_decoder_dense_accuracy: 0.1884 - val_softmax_layer_CC_accuracy: 0.6472 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 330/800\n",
      "4984/4984 [==============================] - 34s 7ms/step - loss: -1.3059 - decoder_dense_loss: 1.2476 - softmax_layer_CC_loss: 0.0153 - softmax_layer_DC_loss: 14.3980 - decoder_dense_accuracy: 0.7255 - softmax_layer_CC_accuracy: 0.9912 - softmax_layer_DC_accuracy: 0.0638 - val_loss: 1.0182 - val_decoder_dense_loss: 6.9626 - val_softmax_layer_CC_loss: 2.8970 - val_softmax_layer_DC_loss: 14.1789 - val_decoder_dense_accuracy: 0.1922 - val_softmax_layer_CC_accuracy: 0.6504 - val_softmax_layer_DC_accuracy: 0.0778\n",
      "Epoch 331/800\n",
      "3328/4984 [===================>..........] - ETA: 8s - loss: -1.3028 - decoder_dense_loss: 1.2348 - softmax_layer_CC_loss: 0.0159 - softmax_layer_DC_loss: 14.3578 - decoder_dense_accuracy: 0.7285 - softmax_layer_CC_accuracy: 0.9910 - softmax_layer_DC_accuracy: 0.0664"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-50b49dbc9aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#                     shuffle=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     epochs=800)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss=['categorical_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],\n",
    "             loss_weights=[0.1,0.6,-0.1],metrics=[\"accuracy\"])\n",
    "history = model.fit(encoder_input_data, \n",
    "                    [decoder_target_data,y_cc,y_dc],\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "#                     shuffle=True,\n",
    "                    epochs=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(max_encoder_len,),name=\"encoder_inputs\")\n",
    "\n",
    "encoder_embedding = Embedding(vocab_len+1,100,trainable=True,weights=[embedding_matrix],input_length=max_encoder_len,mask_zero=True,name=\"encoder_embedding\")\n",
    "\n",
    "encoder_inputs = encoder_embedding(inputs)\n",
    "\n",
    "# encoder_inputs = LSTM(64,return_sequences=True)(encoder_inputs)\n",
    "encoder = LSTM(64,return_state=True)\n",
    "\n",
    "# encoder = Bidirectional(LSTM(64, \n",
    "#                     return_state=True, \n",
    "#                     name = 'encoder'))\n",
    "\n",
    "# encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# # We discard `encoder_outputs` and only keep the states.\n",
    "\n",
    "# encoder_states = [forward_h, backward_h]\n",
    "encoder_states = [state_h,state_c]\n",
    "\n",
    "# # Set up the decoder, using `encoder_states` as initial state.\n",
    "\n",
    "\n",
    "\n",
    "decoder_inputs = RepeatVector(max_encoder_len)(encoder_outputs)\n",
    "\n",
    "# # We set up our decoder to return full output sequences,\n",
    "# # and to return internal states as well. We don't use the\n",
    "# # return states in the training model, but we will use them in inference.\n",
    "\n",
    "decoder_lstm = LSTM(64, \n",
    "                         return_state=True,\n",
    "                        return_sequences=True,\n",
    "                         name = 'decoder_lstm')\n",
    "\n",
    "\n",
    "# # The inital_state call argument, specifying the initial state(s) of a RNN. \n",
    "# # This is used to pass the encoder states to the decoder as initial states.\n",
    "# # Basically making the first memory of the decoder the encoded semantics\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "# decoder_outputs = LSTM(128,return_sequences=True)(decoder_outputs)\n",
    "\n",
    "# decoder_outputs = Dense(64,activation=\"tanh\")(decoder_outputs)\n",
    "decoder_dense = Dense(vocab_len+1, \n",
    "                      activation='softmax', \n",
    "                      name = 'decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# # Define the model that will turn\n",
    "# # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model(inputs,decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, 10, 100)      1000200     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  [(None, 64), (None,  42240       encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_11 (RepeatVector) (None, 10, 64)       0           lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 10, 64), (No 33024       repeat_vector_11[0][0]           \n",
      "                                                                 lstm_13[0][1]                    \n",
      "                                                                 lstm_13[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, 10, 10002)    650130      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,725,594\n",
      "Trainable params: 1,725,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = test_generator(t_encoder_inputs,t_decoder_inputs,Y_CC,Y_DC,batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [encoder_input_data,decoder_input_data],decoder_target_data = next(generator)\n",
    "test_encoder_input_data,[test_decoder_target_data,test_y_cc,test_y_dc] = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data,decoder_target_data = all_data_generator(t_encoder_inputs,t_decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 10)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.3000e+01, 1.6000e+01, 4.1100e+02, 3.0000e+00, 4.3600e+02,\n",
       "       7.1000e+01, 4.1000e+01, 1.0001e+04, 1.2000e+01, 1.5460e+03])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [encoder_input_data,decoder_input_data],decoder_target_data = all_data_generator(t_encoder_inputs,t_decoder_inputs)\n",
    "# encoder_input_data,decoder_target_data = all_data_generator(t_encoder_inputs,t_decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 952., 1064., 2305.,    8.,  269., 4031.,   35.,   13.,  136.,\n",
       "         12.])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnav1712/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 5.5762\n",
      "Epoch 2/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.6231\n",
      "Epoch 3/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.4759\n",
      "Epoch 4/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.4262\n",
      "Epoch 5/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.3857\n",
      "Epoch 6/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.3516\n",
      "Epoch 7/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.3192\n",
      "Epoch 8/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.2938\n",
      "Epoch 9/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.2795\n",
      "Epoch 10/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.2769\n",
      "Epoch 11/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.2260\n",
      "Epoch 12/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.1987\n",
      "Epoch 13/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.1743\n",
      "Epoch 14/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.1423\n",
      "Epoch 15/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.1176\n",
      "Epoch 16/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.0899\n",
      "Epoch 17/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.0734\n",
      "Epoch 18/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.0593\n",
      "Epoch 19/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.0497\n",
      "Epoch 20/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 5.0374\n",
      "Epoch 21/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 5.0038\n",
      "Epoch 22/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.9931\n",
      "Epoch 23/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.9699\n",
      "Epoch 24/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.9323\n",
      "Epoch 25/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.9030\n",
      "Epoch 26/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8771\n",
      "Epoch 27/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8558\n",
      "Epoch 28/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8397\n",
      "Epoch 29/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8313\n",
      "Epoch 30/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8334\n",
      "Epoch 31/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.8259\n",
      "Epoch 32/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.8171\n",
      "Epoch 33/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.8136\n",
      "Epoch 34/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.7631\n",
      "Epoch 35/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.7331\n",
      "Epoch 36/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6886\n",
      "Epoch 37/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6647\n",
      "Epoch 38/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6441\n",
      "Epoch 39/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6293\n",
      "Epoch 40/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6206\n",
      "Epoch 41/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.6141\n",
      "Epoch 42/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.6109\n",
      "Epoch 43/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.5859\n",
      "Epoch 44/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.5816\n",
      "Epoch 45/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.5601\n",
      "Epoch 46/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.5369\n",
      "Epoch 47/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.5132\n",
      "Epoch 48/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.4805\n",
      "Epoch 49/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.4695\n",
      "Epoch 50/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.4549\n",
      "Epoch 51/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.4498\n",
      "Epoch 52/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.4244\n",
      "Epoch 53/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.4071\n",
      "Epoch 54/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3786\n",
      "Epoch 55/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3609\n",
      "Epoch 56/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3427\n",
      "Epoch 57/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3304\n",
      "Epoch 58/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3232\n",
      "Epoch 59/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.3044\n",
      "Epoch 60/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2819\n",
      "Epoch 61/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2567\n",
      "Epoch 62/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2369\n",
      "Epoch 63/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2325\n",
      "Epoch 64/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2310\n",
      "Epoch 65/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2373\n",
      "Epoch 66/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.2002\n",
      "Epoch 67/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.1828\n",
      "Epoch 68/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.1750\n",
      "Epoch 69/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.1797\n",
      "Epoch 70/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 4.1217\n",
      "Epoch 71/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0942\n",
      "Epoch 72/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0746\n",
      "Epoch 73/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0637\n",
      "Epoch 74/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0616\n",
      "Epoch 75/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0608\n",
      "Epoch 76/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0515\n",
      "Epoch 77/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 4.0206\n",
      "Epoch 78/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9977\n",
      "Epoch 79/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9717\n",
      "Epoch 80/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9610\n",
      "Epoch 81/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9422\n",
      "Epoch 82/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9290\n",
      "Epoch 83/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9128\n",
      "Epoch 84/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.9027\n",
      "Epoch 85/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8886\n",
      "Epoch 86/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8849\n",
      "Epoch 87/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8892\n",
      "Epoch 88/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8816\n",
      "Epoch 89/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8570\n",
      "Epoch 90/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8387\n",
      "Epoch 91/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8349\n",
      "Epoch 92/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8211\n",
      "Epoch 93/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.8198\n",
      "Epoch 94/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.7776\n",
      "Epoch 95/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.7511\n",
      "Epoch 96/800\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 3.7408\n",
      "Epoch 97/800\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 3.7292\n",
      "Epoch 98/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.7227\n",
      "Epoch 99/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6748\n",
      "Epoch 101/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6589\n",
      "Epoch 102/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6480\n",
      "Epoch 103/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6429\n",
      "Epoch 104/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6365\n",
      "Epoch 105/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6311\n",
      "Epoch 106/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6160\n",
      "Epoch 107/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.6111\n",
      "Epoch 108/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.5937\n",
      "Epoch 109/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.5853\n",
      "Epoch 110/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.5583\n",
      "Epoch 111/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.5391\n",
      "Epoch 112/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.5095\n",
      "Epoch 113/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.4937\n",
      "Epoch 114/800\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 3.4836\n",
      "Epoch 115/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4808\n",
      "Epoch 116/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4796\n",
      "Epoch 117/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.4770\n",
      "Epoch 118/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4561\n",
      "Epoch 119/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4266\n",
      "Epoch 120/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4163\n",
      "Epoch 121/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4123\n",
      "Epoch 122/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.4310\n",
      "Epoch 123/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.4254\n",
      "Epoch 124/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3796\n",
      "Epoch 125/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3482\n",
      "Epoch 126/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3348\n",
      "Epoch 127/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3318\n",
      "Epoch 128/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3430\n",
      "Epoch 129/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3298\n",
      "Epoch 130/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.3103\n",
      "Epoch 131/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2786\n",
      "Epoch 132/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2556\n",
      "Epoch 133/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2444\n",
      "Epoch 134/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2360\n",
      "Epoch 135/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2307\n",
      "Epoch 136/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.2260\n",
      "Epoch 137/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.2195\n",
      "Epoch 138/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.2111\n",
      "Epoch 139/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.1936\n",
      "Epoch 140/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1853\n",
      "Epoch 141/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1740\n",
      "Epoch 142/800\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 3.1593\n",
      "Epoch 143/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 3.1418\n",
      "Epoch 144/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1272\n",
      "Epoch 145/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1056\n",
      "Epoch 146/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0952\n",
      "Epoch 147/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0854\n",
      "Epoch 148/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1039\n",
      "Epoch 149/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1005\n",
      "Epoch 150/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.1031\n",
      "Epoch 151/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0516\n",
      "Epoch 152/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0335\n",
      "Epoch 153/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0118\n",
      "Epoch 154/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 3.0007\n",
      "Epoch 155/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.9908\n",
      "Epoch 156/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9842\n",
      "Epoch 157/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.9706\n",
      "Epoch 158/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.9634\n",
      "Epoch 159/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.9616\n",
      "Epoch 160/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9718\n",
      "Epoch 161/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9678\n",
      "Epoch 162/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9491\n",
      "Epoch 163/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9195\n",
      "Epoch 164/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9087\n",
      "Epoch 165/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.9014\n",
      "Epoch 166/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8880\n",
      "Epoch 167/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8680\n",
      "Epoch 168/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8436\n",
      "Epoch 169/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8295\n",
      "Epoch 170/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8149\n",
      "Epoch 171/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.8100\n",
      "Epoch 172/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8092\n",
      "Epoch 173/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8295\n",
      "Epoch 174/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8491\n",
      "Epoch 175/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.8433\n",
      "Epoch 176/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7911\n",
      "Epoch 177/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7523\n",
      "Epoch 178/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7332\n",
      "Epoch 179/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7228\n",
      "Epoch 180/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7218\n",
      "Epoch 181/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7203\n",
      "Epoch 182/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7278\n",
      "Epoch 183/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7273\n",
      "Epoch 184/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.7036\n",
      "Epoch 185/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.6807\n",
      "Epoch 186/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.6663\n",
      "Epoch 187/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.6721\n",
      "Epoch 188/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.6802\n",
      "Epoch 189/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.6781\n",
      "Epoch 190/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.6691\n",
      "Epoch 191/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.6326\n",
      "Epoch 192/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.6069\n",
      "Epoch 193/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5874\n",
      "Epoch 194/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5796\n",
      "Epoch 195/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5760\n",
      "Epoch 196/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5848\n",
      "Epoch 197/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5789\n",
      "Epoch 199/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5534\n",
      "Epoch 200/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5274\n",
      "Epoch 201/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5099\n",
      "Epoch 202/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5082\n",
      "Epoch 203/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5279\n",
      "Epoch 204/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5599\n",
      "Epoch 205/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.5306\n",
      "Epoch 206/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4918\n",
      "Epoch 207/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4672\n",
      "Epoch 208/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4654\n",
      "Epoch 209/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4676\n",
      "Epoch 210/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4615\n",
      "Epoch 211/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4415\n",
      "Epoch 212/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4180\n",
      "Epoch 213/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4111\n",
      "Epoch 214/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4082\n",
      "Epoch 215/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.4236\n",
      "Epoch 216/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.4115\n",
      "Epoch 217/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3996\n",
      "Epoch 218/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.3725\n",
      "Epoch 219/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3587\n",
      "Epoch 220/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.3536\n",
      "Epoch 221/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.3472\n",
      "Epoch 222/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3514\n",
      "Epoch 223/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3467\n",
      "Epoch 224/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3545\n",
      "Epoch 225/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3424\n",
      "Epoch 226/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3179\n",
      "Epoch 227/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.3019\n",
      "Epoch 228/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2883\n",
      "Epoch 229/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2865\n",
      "Epoch 230/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2762\n",
      "Epoch 231/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2821\n",
      "Epoch 232/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2769\n",
      "Epoch 233/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2635\n",
      "Epoch 234/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2471\n",
      "Epoch 235/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2424\n",
      "Epoch 236/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2494\n",
      "Epoch 237/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2495\n",
      "Epoch 238/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2422\n",
      "Epoch 239/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2225\n",
      "Epoch 240/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.2082\n",
      "Epoch 241/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1925\n",
      "Epoch 242/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1794\n",
      "Epoch 243/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1703\n",
      "Epoch 244/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1564\n",
      "Epoch 245/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1514\n",
      "Epoch 246/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1443\n",
      "Epoch 247/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1437\n",
      "Epoch 248/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1322\n",
      "Epoch 249/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1254\n",
      "Epoch 250/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1169\n",
      "Epoch 251/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1198\n",
      "Epoch 252/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1222\n",
      "Epoch 253/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1324\n",
      "Epoch 254/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1494\n",
      "Epoch 255/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.1094\n",
      "Epoch 256/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.0847\n",
      "Epoch 257/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 2.0613\n",
      "Epoch 258/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0606\n",
      "Epoch 259/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0735\n",
      "Epoch 260/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0709\n",
      "Epoch 261/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0627\n",
      "Epoch 262/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0462\n",
      "Epoch 263/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0405\n",
      "Epoch 264/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0364\n",
      "Epoch 265/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0332\n",
      "Epoch 266/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0209\n",
      "Epoch 267/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0076\n",
      "Epoch 268/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 1.9915\n",
      "Epoch 269/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9856\n",
      "Epoch 270/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9715\n",
      "Epoch 271/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9688\n",
      "Epoch 272/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9618\n",
      "Epoch 273/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9659\n",
      "Epoch 274/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9677\n",
      "Epoch 275/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9843\n",
      "Epoch 276/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 2.0023\n",
      "Epoch 277/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9676\n",
      "Epoch 278/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9332\n",
      "Epoch 279/800\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 1.9128\n",
      "Epoch 280/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 1.9079\n",
      "Epoch 281/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9169\n",
      "Epoch 282/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9322\n",
      "Epoch 283/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9456\n",
      "Epoch 284/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.9228\n",
      "Epoch 285/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8953\n",
      "Epoch 286/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8738\n",
      "Epoch 287/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8728\n",
      "Epoch 288/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8628\n",
      "Epoch 289/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8645\n",
      "Epoch 290/800\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 1.8569\n",
      "Epoch 291/800\n",
      "128/128 [==============================] - 1s 4ms/step - loss: 1.8582\n",
      "Epoch 292/800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-b36b4a4925a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mdecoder_target_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     epochs=800)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#                     validation_split=0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Desktop/University_Stuff/SMM/Cross-Domain-Fake-News-Detection/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "history = model.fit(encoder_input_data, \n",
    "                    decoder_target_data,\n",
    "                    batch_size=128,\n",
    "                    epochs=800)\n",
    "#                     validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the player who’s who in pippa middletongs young bridal "
     ]
    }
   ],
   "source": [
    "for idx in test_encoder_input_data[100]:\n",
    "    print(idx2word[idx],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict([test_encoder_input_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kim said decided decided to a a a a a "
     ]
    }
   ],
   "source": [
    "for idx in output_array[0][28]:\n",
    "    lookup = np.argmax(idx)\n",
    "#     print(lookup)\n",
    "    if lookup==0:\n",
    "        break\n",
    "    else:\n",
    "        print(idx2word[lookup],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9437695e-07\n"
     ]
    }
   ],
   "source": [
    "for idx in output_array[100]:\n",
    "    print(idx[np.argmax(20)])\n",
    "    break\n",
    "    lookup = np.argmax(idx)\n",
    "#     print(lookup)\n",
    "    if lookup==0:\n",
    "        break\n",
    "    else:\n",
    "        print(idx2word[lookup],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Social",
   "language": "python",
   "name": "social"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
